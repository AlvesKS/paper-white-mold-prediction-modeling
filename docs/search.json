[
  {
    "objectID": "code_modeling.html",
    "href": "code_modeling.html",
    "title": "Models for white mold",
    "section": "",
    "text": "library(iml)           # for model-agnostic interpretation; Ver. 0.11.3\nlibrary(tidyverse)     # general wrangling; Ver. 2.0.0\nlibrary(rsample)       # for bootstrap resampling; Ver. 1.2.1\nlibrary(labelled)      # for general functions to work with labelled data; Ver. 2.13.0\nlibrary(gtsummary)     # automatic use of variable labels in summary tables; Ver. 2.0.3\nlibrary(kableExtra)    # nice table output in html; Ver. 1.4.0\n\nlibrary(ranger)        # random forest model fitting; Ver. 0.16.0\nlibrary(tuneRanger)    # for tuning a ranger model; Ver. 0.7\nlibrary(mlr)           # an mlr task is required by tuneRanger; Ver. 2.19.2\nlibrary(cutpointr)     # Cutpoint determination for classification; Ver. 1.2.0\nlibrary(furrr)         # For parallel processing; Ver. 0.3.1\nlibrary(bayestestR)    # For calculating HDI; Ver. 0.15.0\nlibrary(pROC)          # to get the C-statistic; Ver. 1.18.5\nlibrary(vip)           # Variable importance measures; Ver. 0.4.1\nlibrary(pdp)           # Partial dependence plots; Ver. 0.8.1\n\nlibrary(VSURF)         # for RF-based predictor selection; Ver. 1.2.0\n\nlibrary(predtools)     # Calibration plot; Ver. 0.0.3\nlibrary(patchwork)     # Combining plots; Ver. 1.3.0\nlibrary(lattice)       # Wireframe plots for 2-way pdp; Ver. 0.22-6\n\nlibrary(kernelshap)    # Getting SHAP values; Ver. 0.7.0\nlibrary(shapviz)       # Visualize said SHAP values; Ver. 0.9.6\nlibrary(tictoc)        # How long stuff takes; Ver.  1.2.1\n\n\nlibrary(rms)               # Fit logistic regression models; Ver. 6.8-2\nlibrary(CalibrationCurves) # Plot calibration curves; Ver.2.0.3\nlibrary(R.devices)     # suppressGraphics function to suppress the automatic outputting of a plot     \n\n\nmake_kable &lt;- function(...) {\n  # kable and kableExtra styling to avoid repetitively calling the styling over and over again\n  # See: https://stackoverflow.com/questions/73718600/option-to-specify-default-kableextra-styling-in-rmarkdown\n  # knitr::kable(...) %&gt;%\n  kable(..., format = \"html\", row.names = TRUE, align = 'l') %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\"), position = \"left\", font_size = 11, full_width = FALSE) \n}\n\n\nload(here::here(\"DataFusion\", \"FusedData.RData\"))  # X",
    "crumbs": [
      "Data analysis",
      "Models for white mold"
    ]
  },
  {
    "objectID": "code_modeling.html#packages",
    "href": "code_modeling.html#packages",
    "title": "Models for white mold",
    "section": "",
    "text": "library(iml)           # for model-agnostic interpretation; Ver. 0.11.3\nlibrary(tidyverse)     # general wrangling; Ver. 2.0.0\nlibrary(rsample)       # for bootstrap resampling; Ver. 1.2.1\nlibrary(labelled)      # for general functions to work with labelled data; Ver. 2.13.0\nlibrary(gtsummary)     # automatic use of variable labels in summary tables; Ver. 2.0.3\nlibrary(kableExtra)    # nice table output in html; Ver. 1.4.0\n\nlibrary(ranger)        # random forest model fitting; Ver. 0.16.0\nlibrary(tuneRanger)    # for tuning a ranger model; Ver. 0.7\nlibrary(mlr)           # an mlr task is required by tuneRanger; Ver. 2.19.2\nlibrary(cutpointr)     # Cutpoint determination for classification; Ver. 1.2.0\nlibrary(furrr)         # For parallel processing; Ver. 0.3.1\nlibrary(bayestestR)    # For calculating HDI; Ver. 0.15.0\nlibrary(pROC)          # to get the C-statistic; Ver. 1.18.5\nlibrary(vip)           # Variable importance measures; Ver. 0.4.1\nlibrary(pdp)           # Partial dependence plots; Ver. 0.8.1\n\nlibrary(VSURF)         # for RF-based predictor selection; Ver. 1.2.0\n\nlibrary(predtools)     # Calibration plot; Ver. 0.0.3\nlibrary(patchwork)     # Combining plots; Ver. 1.3.0\nlibrary(lattice)       # Wireframe plots for 2-way pdp; Ver. 0.22-6\n\nlibrary(kernelshap)    # Getting SHAP values; Ver. 0.7.0\nlibrary(shapviz)       # Visualize said SHAP values; Ver. 0.9.6\nlibrary(tictoc)        # How long stuff takes; Ver.  1.2.1\n\n\nlibrary(rms)               # Fit logistic regression models; Ver. 6.8-2\nlibrary(CalibrationCurves) # Plot calibration curves; Ver.2.0.3\nlibrary(R.devices)     # suppressGraphics function to suppress the automatic outputting of a plot     \n\n\nmake_kable &lt;- function(...) {\n  # kable and kableExtra styling to avoid repetitively calling the styling over and over again\n  # See: https://stackoverflow.com/questions/73718600/option-to-specify-default-kableextra-styling-in-rmarkdown\n  # knitr::kable(...) %&gt;%\n  kable(..., format = \"html\", row.names = TRUE, align = 'l') %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\"), position = \"left\", font_size = 11, full_width = FALSE) \n}\n\n\nload(here::here(\"DataFusion\", \"FusedData.RData\"))  # X",
    "crumbs": [
      "Data analysis",
      "Models for white mold"
    ]
  },
  {
    "objectID": "code_modeling.html#data-dictionary",
    "href": "code_modeling.html#data-dictionary",
    "title": "Models for white mold",
    "section": "Data dictionary",
    "text": "Data dictionary\n\n\nX_dictionary &lt;- X_labelled |&gt; \n  generate_dictionary()\n\nX_dictionary |&gt; \n  make_kable()\n\n\n\n\n\npos\nvariable\nlabel\ncol_type\nmissing\nlevels\nvalue_labels\n\n\n\n\n1\n1\nsubject\nSnap bean field\ndbl\n0\nNULL\nNULL\n\n\n2\n2\nwm\nWhite mold presence\ndbl\n0\nNULL\nNULL\n\n\n3\n3\ndrainage\nSoil drainage class\nfct\n0\nWell_Drained , Poorly_Drained\nNULL\n\n\n4\n4\nhydrol\nSoil hydrological group\nfct\n0\nA, B, C, D\nNULL\n\n\n5\n5\ncd\nClimate division\nfct\n0\nCentral Lakes, Great Lakes\nNULL\n\n\n6\n6\nharv.optim\nField harvested &lt;=60 dap\nfct\n0\nYes, No\nNULL\n\n\n7\n7\nph\nSoil pH\ndbl\n0\nNULL\nNULL\n\n\n8\n8\nom\nSoil organic matter content (%)\ndbl\n0\nNULL\nNULL\n\n\n9\n9\nlog_sand_clay\nLogratio sand:clay\ndbl\n0\nNULL\nNULL\n\n\n10\n10\nlog_silt_clay\nLogratio silt:clay\ndbl\n0\nNULL\nNULL\n\n\n11\n11\ncc35\nCanopy gap (cm) at 35 dap\ndbl\n0\nNULL\nNULL\n\n\n12\n12\nrainto35dap\nTotal rain (mm) from planting to 35 dap\ndbl\n0\nNULL\nNULL\n\n\n13\n13\nrain36to50dap\nTotal rain (mm) from 36 to 50 dap\ndbl\n0\nNULL\nNULL\n\n\n14\n14\nt2m_mean_to_4dap\nMean air temperature (°C) from planting to 4 dap\ndbl\n0\nNULL\nNULL\n\n\n15\n15\nsm_4dbp_to_3dap\nMean vsw (m³/m³) from 4 days before planting to 3 dap\ndbl\n0\nNULL\nNULL\n\n\n16\n16\nsm_5dap_to_15dap\nMean vsw (m³/m³) from 5 to 15 dap\ndbl\n0\nNULL\nNULL\n\n\n17\n17\nsm_17dap_to_24dap\nMean vsw (m³/m³) from 17 to 24 dap\ndbl\n0\nNULL\nNULL\n\n\n18\n18\nsm_40dap_to_49dap\nMean vsw (m³/m³) from 40 to 49 dap\ndbl\n0\nNULL\nNULL\n\n\n19\n19\nstsm_35dap_to_44dap\nMean soil temperature (°C):vsw (m³/m³) ratio from 35 to 44 dap\ndbl\n0\nNULL\nNULL",
    "crumbs": [
      "Data analysis",
      "Models for white mold"
    ]
  },
  {
    "objectID": "code_modeling.html#the-variables",
    "href": "code_modeling.html#the-variables",
    "title": "Models for white mold",
    "section": "The variables",
    "text": "The variables\n\nX_labelled |&gt; \n  dplyr::select(-subject) |&gt;\n  tbl_summary(\n    by = wm\n  ) |&gt; \n  bold_labels()\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n0 N = 2831\n1 N = 731\n\n\n\n\nSoil drainage class\n\n\n\n\n\n\n    Well_Drained\n210 (74%)\n56 (77%)\n\n\n    Poorly_Drained\n73 (26%)\n17 (23%)\n\n\nSoil hydrological group\n\n\n\n\n\n\n    A\n40 (14%)\n14 (19%)\n\n\n    B\n9 (3.2%)\n6 (8.2%)\n\n\n    C\n51 (18%)\n5 (6.8%)\n\n\n    D\n183 (65%)\n48 (66%)\n\n\nClimate division\n\n\n\n\n\n\n    Central Lakes\n71 (25%)\n22 (30%)\n\n\n    Great Lakes\n212 (75%)\n51 (70%)\n\n\nField harvested &lt;=60 dap\n221 (78%)\n49 (67%)\n\n\nSoil pH\n6.01 (5.86, 6.12)\n6.01 (5.86, 6.13)\n\n\nSoil organic matter content (%)\n0.52 (0.49, 0.55)\n0.53 (0.50, 0.55)\n\n\nLogratio sand:clay\n0.75 (0.27, 1.00)\n0.70 (0.33, 0.96)\n\n\nLogratio silt:clay\n1.10 (1.03, 1.17)\n1.11 (1.02, 1.23)\n\n\nCanopy gap (cm) at 35 dap\n51 (44, 56)\n49 (44, 52)\n\n\nTotal rain (mm) from planting to 35 dap\n110 (80, 160)\n146 (89, 163)\n\n\nTotal rain (mm) from 36 to 50 dap\n41 (23, 76)\n60 (41, 80)\n\n\nMean air temperature (°C) from planting to 4 dap\n20.10 (17.73, 21.24)\n21.40 (20.06, 22.81)\n\n\nMean vsw (m³/m³) from 4 days before planting to 3 dap\n0.30 (0.26, 0.33)\n0.33 (0.29, 0.35)\n\n\nMean vsw (m³/m³) from 5 to 15 dap\n0.30 (0.26, 0.35)\n0.33 (0.29, 0.36)\n\n\nMean vsw (m³/m³) from 17 to 24 dap\n0.30 (0.25, 0.33)\n0.30 (0.28, 0.36)\n\n\nMean vsw (m³/m³) from 40 to 49 dap\n0.29 (0.24, 0.36)\n0.33 (0.30, 0.37)\n\n\nMean soil temperature (°C):vsw (m³/m³) ratio from 35 to 44 dap\n68 (58, 84)\n61 (55, 67)\n\n\n\n1 n (%); Median (Q1, Q3)\n\n\n\n\n\n\n\n\nExport the Table to a Word file to use in the manuscript\n\nX_labelled |&gt; \n  dplyr::select(-subject) |&gt;\n  tbl_summary(\n    by = wm\n  ) |&gt; \n  bold_labels() |&gt; \n  as_gt() |&gt; \n  gt::gtsave(filename = here::here(\"Modeling\", \"foo.docx\"))",
    "crumbs": [
      "Data analysis",
      "Models for white mold"
    ]
  },
  {
    "objectID": "code_modeling.html#examine-the-fitted-model",
    "href": "code_modeling.html#examine-the-fitted-model",
    "title": "Models for white mold",
    "section": "Examine the fitted model",
    "text": "Examine the fitted model\n\n# Load the fitted model and associated dataframe:\nload(here::here(\"Modeling\", \"rf_tuned.RData\"))  # rf.tuned, X.df\n\n# Model with the new tuned hyperparameters\n# rf.tuned$model\n\n\n# Predicted probabilities directly from the tuned model.\n\n# Prediction (a data frame with two columns: truth, response)\n# where truth = 0, 1 for the observed wm status\n# response = the fitted Prob(wm = 1)\n# NOTE: here truth is a factor\npred &lt;-\n  predict(rf.tuned$model, newdata = X.df)$data %&gt;%\n  dplyr::select(truth, prob.1) %&gt;%\n  dplyr::rename(response = prob.1)\n\n# Check:\nhead(pred)\n\n\nThe tuned hyperparameters\n\nrf.tuned.pars &lt;-\n  list(\n    mtry = rf.tuned$recommended.pars$mtry,\n    min.node.size = rf.tuned$recommended.pars$min.node.size,\n    sample.fraction = rf.tuned$recommended.pars$sample.fraction,\n    num.trees = rf.tuned$model$learner$par.vals$num.trees\n  ) \n\nrf.tuned.pars %&gt;%\n  as_tibble() %&gt;%\n  make_kable()\n\n\n\n\n\nmtry\nmin.node.size\nsample.fraction\nnum.trees\n\n\n\n\n1\n2\n9\n0.844\n1000\n\n\n\n\n\n\n\n\n\nEstimating optimism\n\n# Function to calculate Youden Index, Sensitivity, and Specificity using the `cutpointr` package:\ncalc_metrics_cutpointr &lt;- function(pred) {\n  opt_cut &lt;- cutpointr::cutpointr(\n    data = pred,\n    x = response,\n    class = truth,\n    direction = \"&gt;=\",\n    pos_class = 1,\n    neg_class = 0,\n    method = maximize_metric,\n    metric = youden\n  )\n  \n  list(\n    YI = opt_cut$youden,\n    Se = as.numeric(opt_cut$sensitivity),\n    Sp = as.numeric(opt_cut$specificity),\n    AUC = opt_cut$AUC,\n    brier = mean((pred$response - ifelse(pred$truth == \"1\", 1, 0))^2)\n  )\n}\n\n# 1. Prepare predictions dataframe:\npred &lt;-\n  predict(rf.tuned$model, newdata = X.df)$data %&gt;%\n  dplyr::select(truth, prob.1) %&gt;%\n  dplyr::rename(response = prob.1)\n\n# 2. Calculate apparent performance\napparent_perf &lt;- calc_metrics_cutpointr(pred)\n\n# 3. Bootstrap process\nbootstrap_fxn &lt;- function(i) {\n  # Print the current iteration\n  cat(\"Currently processing iteration: \", i, \"\\n\")\n  \n  # a. Generate bootstrap sample\n  boot_sample &lt;- \n    rsample::bootstraps(X.df, times = 1, strata = wm)$splits[[1]] %&gt;%\n    rsample::analysis()\n  \n  # b. Create classification task and fit model on bootstrap sample with error handling\n  cc.task &lt;- mlr::makeClassifTask(data = boot_sample, target = \"wm\", positive = \"1\")\n  \n  boot_model &lt;- tryCatch(\n    expr = {\n      tuneRanger::tuneRanger(cc.task, num.trees = 1000, show.info = FALSE)$model\n    },\n    error = function(e) {\n      cat(\"Error occurred in iteration\", i, \":\", e$message, \"\\n\")\n      return(NULL) # Return NULL if an error occurs\n    }\n  )\n  \n  # If boot_model is NULL, skip this iteration\n  if (is.null(boot_model)) {\n    cat(\"Skipping iteration\", i, \"due to error.\\n\")\n    return(NULL)\n  }\n  \n  # c. Calculate metrics on bootstrap sample (training performance)\n  boot_pred &lt;- \n    predict(boot_model, newdata = boot_sample)$data %&gt;%\n    dplyr::select(truth, prob.1) %&gt;%\n    dplyr::rename(response = prob.1)\n  \n  train_perf &lt;- calc_metrics_cutpointr(boot_pred)\n  \n  # d & e. Apply bootstrap model to original dataset and calculate metrics (test performance)\n  test_pred &lt;- predict(boot_model, newdata = X.df)$data %&gt;%\n    dplyr::select(truth, prob.1) %&gt;%\n    dplyr::rename(response = prob.1)\n  \n  test_perf &lt;- calc_metrics_cutpointr(test_pred)\n  \n  # f. Calculate optimism\n  optimism &lt;- purrr::map2_dbl(train_perf, test_perf, \\(x, y) x-y)\n  \n  tibble(\n    iteration = i,\n    metric = names(train_perf),\n    train = unlist(train_perf),\n    test = unlist(test_perf),\n    optimism = optimism)\n  }\n\nset.seed(123)\n# Run bootstrap_fxn in parallel:\nmy.seeds &lt;- c(1:1000)\n\nplan(list(tweak(multisession, workers = 5), sequential))\ntictoc::tic()\nbootstrap_results &lt;- \n  furrr::future_map(my.seeds, bootstrap_fxn, .options = furrr_options(seed = TRUE)) %&gt;% \n  purrr::list_rbind()\ntictoc::toc()  # 11404.61 sec ~ 3.17 hr\n\nplan(sequential)\n\n# 4. Apparent performance bootstrap HDI\napparent_CI &lt;-\n  bootstrap_results %&gt;% \n  dplyr::group_by(metric) %&gt;% \n  dplyr::summarize(app_LCI = bayestestR::hdi(train, ci = 0.95)$CI_low, \n                   app_UCI = bayestestR::hdi(train, ci = 0.95)$CI_high)\n\n# 5. Average optimism across all bootstrap samples\navg_optimism &lt;- \n  bootstrap_results %&gt;%\n  dplyr::group_by(metric) %&gt;%\n  dplyr::summarize(avg_optimism = mean(optimism))\n\n# 6. Calculate optimism-corrected performance\ncorrected_perf &lt;- \n  tibble(\n    metric = names(apparent_perf),\n    apparent = unlist(apparent_perf)\n  ) %&gt;%\n  dplyr::left_join(apparent_CI, by = \"metric\") %&gt;%\n  dplyr::left_join(avg_optimism, by = \"metric\") %&gt;%\n  dplyr::mutate(corrected = apparent - avg_optimism)\n\n\n# Save the results so that you don't have to ever, ever repeat this time-consuming process:\nsave(pred, apparent_perf, bootstrap_results, corrected_perf, file = here::here(\"Modeling\", \"RFoptimism.RData\"))\n\n\n# Load the model-fitted probs and the optimism estimates:\n# pred, apparent_perf, bootstrap_results, corrected_perf\nload(here::here(\"Modeling\", \"RFoptimism.RData\"))  \n\ncorrected_perf %&gt;% \n  make_kable()\n\n\n\n\n\nmetric\napparent\napp_LCI\napp_UCI\navg_optimism\ncorrected\n\n\n\n\n1\nYI\n0.968\n0.975\n1.00\n0.196\n0.772\n\n\n2\nSe\n1.000\n0.986\n1.00\n0.121\n0.879\n\n\n3\nSp\n0.968\n0.979\n1.00\n0.075\n0.893\n\n\n4\nAUC\n0.997\n0.999\n1.00\n0.046\n0.951\n\n\n5\nbrier\n0.039\n0.001\n0.01\n-0.052\n0.090\n\n\n\n\n\n\n\n\n\nC-statistic\n\n# Get the C-statistic (AUROC):\n# Assuming your data frame is called 'pred' with columns 'truth' and 'response'\nroc_obj &lt;- pROC::roc(pred$truth, pred$response, levels = c(0, 1))\n\n# 95% CI for the C-statistic:\nci_obj &lt;- ci.auc(roc_obj)\n\n\nres_C &lt;- matrix(\n  c(ci_obj[2],\n    ci_obj[1],\n    ci_obj[3],\n    corrected_perf %&gt;% dplyr::filter(metric == \"AUC\") %&gt;% dplyr::pull(corrected) %&gt;% unname(),\n    NA,\n    NA),\n  \n  nrow = 1,\n  ncol = 6,\n  byrow = T,\n  \n  dimnames = list(\n    c(\"C-statistic\"),\n    rep(c(\"Estimate\", \"Lower .95 \", \"Upper .95\"), 2)\n  )\n)\n\nres_C %&gt;% \n  make_kable() %&gt;% \n  add_header_above(c(\" \" = 1, \n                     \"Apparent\" = 3, \n                     \"Internal\" = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApparent\n\n\nInternal\n\n\n\n\nEstimate\nLower .95\nUpper .95\nEstimate\nLower .95\nUpper .95\n\n\n\n\nC-statistic\n0.997\n0.995\n1\n0.951\nNA\nNA\n\n\n\n\n\n\n\n\n\nBrier score\n\n# Brier score:\ntibble::tibble(`Brier (apparent)` = corrected_perf %&gt;% dplyr::filter(metric == \"brier\") %&gt;% dplyr::pull(apparent) %&gt;% unname(), \n               `Brier (internal)` = corrected_perf %&gt;% dplyr::filter(metric == \"brier\") %&gt;% dplyr::pull(corrected) %&gt;% unname()) %&gt;% \n  make_kable()\n\n\n\n\n\nBrier (apparent)\nBrier (internal)\n\n\n\n\n1\n0.039\n0.09\n\n\n\n\n\n\n\n\n\nCalibration\n\nCalibration plot\n\n# Assuming 'pred' is your data frame with 'truth' and 'response' columns\n# For the function to work, you need to represent truth as a numeric\nRF1CC &lt;- CalibrationCurves::valProbggplot(pred$response, ifelse(pred$truth == \"1\", 1, 0))$ggPlot\n\nWarning: glm.fit: probabilidades ajustadas numericamente 0 ou 1 ocorreu\n\nRF1CC\n\n\n\n\nCalibration plot for a tuned random forest model fit to 16 predictors.\n\n\n\n\n\n\nHistogram of predicted probabilities\n\npred %&gt;%\n  ggplot(aes(response)) +\n  # geom_histogram(col = \"white\", bins = 20) +\n  geom_histogram(fill = \"orange\", col = \"orange3\", bins = 20) +\n  facet_wrap(~ as.factor(truth), ncol = 1, \n             labeller = as_labeller(c(\"0\" = \"No White Mold\", \"1\" = \"White Mold Present\"))) +\n  geom_rug(col = \"blue\", alpha = 0.5) + \n  labs(x = \"Probability estimate of white mold\", y = \"No. of fields\") +\n  theme_bw()\n\n\n\n\nHistogram of fitted probabilities for a tuned random forest model utilizing all 16 predictors.\n\n\n\n\n\n\nICI\n\nApparent\n\n# As a background check:\n# Get the apparent estimate using CalibrationCurves, which generates a flexible calibration curve based on loess as the default.\nRFcalPerf &lt;- suppressGraphics({\n  CalibrationCurves::val.prob.ci.2(pred$response, ifelse(pred$truth == \"1\", 1, 0))\n})\n\nRFcalPerf$stats[\"Eavg\"]\n\n\n# Calibration measures ICI, E50, E90 based on secondary logistic regression\n# Create a copy of the X.df data frame, with pred added as a column:\nXmod &lt;-\n  X.df %&gt;%\n  # For the loess, need wm as a numeric:\n  dplyr::mutate(wm = as.numeric(wm)-1) %&gt;% \n  dplyr::mutate(pred = pred$response)\n\n# Use loess to fit the flexible calibration curve:\nfit_cal &lt;- loess(wm ~ pred, \n                 data = Xmod, \n                 span = 0.75, \n                 degree = 2, \n                 family = \"gaussian\")\n\ncal_obs &lt;- predict(fit_cal)\n\ndt_cal &lt;- \n  cbind.data.frame(\n    \"obs\" = cal_obs,\n    \"pred\" = pred$response)\n\n\nres_calmeas &lt;-\n  c(\n    \"ICI\" = mean(abs(dt_cal$obs - dt_cal$pred)),\n    \"E50\" = median(abs(dt_cal$obs - dt_cal$pred)),\n    \"E90\" = unname(quantile(abs(dt_cal$obs - dt_cal$pred), \n                            probs = .90))\n)\n\n\n## Bootstrap confidence intervals for the calibration  measures (ICI, E50, E90)\nalpha &lt;- .05\n# Set B = 2000 although it takes more time:\nB &lt;- 2000 \nset.seed(2022)\n# Using stratification on wm because of the imbalance between the classes:\nvboot &lt;- rsample::bootstraps(X.df, times = B, strata = wm)\n\n# Bootstrap calibration measures\nnumsum_boot &lt;- function(split) {\n  \n  boot_pred &lt;- predict(rf.tuned$model, newdata = rsample::analysis(split))$data %&gt;%\n  dplyr::pull(prob.1)\n\n  v_cal &lt;- loess(wm ~ boot_pred, \n                 data = rsample::analysis(split) %&gt;% dplyr::mutate(wm = as.numeric(wm)-1), \n                 span = 0.75, \n                 degree = 2, \n                 family = \"gaussian\")\n\n  cal_obs_boot &lt;- predict(v_cal)\n  \n  # Save objects needed\n  db_cal_boot &lt;- data.frame(\n    \"obs\" = cal_obs_boot,\n    \"pred\" = boot_pred\n    )\n  \n  absdiff_boot &lt;- abs(db_cal_boot$obs - db_cal_boot$pred)\n\n  res_cal_boot &lt;- data.frame(\n    \"ICI\" = mean(absdiff_boot),\n    \"E50\" = quantile(absdiff_boot, probs = .5),\n    \"E90\" = quantile(absdiff_boot, probs = .9)\n  )\n  }  # end of function\n\n# Example of use:\n# (bar &lt;- purrr::pluck(vboot, \"splits\", 1) %&gt;% numsum_boot())\n\n\ntictoc::tic()\nnumsum_b &lt;- \n  vboot %&gt;% \n  dplyr::mutate(num_cal_boot = purrr::map(splits, numsum_boot),\n                \n                ICI = purrr::map_dbl(num_cal_boot, ~ .x$ICI),\n         \n                E50 = purrr::map_dbl(num_cal_boot, ~ .x$E50),\n         \n                E90 = purrr::map_dbl(num_cal_boot, ~ .x$E90)\n         )\ntictoc::toc()  # ~ 74 sec\n\n46.36 sec elapsed\n\n# numsum_b is a tibble with 2,000 rows\n# Now we need to process the results to get the bootstrap CIs:\nalpha &lt;- .05\nres_numcal &lt;- matrix(c(res_calmeas[\"ICI\"],\n                       quantile(numsum_b$ICI, probs = alpha / 2),\n                       quantile(numsum_b$ICI, \n                                probs = 1 - alpha / 2),\n                       \n                       res_calmeas[\"E50\"],\n                       quantile(numsum_b$E50, probs = alpha / 2),\n                       quantile(numsum_b$E50, probs = 1 - alpha / 2),\n                       \n                       res_calmeas[\"E90\"],\n                       quantile(numsum_b$E90, probs = alpha / 2),\n                       quantile(numsum_b$E90, probs = 1 - alpha / 2)\n                       ),\n                     nrow = 1,\n                     ncol = 9,\n                     byrow = T,\n                     dimnames = list(\n                       c(\"Apparent\"),\n                       rep(\n                         c(\"Estimate\", \"Lower.95\", \"Upper.95\"),\n                         3))\n)\n\n\n# Present the results:\nres_numcal %&gt;% \n  make_kable()  %&gt;% \n  add_header_above(c(\" \" = 1,  \n                     \"ICI\" = 3, \n                     \"E50\" = 3,\n                     \"E90\" = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nICI\n\n\nE50\n\n\nE90\n\n\n\n\nEstimate\nLower.95\nUpper.95\nEstimate\nLower.95\nUpper.95\nEstimate\nLower.95\nUpper.95\n\n\n\n\nApparent\n0.102\n0.093\n0.111\n0.077\n0.065\n0.086\n0.255\n0.218\n0.291",
    "crumbs": [
      "Data analysis",
      "Models for white mold"
    ]
  },
  {
    "objectID": "code_modeling.html#model-agnostic-interpretation",
    "href": "code_modeling.html#model-agnostic-interpretation",
    "title": "Models for white mold",
    "section": "Model-agnostic interpretation",
    "text": "Model-agnostic interpretation\n\nNotes\nWhen using model agnostic procedures, additional code preparation is often required. The iml (Molnar 2019) package uses purely model agnostic procedures. Consequently, we need to create a model agnostic object that contains three components:\n\nA data frame with just the features (must be of class “data.frame”).\nA vector with the actual responses (must be numeric—0/1 for binary classification problems).\nA custom function that will take the features from (1), apply the ML algorithm, and return the predicted values as a vector.\n\n\n\n# We need `X.df` from the chunk `data-setup-for-ranger-tuning`. \n\n# Refit model (with the tuned parameters) using the ranger package directly (makes life easier):\n# Reminder: the tuned parameter settings:\n# rf.tuned.pars\n\nset.seed(14092)\nm1 &lt;- ranger(wm ~ ., \n             data = X.df, \n             importance = 'permutation',\n             scale.permutation.importance = TRUE,\n             # Set the hyperparameters:\n             num.trees = rf.tuned.pars$num.trees,\n             mtry = rf.tuned.pars$mtry,\n             min.node.size = rf.tuned.pars$min.node.size, \n             sample.fraction = rf.tuned.pars$sample.fraction,\n             probability = TRUE)\n\n\n# 1) create a data frame with just the features\nfeatures &lt;- X.df %&gt;% dplyr::select(-wm)\n\n# 2) Create a vector with the actual responses\nresponse &lt;-  X.df %&gt;% dplyr::mutate(wm = ifelse(wm == \"1\", 1, 0)) %&gt;% dplyr::pull(wm)\n\n# 3) Create custom predict function that returns the predicted values as a vector\npred.fxn &lt;- function(object, newdata) {\n  results &lt;- predict(object, data = newdata, type = \"response\")$predictions[, 2]\n  return(results)\n}\n\n# Example of prediction output:\n# pred.fxn(m1, X.df) %&gt;% head()\n\n# Once we have these three components we can create our model agnostic objects for the iml package, which will just pass these downstream components (along with the ML model) to other functions.\n# iml model agnostic object:\ncomponents_iml &lt;- Predictor$new(\n  model = m1, \n  data = features, \n  y = response, \n  predict.fun = pred.fxn\n)\n\n\n\nVariable importance\nThe variable importance plot shows that not all predictors may be necessary.\n\n# You can get an importance measure from ranger:\n# ranger::importance(m1)\n\n# List of available metrics in `vip` for binary classification:\n# list_metrics() %&gt;% \n#   dplyr::filter(!task == \"Regression\") %&gt;% \n#   make_kable()\n\n\n# Here we will use the vi_permute function to set some other options, like use logloss as the metric.\ntictoc::tic()\nwm_vip &lt;- \n  vi_permute(\n    m1,\n  # metric arg calls yardstick, which expects wm = 1 (target) as the FIRST level; and as a factor:\n  train = X.df %&gt;% dplyr::mutate(wm = factor(wm, levels = c(\"1\", \"0\"))),\n  target = \"wm\",\n  metric = \"logloss\",\n  smaller_is_better = TRUE,  # see list_metrics()\n  nsim = 100,\n  pred_wrapper = pred.fxn\n)\ntictoc::toc() # 48 sec\n\n# Save the result:\nsave(wm_vip, file = here::here(\"Modeling\", \"vip.RData\"))\n\n\n# Load the vip results:\nload(here::here(\"Modeling\", \"vip.RData\"))\n\n# A named vector of the labels:\nwm_vip_labels &lt;-\n  wm_metadata %&gt;% \n  dplyr::filter(!variable %in% c(\"subject\", \"wm\", \"drainage\")) %&gt;% \n  tibble::deframe()\n\n# Plot with the more descriptive variable labels: \nwm_vip %&gt;%\n  dplyr::arrange(Importance) %&gt;% \n  dplyr::mutate(rel.imp = 100*Importance/sum(Importance)) %&gt;% \n  # use reorder() to sort the plot so highest importance is at the top:\n  ggplot(., aes(y = reorder(Variable, rel.imp), x = rel.imp)) +  \n  geom_point(size = 3, color = \"orange\") +\n  scale_y_discrete(labels = wm_vip_labels, name = NULL) +\n  # theme_minimal() +\n  theme_bw() +\n  scale_x_continuous(name = \"Relative Importance\") + \n  theme(axis.title.x = element_text(face = \"bold\", size = 11)) +\n  theme(axis.text.y = element_text(size = 8))\n\n\n\n\nPermutation-based variable importance for a tuned random forest model. Importance was computed using log loss as the performance metric.",
    "crumbs": [
      "Data analysis",
      "Models for white mold"
    ]
  },
  {
    "objectID": "code_modeling.html#tuning-on-the-selected-vars",
    "href": "code_modeling.html#tuning-on-the-selected-vars",
    "title": "Models for white mold",
    "section": "Tuning on the selected vars",
    "text": "Tuning on the selected vars\nNo output to show.\nJust presenting the code on how it was done.\n\n# mlr expects a data frame:\nclass(X.sel)  # we are good\n\n# Set seed for reproducibility:\nset.seed(5309)  # Jenny, I got your number!\n\n# For tuneRanger, a mlr task has to be created:\ncc.task &lt;- mlr::makeClassifTask(data = X.sel, target = \"wm\", positive = \"1\")\n\n# Tuning process:\ntictoc::tic()\nrf.tuned.sel &lt;- tuneRanger(cc.task, num.trees = 500, show.info = FALSE) # Reducing the no. trees as we don't have many predictors\ntictoc::toc()  # 42 sec\n\n# Save the fitted model so that you don't have to re-tune:\nsave(rf.tuned.sel, X.sel, selected_vars, file = here::here(\"Modeling\", \"rf_tuned_sel.RData\"))",
    "crumbs": [
      "Data analysis",
      "Models for white mold"
    ]
  },
  {
    "objectID": "code_modeling.html#examine-model-fit",
    "href": "code_modeling.html#examine-model-fit",
    "title": "Models for white mold",
    "section": "Examine model fit",
    "text": "Examine model fit\n\n\nload(here::here(\"Modeling\", \"rf_tuned_sel.RData\"))  # rf.tuned.sel, X.sel, selected_vars\n\n# The tuned hyperparameters:\n# rf.tuned.sel$model\n\n\nThe tuned hyperparameters\n\nrf.tuned.sel.pars &lt;-\n  list(\n    mtry = rf.tuned.sel$recommended.pars$mtry,\n    min.node.size = rf.tuned.sel$recommended.pars$min.node.size,\n    sample.fraction = rf.tuned.sel$recommended.pars$sample.fraction,\n    num.trees = rf.tuned.sel$model$learner$par.vals$num.trees\n    ) \n\nrf.tuned.sel.pars %&gt;%\n  as_tibble() %&gt;%\n  make_kable()\n\n\n\n\n\nmtry\nmin.node.size\nsample.fraction\nnum.trees\n\n\n\n\n1\n1\n6\n0.455\n500\n\n\n\n\n\n\n\n\n\nEstimating optimism\n\n# 1. Prepare predictions dataframe:\nsel_pred &lt;-\n  predict(rf.tuned.sel$model, newdata = X.sel)$data %&gt;%\n  dplyr::select(truth, prob.1) %&gt;%\n  dplyr::rename(response = prob.1)\n\n# 2. Calculate apparent performance\nsel_apparent_perf &lt;- calc_metrics_cutpointr(sel_pred)\n\n# 3. Bootstrap process\nbootstrap_fxn &lt;- function(i) {\n  # Print the current iteration\n  cat(\"Currently processing iteration: \", i, \"\\n\")\n  \n  # a. Generate bootstrap sample\n  boot_sample &lt;- \n    rsample::bootstraps(X.sel, times = 1, strata = wm)$splits[[1]] %&gt;%\n    rsample::analysis()\n  \n  # b. Create classification task and fit model on bootstrap sample with error handling\n  cc.task &lt;- mlr::makeClassifTask(data = boot_sample, target = \"wm\", positive = \"1\")\n  \n  boot_model &lt;- tryCatch(\n    expr = {\n      tuneRanger::tuneRanger(cc.task, num.trees = 500, show.info = FALSE)$model\n    },\n    error = function(e) {\n      cat(\"Error occurred in iteration\", i, \":\", e$message, \"\\n\")\n      return(NULL) # Return NULL if an error occurs\n    }\n  )\n  \n  # If boot_model is NULL, skip this iteration\n  if (is.null(boot_model)) {\n    cat(\"Skipping iteration\", i, \"due to error.\\n\")\n    return(NULL)\n  }\n  \n  # c. Calculate metrics on bootstrap sample (training performance)\n  boot_pred &lt;- \n    predict(boot_model, newdata = boot_sample)$data %&gt;%\n    dplyr::select(truth, prob.1) %&gt;%\n    dplyr::rename(response = prob.1)\n  \n  train_perf &lt;- calc_metrics_cutpointr(boot_pred)\n  \n  # d & e. Apply bootstrap model to original dataset and calculate metrics (test performance)\n  test_pred &lt;- predict(boot_model, newdata = X.sel)$data %&gt;%\n    dplyr::select(truth, prob.1) %&gt;%\n    dplyr::rename(response = prob.1)\n  \n  test_perf &lt;- calc_metrics_cutpointr(test_pred)\n  \n  # f. Calculate optimism\n  optimism &lt;- purrr::map2_dbl(train_perf, test_perf, \\(x, y) x-y)\n  \n  tibble(\n    iteration = i,\n    metric = names(train_perf),\n    train = unlist(train_perf),\n    test = unlist(test_perf),\n    optimism = optimism)\n  }\n\nset.seed(123)\n# Run bootstrap_fxn in parallel:\nmy.seeds &lt;- c(1:1000)\n\nplan(list(tweak(multisession, workers = 5), sequential))\ntictoc::tic()\nbootstrap_results &lt;- \n  furrr::future_map(my.seeds, bootstrap_fxn, .options = furrr_options(seed = TRUE)) %&gt;% \n  purrr::list_rbind()\ntictoc::toc()  # 9755.02 sec ~ 2.7 hr\n\nplan(sequential)\n\n\n# 4. Apparent performance bootstrap HDI\napparent_CI &lt;-\n  bootstrap_results %&gt;%\n  dplyr::group_by(metric) %&gt;%\n  dplyr::summarize(app_LCI = bayestestR::hdi(train, ci = 0.95)$CI_low, \n                   app_UCI = bayestestR::hdi(train, ci = 0.95)$CI_high)\n\n\n# 5. Average optimism across all bootstrap samples\navg_optimism &lt;- \n  bootstrap_results %&gt;%\n  dplyr::group_by(metric) %&gt;%\n  dplyr::summarize(avg_optimism = mean(optimism))\n\n# 6. Calculate optimism-corrected performance\nsel_corrected_perf &lt;- \n  tibble(\n    metric = names(sel_apparent_perf),\n    apparent = unlist(sel_apparent_perf)\n    ) %&gt;%\n  dplyr::left_join(apparent_CI, by = \"metric\") %&gt;%\n  dplyr::left_join(avg_optimism, by = \"metric\") %&gt;%\n  dplyr::mutate(corrected = apparent - avg_optimism)\n\n\n# Save the results so that you don't have to ever, ever repeat this time-consuming process:\nsave(sel_pred, sel_apparent_perf, bootstrap_results, sel_corrected_perf, file = here::here(\"Modeling\", \"RFseloptimism.RData\"))\n\n\n# Load the model-fitted probs and the optimism estimates:\n# sel_pred, sel_apparent_perf, bootstrap_results, sel_corrected_perf\nload(here::here(\"Modeling\", \"RFseloptimism.RData\"))  \n\nsel_corrected_perf %&gt;% \n  make_kable()\n\n\n\n\n\nmetric\napparent\napp_LCI\napp_UCI\navg_optimism\ncorrected\n\n\n\n\n1\nYI\n0.915\n0.972\n1.000\n0.198\n0.717\n\n\n2\nSe\n1.000\n0.986\n1.000\n0.122\n0.878\n\n\n3\nSp\n0.915\n0.975\n1.000\n0.076\n0.839\n\n\n4\nAUC\n0.987\n0.999\n1.000\n0.053\n0.934\n\n\n5\nbrier\n0.065\n0.002\n0.014\n-0.052\n0.117\n\n\n\n\n\n\n\n\n\nC-statistic\nThe AUROC is still extremely high, even with this reduced set of predictors.\n\n# Get the C-statistic (AUROC):\n# Assuming your data frame is called 'sel_pred' with columns 'truth' and 'response'\nroc_obj &lt;- pROC::roc(sel_pred$truth, sel_pred$response, levels = c(0, 1))\n\n# 95% CI for the C-statistic:\nci_obj &lt;- ci.auc(roc_obj)\n\nres_C &lt;- matrix(\n  c(ci_obj[2],\n    ci_obj[1],\n    ci_obj[3],\n    sel_corrected_perf %&gt;% dplyr::filter(metric == \"AUC\") %&gt;% dplyr::pull(corrected) %&gt;% unname(),\n    NA,\n    NA),\n  \n  nrow = 1,\n  ncol = 6,\n  byrow = T,\n  \n  dimnames = list(\n    c(\"C-statistic\"),\n    rep(c(\"Estimate\", \"Lower .95 \", \"Upper .95\"), 2)\n  )\n)\n\nres_C %&gt;% \n  make_kable() %&gt;% \n  add_header_above(c(\" \" = 1, \n                     \"Apparent\" = 3, \n                     \"Internal\" = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApparent\n\n\nInternal\n\n\n\n\nEstimate\nLower .95\nUpper .95\nEstimate\nLower .95\nUpper .95\n\n\n\n\nC-statistic\n0.987\n0.979\n0.995\n0.934\nNA\nNA\n\n\n\n\n\n\n\n\n\nBrier score\n\n# Brier score:\ntibble::tibble(`Brier (apparent)` = sel_corrected_perf %&gt;% dplyr::filter(metric == \"brier\") %&gt;% dplyr::pull(apparent) %&gt;% unname(), \n               `Brier (internal)` = sel_corrected_perf %&gt;% dplyr::filter(metric == \"brier\") %&gt;% dplyr::pull(corrected) %&gt;% unname()) %&gt;% \n  make_kable()\n\n\n\n\n\nBrier (apparent)\nBrier (internal)\n\n\n\n\n1\n0.065\n0.117\n\n\n\n\n\n\n\n\n\nCalibration\n\nCalibration plot\nIndicates:\n\npoorly calibrated predicted probabilities\n\nThis model may be useful for explanation; that is, how the variables in the model influence or determine the predicted outcome. We cannot interpret such relationships causally.\nWould not be advisable to use this model for predicted probability, because of the poor calibration, unless it can be re-calibrated.\nFor more information on calibration and re-calibration approaches, see this tidymodels post.\nIt may still be useful for discrimination.\n\n# Assuming 'sel_pred' is your data frame with 'truth' and 'response' columns\ncalibration_plot(\n  data = sel_pred %&gt;% dplyr::mutate(truth = ifelse(truth == \"1\", 1, 0)),\n  obs = \"truth\",\n  pred = \"response\",\n  title = \"Calibration Plot for Random Forest Model\",\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\"\n)\n\n# This is a nicer format I think:\nRF2CC &lt;- CalibrationCurves::valProbggplot(sel_pred$response, ifelse(sel_pred$truth == \"1\", 1, 0))$ggPlot\n\nRF2CC\n\n\n\n\nCalibration plot for a tuned random forest model fit to a selected subset of the predictors. The subset was identified by the VSURF algorithm.\n\n\n\n\n\n\nHistogram of predicted probabilities\n\nsel_pred %&gt;%\n  ggplot(aes(response)) +\n  geom_histogram(fill = \"orange\", col = \"orange3\", bins = 20) +\n  facet_wrap(~ as.factor(truth), ncol = 1, \n             labeller = as_labeller(c(\"0\" = \"No White Mold\", \"1\" = \"White Mold Present\"))) +\n  geom_rug(col = \"blue\", alpha = 0.5) + \n  labs(x = \"Probability estimate of white mold\", y = \"No. of fields\") +\n  theme_bw()\n\n\n\n\nHistogram of fitted probabilities for a tuned random forest model utilizing a subset of predictors identified by the VSURF algorithm.\n\n\n\n\n\n\nICI\n\nApparent\n\n# As a background check:\n# Get the apparent estimate using CalibrationCurves, which generates a flexible calibration curve based on loess as the default.\nRFsel_calPerf &lt;- suppressGraphics({\n  CalibrationCurves::val.prob.ci.2(sel_pred$response, ifelse(sel_pred$truth == \"1\", 1, 0))\n})\n\nRFsel_calPerf$stats[\"Eavg\"]\n\n\n# Calibration measures ICI, E50, E90 based on secondary logistic regression\n# Create a copy of the X.sel data frame, with sel_pred added as a column:\nXmod &lt;-\n  X.sel %&gt;%\n  # For the loess, need wm as a numeric:\n  dplyr::mutate(wm = as.numeric(wm)-1) %&gt;% \n  dplyr::mutate(pred = sel_pred$response)\n\n# Use loess to fit the flexible calibration curve:\nfit_cal &lt;- loess(wm ~ pred, \n                 data = Xmod, \n                 span = 0.75, \n                 degree = 2, \n                 family = \"gaussian\")\n\ncal_obs &lt;- predict(fit_cal)\n\ndt_cal &lt;- \n  cbind.data.frame(\n    \"obs\" = cal_obs,\n    \"pred\" = sel_pred$response)\n\n\nres_calmeas &lt;-\n  c(\n    \"ICI\" = mean(abs(dt_cal$obs - dt_cal$pred)),\n    \"E50\" = median(abs(dt_cal$obs - dt_cal$pred)),\n    \"E90\" = unname(quantile(abs(dt_cal$obs - dt_cal$pred), \n                            probs = .90))\n)\n\n\n## Bootstrap confidence intervals for the calibration  measures (ICI, E50, E90)\nalpha &lt;- .05\n# Set B = 2000 although it takes more time:\nB &lt;- 2000 \nset.seed(2022)\n# Using stratification on wm because of the imbalance between the classes:\nvboot &lt;- rsample::bootstraps(X.sel, times = B, strata = wm)\n\n# Bootstrap calibration measures\nnumsum_boot &lt;- function(split) {\n  \n  boot_pred &lt;- predict(rf.tuned.sel$model, newdata = rsample::analysis(split))$data %&gt;%\n  dplyr::pull(prob.1)\n\n  v_cal &lt;- loess(wm ~ boot_pred, \n                 data = rsample::analysis(split) %&gt;% dplyr::mutate(wm = as.numeric(wm)-1), \n                 span = 0.75, \n                 degree = 2, \n                 family = \"gaussian\")\n\n  cal_obs_boot &lt;- predict(v_cal)\n  \n  # Save objects needed\n  db_cal_boot &lt;- data.frame(\n    \"obs\" = cal_obs_boot,\n    \"pred\" = boot_pred\n    )\n  \n  absdiff_boot &lt;- abs(db_cal_boot$obs - db_cal_boot$pred)\n\n  res_cal_boot &lt;- data.frame(\n    \"ICI\" = mean(absdiff_boot),\n    \"E50\" = quantile(absdiff_boot, probs = .5),\n    \"E90\" = quantile(absdiff_boot, probs = .9)\n  )\n  }  # end of function\n\n# Example of use:\n# (bar &lt;- purrr::pluck(vboot, \"splits\", 1) %&gt;% numsum_boot())\n\n\ntictoc::tic()\nnumsum_b &lt;- \n  vboot %&gt;% \n  dplyr::mutate(num_cal_boot = purrr::map(splits, numsum_boot),\n                \n                ICI = purrr::map_dbl(num_cal_boot, ~ .x$ICI),\n         \n                E50 = purrr::map_dbl(num_cal_boot, ~ .x$E50),\n         \n                E90 = purrr::map_dbl(num_cal_boot, ~ .x$E90)\n         )\ntictoc::toc()  # ~ 74 sec\n\n23.74 sec elapsed\n\n# numsum_b is a tibble with 2,000 rows\n# Now we need to process the results to get the bootstrap CIs:\nalpha &lt;- .05\nres_numcal &lt;- matrix(c(res_calmeas[\"ICI\"],\n                       quantile(numsum_b$ICI, probs = alpha / 2),\n                       quantile(numsum_b$ICI, \n                                probs = 1 - alpha / 2),\n                       \n                       res_calmeas[\"E50\"],\n                       quantile(numsum_b$E50, probs = alpha / 2),\n                       quantile(numsum_b$E50, probs = 1 - alpha / 2),\n                       \n                       res_calmeas[\"E90\"],\n                       quantile(numsum_b$E90, probs = alpha / 2),\n                       quantile(numsum_b$E90, probs = 1 - alpha / 2)\n                       ),\n                     nrow = 1,\n                     ncol = 9,\n                     byrow = T,\n                     dimnames = list(\n                       c(\"Apparent\"),\n                       rep(\n                         c(\"Estimate\", \"Lower.95\", \"Upper.95\"),\n                         3))\n)\n\n\n# Present the results:\nres_numcal %&gt;% \n  make_kable()  %&gt;% \n  add_header_above(c(\" \" = 1,  \n                     \"ICI\" = 3, \n                     \"E50\" = 3,\n                     \"E90\" = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nICI\n\n\nE50\n\n\nE90\n\n\n\n\nEstimate\nLower.95\nUpper.95\nEstimate\nLower.95\nUpper.95\nEstimate\nLower.95\nUpper.95\n\n\n\n\nApparent\n0.124\n0.114\n0.135\n0.101\n0.093\n0.115\n0.301\n0.263\n0.346\n\n\n\n\n\n\n\n\n\n\nComparison to the RF1 model\n\n(RF1CC + RF2CC) +\n  plot_annotation(tag_levels = \"A\") +\n  plot_layout(nrow = 2, byrow = TRUE, guides = \"collect\") & \n  cowplot::theme_half_open(font_size = 12) &\n  theme(legend.position = \"bottom\", \n        legend.direction = \"horizontal\")\n\n\n\n\n\n\n\nggsave(\"figs/RF_calibration_plot.png\", dpi = 600, height = 7, width = 5, bg = \"white\" )\nggsave(\"figs/RF_calibration_plot.pdf\", dpi = 600, height = 7, width = 5, bg = \"white\" )\n\n\n\n\nRe-Calibration\nHere we just use simple Platt scaling manually. NOTE: Platt scaling assumes a sigmoid relationship between the original probabilities and the true probabilities. If this assumption doesn’t hold for your data, you might want to consider other calibration methods like isotonic regression (which we do not pursue here).\nYou can see that a few things: (i) one’s assessment of calibration can be biased by the graphical output of the program used to generate the calibration plot. The simpler plot is generated by the calibration_plot function in the predtools package. It uses a simple form of binning. The second plot is generated by the valProbggplot function in the CalibrationCurves, which is based on functions from the rms package, and provides some additional statistics with the plot.\nWhile the slope has improved, we can see that the re-calibrated probs are closer to the extremes of 0 and 1.\nIn summary, this short exercise shows that re-calibration does require some work – Platt scaling works partially here. The small sample size here is also an issue that may be having an effect.\n\nRe-calibration plot\n\npredtools package\n\n# Fit a logistic regression model to the response using the predicted probs from the RF model: \nplatt_model &lt;- glm(truth ~ response, data = sel_pred, family = \"binomial\")\n\n# Apply calibration to the data:\ncalibrated_probs &lt;- predict(platt_model, newdata = data.frame(response = sel_pred$response), type = \"response\")\n\ncalibrated_data &lt;- data.frame(truth = sel_pred$truth, response = calibrated_probs)\n\ncalibration_plot(\n  data = calibrated_data %&gt;% dplyr::mutate(truth = ifelse(truth == \"1\", 1, 0)),\n  obs = \"truth\",\n  pred = \"response\",\n  title = \"Re-Calibration Plot for Random Forest Model\",\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\"\n)\n\n\n\n\nCalibration plot for the recalibrated random forest model, plotted using the predtools package\n\n\n\n\n\n\nCalibrationCurves package\n\n# This is a nicer format I think:\nCalibrationCurves::valProbggplot(calibrated_data$response, ifelse(calibrated_data$truth == \"1\", 1, 0))$ggPlot\n\n\n\n\nCalibration plot for the recalibrated random forest model, plotted using the CalibrationCurves package\n\n\n\n\n\n\n\nHistogram of calibrated probabilities\n\ncalibrated_data %&gt;%\n  ggplot(aes(response)) +\n  geom_histogram(fill = \"orange\", col = \"orange3\", bins = 20) +\n  facet_wrap(~ as.factor(truth), ncol = 1, \n             labeller = as_labeller(c(\"0\" = \"No White Mold\", \"1\" = \"White Mold Present\"))) +\n  geom_rug(col = \"blue\", alpha = 0.5) + \n  labs(x = \"Calibrated probability estimate of white mold\") +\n  theme_bw()",
    "crumbs": [
      "Data analysis",
      "Models for white mold"
    ]
  },
  {
    "objectID": "code_modeling.html#model-agnostic-interpretation-1",
    "href": "code_modeling.html#model-agnostic-interpretation-1",
    "title": "Models for white mold",
    "section": "Model-agnostic interpretation",
    "text": "Model-agnostic interpretation\n\n\n# Reminder: the tuned params:\nrf.tuned.sel.pars\n\n\nset.seed(14092)\nm2 &lt;- ranger(wm ~ ., \n             data = X.sel, \n             importance = 'permutation',\n             scale.permutation.importance = TRUE,\n             # Set the hyperparameters:\n             num.trees = rf.tuned.sel.pars$num.trees,\n             mtry = rf.tuned.sel.pars$mtry,\n             min.node.size = rf.tuned.sel.pars$min.node.size, \n             sample.fraction = rf.tuned.sel.pars$sample.fraction,\n             probability = TRUE)\n\n\n\n# 1) create a data frame with just the features\nfeatures &lt;- X.sel %&gt;% dplyr::select(-wm)\n\n# 2) Create a vector with the actual responses\nresponse &lt;-  X.sel %&gt;% dplyr::mutate(wm = ifelse(wm == \"1\", 1, 0)) %&gt;% dplyr::pull(wm)\n\n# 3) Create custom predict function that returns the predicted values as a vector\npred.fxn &lt;- function(object, newdata) {\n  results &lt;- predict(object, data = newdata, type = \"response\")$predictions[, 2]\n  return(results)\n}\n\n# Example:\n# pred.fxn(m2, X.sel) %&gt;% head()\n\n# Once we have these three components we can create our model agnostic objects for the iml package, which will just pass these downstream components (along with the ML model) to other functions.\n# iml model agnostic object:\ncomponents_iml &lt;- Predictor$new(\n  model = m2, \n  data = features, \n  y = response, \n  predict.fun = pred.fxn\n)\n\n\nVariable importance\n\n# Here we will use the vi_permute function to set some other options, like use logloss as the metric.\nwm_vip &lt;- \n  vi_permute(\n    m2,\n  # metric arg calls yardstick, which expects wm = 1 (target) as the FIRST level; and as a factor:\n  train = X.sel %&gt;% dplyr::mutate(wm = factor(wm, levels = c(\"1\", \"0\"))),\n  target = \"wm\",\n  metric = \"logloss\",\n  smaller_is_better = TRUE,  # see list_metrics()\n  nsim = 10,\n  pred_wrapper = pred.fxn\n)\n\n# A named vector of the labels:\nwm_vip_labels &lt;-\n  wm_metadata %&gt;% \n  dplyr::filter(variable %in% selected_vars) %&gt;% \n  tibble::deframe()\n\n# Plot with the more descriptive variable labels: \nwm_vip %&gt;%\n  dplyr::arrange(Importance) %&gt;% \n  dplyr::mutate(rel.imp = 100*Importance/sum(Importance)) %&gt;% \n  # use reorder() to sort the plot so highest importance is at the top:\n  ggplot(., aes(y = reorder(Variable, rel.imp), x = rel.imp)) +  \n  geom_point(size = 3, color = \"orange\") +\n  scale_y_discrete(labels = wm_vip_labels, name = NULL) +\n  theme_bw() +\n  scale_x_continuous(name = \"Relative Importance\") + \n  theme(axis.title.x = element_text(face = \"bold\", size = 11))\n\n\n\n\nPermutation-based variable importance for a tuned random forest model fit to a VSURF-identified subset of variables. Importance was computed using log loss as the performance metric.\n\n\n\n\n\n\nPartial dependence\nI think the main point these plots demonstrate is the non-linearity in the relationship between the response and the predictors. On the probability scale (y-axis), we would expect a S-shaped curve. If the logit were plotted on the y-axis instead, the relationships should appear linear.\n\nProbability scale\n\n# Reminder: the set of vars\n# selected_vars\n\n# If grid.resolution left NULL, it will default to the minimum between 51 and the number of unique data points for each of the continuous independent variables listed in pred.var. Therefore, let's get an idea of the number of unique data points for each var. I will use this info to set grid.resolution to a value that will be above the minimum of 51 grid points, but not too high to slow down computation.\n\n# length(unique(X.sel$t2m_mean_to_4dap))     # 297\n# length(unique(X.sel$sm_40dap_to_49dap))    # 297\n# length(unique(X.sel$stsm_35dap_to_44dap))  # 297\n# length(unique(X.sel$sm_5dap_to_15dap))     # 297\n# length(unique(X.sel$rain36to50dap))        # 213\n# length(unique(X.sel$rainto35dap))          # 210\n# length(unique(X.sel$log_silt_clay))        # 305\n\n# Compute partial dependence values. Going to write a wrapper function to get the partial values.\n# NOTE: there is a bug in the pdp::partial function. The prob arg always returns probabilities, despite the default being FALSE. \n\nget.pd.df &lt;- function(var, getprob) {\n  # Get the partial dependence values on the probability scale\n  # Args:\n  #  var = quoted character string of the predictor's name\n  #  getprob = logical to be passed to the prob argument of the partial function\n  # Returns:\n  #  a data frame with columns labelled `predvar` and `yhat`\n  \n  partial(m2, \n         train = X.sel,\n         pred.var = var, \n         plot = FALSE,\n         chull = TRUE, \n         progress = FALSE,\n         type = \"classification\",  # the type of model we have\n         which.class = 2,          # Pr(wm=1) is in the 2nd column of probs returned by ranger\n         grid.resolution = 100,\n         prob = getprob) %&gt;% \n    # Rename the 1st column, which automatically gets the var's name, to predvar\n    # Doing this to make it easier (later on) to pass this data frame to a wrapper plotting function\n    dplyr::rename_with(~ \"predvar\", .cols = all_of(var))\n}\n  \n\nmy.pdp.plot &lt;- function(var) {\n  # Partial dependence plot\n  # Args:\n  #  var = quoted character string of the predictor's name\n  # Returns:\n  #  a ggplot representation of the pdp\n  \n  pd &lt;- get.pd.df(var, getprob = T)\n  \n  ggplot(pd, aes(x = predvar, y = yhat)) +\n  geom_line(color = \"orange\", linewidth = 1) +\n  geom_rug(color = \"grey50\", alpha = 0.5, sides = \"l\") +\n  labs(x = wm_vip_labels[var],\n       y = \"Marginal Probability\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_line(color = \"gray90\"),\n        panel.grid.minor = element_line(color = \"gray95\"),\n        # Make x-axis labels smaller to reduce crowding:\n        axis.title.x = element_text(face = \"bold\", size = 6),\n        axis.title.y = element_text(face = \"bold\", size = 8))\n}\n\n# Do the pdp's for the 7 vars: \np1 &lt;- my.pdp.plot(\"t2m_mean_to_4dap\")\np2 &lt;- my.pdp.plot(\"sm_40dap_to_49dap\")\np3 &lt;- my.pdp.plot(\"stsm_35dap_to_44dap\")\np4 &lt;- my.pdp.plot(\"sm_5dap_to_15dap\")\np5 &lt;- my.pdp.plot(\"rain36to50dap\")\np6 &lt;- my.pdp.plot(\"rainto35dap\")\np7 &lt;- my.pdp.plot(\"log_silt_clay\")\n\n# Combine the plots, collect the common y-axis labels:\np1 + p2 + p3 + p4 + p5 + p6 + p7 + plot_layout(ncol = 3, axis_titles = \"collect\")\n\n\n\n\nPartial dependence plots for the VSURF-selected variables used in fitting a random forest model for predicting white mold in snap bean. The y-axis is on the probability scale.\n\n\n\n\n\n\nLogit scale\nIn the partial dependence function, the values are on a centered logit scale, a scale similar to the logit.\n\nmy.pdp.plot.logit &lt;- function(var) {\n  # Partial dependence plot on the logit scale\n  # Args:\n  #  var = quoted character string of the predictor's name\n  # Returns:\n  #  a ggplot representation of the pdp\n  \n  pd &lt;- get.pd.df(var, getprob = FALSE)\n\n  ggplot(pd, aes(x = predvar, y = yhat)) +\n  geom_line(color = \"orange\", linewidth = 1) +\n  geom_rug(color = \"grey50\", alpha = 0.5, sides = \"l\") +\n  labs(x = wm_vip_labels[var],\n       y = \"Centered logit\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_line(color = \"gray90\"),\n        panel.grid.minor = element_line(color = \"gray95\"),\n        # Make x-axis labels smaller to reduce crowding:\n        axis.title.x = element_text(face = \"bold\", size = 6),\n        axis.title.y = element_text(face = \"bold\", size = 8))\n}\n\n\n\np1 &lt;- my.pdp.plot.logit(\"t2m_mean_to_4dap\")\np2 &lt;- my.pdp.plot.logit(\"sm_40dap_to_49dap\")\np3 &lt;- my.pdp.plot.logit(\"stsm_35dap_to_44dap\")\np4 &lt;- my.pdp.plot.logit(\"sm_5dap_to_15dap\")\np5 &lt;- my.pdp.plot.logit(\"rain36to50dap\")\np6 &lt;- my.pdp.plot.logit(\"rainto35dap\")\np7 &lt;- my.pdp.plot.logit(\"log_silt_clay\")\n\n# Combine the plots, collect the common y-axis labels:\np1 + p2 + p3 + p4 + p5 + p6 + p7 + plot_layout(ncol = 3, axis_titles = \"collect\")\n\n\n\n\nPartial dependence plots for the VSURF-selected variables used in fitting a random forest model for predicting white mold in snap bean. The y-axis is on the logit scale.\n\n\n\n\n\n\n\nFeature interactions\nEstimate the interaction between any two features, which tells us how strongly two specific features interact with each other in the model.\n\nH-statistics\nThe variable with the strongest interaction signal is the soil temperature to volumetric soil water content from 35 to 44 dap (stsm_35dap_to_44dap).\n\n# Set the seed for reproducibility and increase the grid size from the default. The interactions are not strong, and without the seed set, there is variability in the results.  The increased grid size also helps mitigate against the variability. \nset.seed(456)\ntictoc::tic()\ninteract &lt;- iml::Interaction$new(components_iml, grid.size = 100)\n\nWarning: pacote 'TH.data' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'DiceKriging' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'sandwich' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'zoo' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'gt' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'rprojroot' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'randomForest' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'here' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'withr' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'doParallel' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'htmlTable' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'R.utils' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'parallelMap' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'Metrics' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'markdown' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'bookdown' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'svglite' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'mvtnorm' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'fastmatch' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'multcomp' foi compilado no R versão 4.4.2\n\n\nWarning: pacote 'cards' foi compilado no R versão 4.4.2\n\ntictoc::toc()  # ~ 11 sec\n\n# Only 7 rows of course, as there are only 7 vars:\niml.inter.results &lt;- interact$results\n\n# Reminder: Vector of the vars:\nselected_vars\n\n# A named vector of the labels:\ninter_labels &lt;-\n  wm_metadata %&gt;% \n  dplyr::filter(variable %in% selected_vars) %&gt;% \n  tibble::deframe()\n\n# Plot with the more descriptive variable labels: \niml.inter.results %&gt;%\n  dplyr::arrange(desc(.interaction)) %&gt;% \n  # use reorder() to sort the plot so highest importance is at the top:\n  ggplot(., aes(y = reorder(.feature, .interaction), x = .interaction)) +  \n  geom_point(size = 3, color = \"orange\") +\n  scale_y_discrete(labels = inter_labels, name = NULL) +\n  theme_minimal() +\n  scale_x_continuous(name = \"Overall interaction strength\", limits = c(0, 0.35)) + \n  theme(axis.title.x = element_text(face = \"bold\", size = 11))\n\n\n\n\nH-statistic for interactions among the variables in a random forest model for forecasting white mold in snap bean.\n\n\n\n# Highest:\n# stsm_35dap_to_44dap\n\n\n\nstsm_35dap_to_44dap\nNow we look at the interaction between stsm_35dap_to_44dap and all other features, which tells us how strongly (in total) stsm_35dap_to_44dap interacts in the model with all the other features.\nWe see that stsm_35dap_to_44dap interacts mostly with sm_5dap_to_15dap and rainto35dap.\n\ninteract_2way &lt;- iml::Interaction$new(components_iml, feature = \"stsm_35dap_to_44dap\")\n\ninteract_2way$results %&gt;% \n  dplyr::arrange(desc(.interaction)) %&gt;% \n  make_kable()\n\n\n\n\n\n.feature\n.interaction\n\n\n\n\n1\nsm_5dap_to_15dap:stsm_35dap_to_44dap\n0.329\n\n\n2\nrainto35dap:stsm_35dap_to_44dap\n0.288\n\n\n3\nrain36to50dap:stsm_35dap_to_44dap\n0.136\n\n\n4\nlog_silt_clay:stsm_35dap_to_44dap\n0.115\n\n\n5\nsm_40dap_to_49dap:stsm_35dap_to_44dap\n0.114\n\n\n6\nt2m_mean_to_4dap:stsm_35dap_to_44dap\n0.112\n\n\n\n\n\n\n\n\nsm_5dap_to_15dap and stsm_35dap_to_44dap\n\n# Two-way PDP using iml:\ninteraction_pdp &lt;- iml::FeatureEffect$new(\n  components_iml, \n  c(\"sm_5dap_to_15dap\", \"stsm_35dap_to_44dap\"), \n  method = \"pdp\",\n  grid.size = 30\n) \n\n# Default output is a heatmap:\n# plot(interaction_pdp)\n\n# A wireframe plot is more informative:\n# See: https://www.stat.math.ethz.ch/pipermail/r-help/2009-August/400463.html\n# for how to make adjustments to the tick, axis and legend labels.\nwireframe(.value ~ sm_5dap_to_15dap*stsm_35dap_to_44dap, \n          data = interaction_pdp$results,\n          # Adjust the size of the axis labels:\n          xlab = list(cex = .7),\n          ylab = list(cex = .7),\n          zlab = list(label = \"Pr(wm)\", cex = 0.7),\n          main = NULL,\n          drape = TRUE,\n          colorkey = TRUE,\n          screen = list(z = -60, x = -60),\n          # Change tick label size on the x, y, z axes:\n          scales = list(arrows = FALSE, x = list(cex = .5), \n                        y = list(cex = .5), z = list(cex = 0.5)),\n          # Change the tick label size on the legend:\n          par.settings = list(axis.text = list(cex = 0.5))\n          )\n\n\n\n\n\n\n\n\n\n\n\n\nSHAP – (SHapley Additive exPlanations)\n\n# Select background data. \n# Approach 1\n#  Because of the imbalance in the data, create a subsample which consists of all wm(1) obs, plus 127 wm(0) obs, to give a sample of 200 rows:\n# set.seed(461)\n# bg_X &lt;- bind_rows(X.sel %&gt;% dplyr::filter(wm == 1),\n#           X.sel %&gt;% dplyr::filter(wm == 0) %&gt;% dplyr::slice_sample(n = 127))\n\n# Crunch SHAP values for all 200 rows:\n# Note: Since the number of features is small, we use permshap()\n# tic()\n# ps &lt;- kernelshap::permshap(m2, X.sel[-8], bg_X = bg_X)\n# toc()  # ~ 1 min\n\n\n# Approach 2 (which we use here)\n# Using all the data:\ntictoc::tic()\n# ps &lt;- kernelshap::permshap(m2, X.sel[-8], bg_X = X.sel)  # wm is in the 8th column\n# Setting it up this way, to keep the wm variable for labelling dependence plots later\nps &lt;- kernelshap::permshap(m2, X.sel, bg_X = X.sel, feature_names = names(X.sel))\ntictoc::toc() # ~ 214 sec\n\n# Create the viz object:\nsv &lt;- shapviz(ps)\n\n# Save the object to avoid having to repeat recreating the sv object:\nsave(sv, file = here::here(\"Modeling\", \"shapvis.RData\"))\n\n\nload(here::here(\"Modeling\", \"shapvis.RData\"))  # sv\n\n\nSHAP Importance Plots\nA beeswarm plot (sometimes called “SHAP summary plot”).\nThe features are sorted in decreasing order of importance.\n\n# wm_vip_labels is defined in chunk `sel-RF-model-vip`\n# With all 356 data points, the beeswarm plot can look too cluttered, can't tell much. So set a transparency via alpha = 0.5 and smaller points.\n\n# We need some further manipulations so that the plot does not include a row for wm. \n# sv is a list of two: 0 and 1. We want the values under 1 corresponding to wm=1\n# The list under 1 consists of two parts: S (matrix format), X (dataframe)\n# We need to remove the wm parts from S and X.\n# Make and work with a copy of sv:\nsv.mod &lt;- sv\n\nsv.mod[[\"1\"]]$S &lt;- sv.mod[[\"1\"]]$S[, -8]  # because S is a matrix\nsv.mod[[\"1\"]]$X &lt;- sv.mod[[\"1\"]]$X[-8]    # because X is a dataframe\n\nsv_importance(sv.mod[[\"1\"]], kind = \"bee\", alpha = 1, size = 1) +\n  scale_y_discrete(labels = wm_vip_labels, name = NULL) +\n  scale_color_gradient(low =\"#1e88e5ff\", high = \"#ff0d57ff\",limits =c(0,1),\n                       breaks = c(0, 1), labels = c(\" Low\", \"High \"),\n                       guide = guide_colorbar(barwidth = 0.3,barheight =15 ))+\n  cowplot::theme_half_open(font_size = 12)\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\nPermutation-based SHAP summary plot depicting the contributions of variables in a random forest model to the prediction of white mold in snap bean.\n\n\n\nggsave(\"figs/shap_beeswarm.png\", dpi = 600, height = 5, width = 7, bg = \"white\")\nggsave(\"figs/shap_beeswarm.pdf\", dpi = 600, height = 5, width = 7, bg = \"white\")\n\n\n\nSHAP Dependence Plot\nWe focus here on pure main effects\n\n# Reminder of the features:\n# names(X.sel)\n\n# Examples:\n# sv_dependence(sv[[\"1\"]], v = \"sm_40dap_to_49dap\", color_var = \"stsm_35dap_to_44dap\")\n# sv_dependence(sv[[\"1\"]], v = \"sm_40dap_to_49dap\", color_var = \"rainto35dap\")\n\n# A wrapper function to do a main dependence plot (no interactions shown)\nmy_dependence_fxn &lt;- function(i) {\n  # Args:\n  #  i = numeric index 1-7 to pull out a var from the respective vector\n  # Returns:\n  #  an edited dependence plot\n  \n  sv_dependence(sv[[\"1\"]], selected_vars[i], color_var = NULL, color = \"orange\", size=1) +\n  geom_smooth(method = \"loess\", formula = 'y ~ x', se = FALSE) +\n  xlab(wm_vip_labels[selected_vars[i]]) + \n  theme_bw() +\n  theme(axis.title.y = element_text(face = \"bold\", size = 8)) +\n  theme(\n        axis.text.x = element_text(size = 7),  # Decrease x-axis tick label size\n        axis.text.y = element_text(size = 7)  # Decrease y-axis tick label size\n        ) +\n  theme(axis.title.x = element_text(face = \"bold\", size = 8)) \n}\n\n\n# The main goal is to color the points in the plot by wm status (i.e., use separate colors for the wm=0 and wm=1 groups)\n\n# The best way to do this was to extract the relevant parts out of sv and create a tibble that can be passed directly into ggplot.\n\nmy_dependence_fxn &lt;- function(i) {\n  # Args:\n  #  i = numeric index 1-7 to pull out a var from the respective vector\n  # Returns:\n  #  an edited dependence plot\n  \n  # Define a colorblind-friendly palette\n  cbPalette &lt;- c(\"#999999\", \"#E69F00\")  # Two colors for binary factor levels\n  \n  foo &lt;- tibble(x = sv[[\"1\"]]$X[, i], y = sv[[\"1\"]]$S[, i], wm = sv[[\"1\"]]$X[, 8])\n  \n  ggplot(data = foo, aes(x = x, y = y, color = wm)) +\n    geom_point(size = 1) +  \n    # Single smooth line\n    geom_smooth(method = \"loess\", formula = 'y ~ x', se = FALSE, \n                color = \"blue1\", aes(group = 1)) +  \n    # Apply colorblind-friendly palette\n    scale_color_manual(values = cbPalette) +  \n    theme_bw() +\n    xlab(wm_vip_labels[selected_vars[i]]) + \n    ylab(\"SHAP value\") +\n    theme_bw() +\n    theme(axis.title.y = element_text(face = \"bold\", size = 8)) +\n    theme(\n      axis.text.x = element_text(size = 7),  # Decrease x-axis tick label size\n      axis.text.y = element_text(size = 7)  # Decrease y-axis tick label size\n    ) +\n    theme(axis.title.x = element_text(face = \"bold\", size = 8)) \n}\n\n\np1 &lt;- my_dependence_fxn(1)\np2 &lt;- my_dependence_fxn(2)\np3 &lt;- my_dependence_fxn(3)\np4 &lt;- my_dependence_fxn(4)\np5 &lt;- my_dependence_fxn(5)\np6 &lt;- my_dependence_fxn(6)\np7 &lt;- my_dependence_fxn(7)\n\n# This places the legend guide in the unoccupied panel\n(p1 + p2 + p3 + p4 + p5 + p6 + p7 + guide_area()) +\n  plot_annotation(tag_levels = \"A\") +\n  plot_layout(ncol = 2, byrow = TRUE, guides = \"collect\", axis_titles = \"collect\") & \n  theme(legend.position = \"bottom\", \n        legend.direction = \"horizontal\")\n\n\n\n\nMain effects SHAP dependence plots for the seven predictors in a random forest model for white mold in snap bean.",
    "crumbs": [
      "Data analysis",
      "Models for white mold"
    ]
  },
  {
    "objectID": "code_modeling.html#lr-model-i",
    "href": "code_modeling.html#lr-model-i",
    "title": "Models for white mold",
    "section": "LR model I",
    "text": "LR model I\n\nPredictors represented in the model as-is\nNo representation as restricted cubic splines (to account for nonlinearities in the logit)\nNo penalization\n\n\nload(here::here(\"Modeling\", \"rf_tuned_sel.RData\"))  # rf.tuned.sel, X.sel, selected_vars \n# rm(list = ls()[!(ls() %in% c(\"X.sel\", \"make_kable\"))])\n\n\n# Makes life easier to have wm as a numeric (0,1) instead of as a factor:\nX.sel &lt;- \n  X.sel %&gt;% \n  dplyr::mutate(wm = as.numeric(wm)-1)\n\n# Create a datadist object\ndd &lt;- rms::datadist(X.sel)\n\noptions(datadist = \"dd\")\n\nfmla &lt;- formula(paste(\"wm\", \"~\", \n                      paste(c(\"sm_40dap_to_49dap\",\n                              \"t2m_mean_to_4dap\",\n                              \"stsm_35dap_to_44dap\",\n                              \"sm_5dap_to_15dap\", \n                              \"log_silt_clay\", \n                              \"rainto35dap\", \n                              \"rain36to50dap\"),\n                            collapse = \"+\")))\n\n\n\n# NOTE: getting this warning message, and have not been able to solve it. \n# From what I read, this is a software bug. Therefore, I am wrapping the code in suppressWarnings...\n# Warning message:\n# In formula.character(object, env = baseenv()) :\n#   Using formula(x) is deprecated when x is a character vector of length &gt; 1.\n#   Consider formula(paste(x, collapse = \" \")) instead.\n\n# Set x = T and y = T to be able to compute bootstraps later for assessing optimism:\nM0 &lt;- rms::lrm(fmla, data = X.sel, x = T, y = T)\n\nsuppressWarnings(pred &lt;- predict(M0, type = \"fitted.ind\"))\n\n# No. obs:\nn &lt;- as.vector(M0$stats[\"Obs\"])\n\n# LR = Likelihood Ratio\nLR &lt;- as.vector(M0$stats[\"Model L.R.\"])\n\n# Max CS_app:\nMax_CS_app &lt;- as.vector(1 - exp(-M0$deviance[1]/n))\n\n# Model summary:\nprint(M0, coef = T)\n\n# The no. of predictor parameters:\nas.vector(M0$stats[\"d.f.\"])\n\n## IF you have the original development data, you can adjust for optimism via bootstrapping:\nset.seed(2022)\nM0.boot &lt;- rms::validate(M0, B = 1000)\n\n\n# Set up for model performance statistics via the CalibrationCurves package:\ncalPerf &lt;- suppressGraphics({\n  CalibrationCurves::val.prob.ci.2(pred, X.sel$wm)\n})\n\n\n# Get set up for using the `pminternal` package.  See the file Troubleshooting.R for more details. Basically, to avoid error messages and conflicts, we (i) do not load `pminternal`, (ii) write custom model fitting and predict functions.\nmodel_fun &lt;- function(data, ...) {\n  glm(wm ~ \n        sm_40dap_to_49dap +\n        t2m_mean_to_4dap +\n        stsm_35dap_to_44dap +\n        sm_5dap_to_15dap +\n        log_silt_clay +\n        rainto35dap +\n        rain36to50dap, \n      data = data, family = \"binomial\")\n}\n\n\npred_fun &lt;- function(model, data, ...) {\n  predict(model, newdata = data, type = \"response\")\n}\n\n# for calibration plot:\neval &lt;- seq(min(pred), max(pred), length.out = 200)\n\nset.seed(2022)\n# Takes ~ 32 sec\nmod_iv &lt;- pminternal::validate(method = \"boot_optimism\", \n                               data = X.sel, \n                               outcome = \"wm\", \n                               model_fun = model_fun, \n                               pred_fun = pred_fun, \n                               B = 2000, \n                               cores = 4,\n                               # Here we use loess:\n                               calib_args = list(smooth = \"loess\", eval = eval))\n\n\nModel summary\n\n# Model summary:\nprint(M0, coef = T)\n\nLogistic Regression Model\n\nrms::lrm(formula = fmla, data = X.sel, x = T, y = T)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs           356    LR chi2      42.67      R2       0.177    C       0.728    \n 0            283    d.f.             7      R2(7,356)0.095    Dxy     0.456    \n 1             73    Pr(&gt; chi2) &lt;0.0001    R2(7,174.1)0.185    gamma   0.456    \nmax |deriv| 1e-08                            Brier    0.140    tau-a   0.149    \n\n                    Coef     S.E.   Wald Z Pr(&gt;|Z|)\nIntercept            -7.5358 3.1717 -2.38  0.0175  \nsm_40dap_to_49dap    13.5259 7.7544  1.74  0.0811  \nt2m_mean_to_4dap      0.3344 0.0726  4.61  &lt;0.0001 \nstsm_35dap_to_44dap  -0.0150 0.0155 -0.97  0.3322  \nsm_5dap_to_15dap    -16.5571 7.5436 -2.19  0.0282  \nlog_silt_clay         0.9923 0.8333  1.19  0.2337  \nrainto35dap           0.0038 0.0041  0.93  0.3521  \nrain36to50dap        -0.0037 0.0071 -0.52  0.6020  \n\n\n\n\nFitted model formula\nOutput a function for obtaining predictions on the logit sale from the fitted LR model.\n\nFunction(M0)\n\nfunction (sm_40dap_to_49dap = 0.30222176, t2m_mean_to_4dap = 20.353918, \n    stsm_35dap_to_44dap = 65.915896, sm_5dap_to_15dap = 0.30405061, \n    log_silt_clay = 1.102371, rainto35dap = 114.4, rain36to50dap = 46.65) \n{\n    -7.5357792 + 13.525937 * sm_40dap_to_49dap + 0.33437285 * \n        t2m_mean_to_4dap - 0.014985675 * stsm_35dap_to_44dap - \n        16.557108 * sm_5dap_to_15dap + 0.99230044 * log_silt_clay + \n        0.0037716469 * rainto35dap - 0.0037190062 * rain36to50dap\n}\n&lt;environment: 0x0000026ed7761b68&gt;\n\n\n\n\nModel fit statistics\nWe’ll focus on statistics that Riley and colleagues have suggested should be reported for logistic regression models.\n\nNo. predictor variables\n\nlength(M0$Design$name) %&gt;% \n  make_kable(col.names = \"No. predictors\")\n\n\n\n\n\nNo. predictors\n\n\n\n\n1\n7\n\n\n\n\n\n\n\n\n\nNo. predictor parameters\n\nas.vector(M0$stats[\"d.f.\"]) %&gt;% \n  make_kable(col.names = \"No. predictor params\")\n\n\n\n\n\nNo. predictor params\n\n\n\n\n1\n7\n\n\n\n\n\n\n\n\n\nEvents per predictor param\n\n(sum(M0$y)/as.vector(M0$stats[\"d.f.\"])) %&gt;% \n  make_kable(col.names = \"EPP\")\n\n\n\n\n\nEPP\n\n\n\n\n1\n10.4\n\n\n\n\n\n\n\n\n\nMax Cox-Snell and LR\n\ntibble::tibble(`Max Cox-Snell (app)` = Max_CS_app, `Likelihood ratio` = LR) %&gt;% \n  make_kable()\n\n\n\n\n\nMax Cox-Snell (app)\nLikelihood ratio\n\n\n\n\n1\n0.637\n42.7\n\n\n\n\n\n\n\n\n\nNagelkerke\n\nNagelkerke &lt;- as.vector(M0$stats[\"R2\"])\n\n# Bootstrap CI for Nagelkerke R2:\ng &lt;- rms::bootcov(M0, stat = 'R2', seed = 4, B = 1000)\n# The CI limits:\ng.quan &lt;- quantile(g$boot.stats, c(.025, .975))\n# Lower CI bound:\nNagelkerke.LL &lt;- g.quan[1]\n# Upper CI bound:\nNagelkerke.UL &lt;- g.quan[2]\n\n# Internal validation: optimism-adjusted Nagelkerke's R2 from the bootstrap validation results:\nNagelkerke_adj &lt;- M0.boot[2, 5]\n\nres_Nagelkerke &lt;- matrix(\n  c(Nagelkerke,\n    Nagelkerke.LL,\n    Nagelkerke.UL,\n    Nagelkerke_adj,\n    NA,\n    NA),\n  \n  nrow = 1,\n  ncol = 6,\n  byrow = T,\n  \n  dimnames = list(\n    c(\"Nagelkerke R2\"),\n    rep(c(\"Estimate\", \"Lower .95 \", \"Upper .95\"), 2)\n  )\n)\n\nres_Nagelkerke %&gt;% \n  make_kable() %&gt;% \n  add_header_above(c(\" \" = 1, \n                     \"Apparent\" = 3, \n                     \"Internal\" = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApparent\n\n\nInternal\n\n\n\n\nEstimate\nLower .95\nUpper .95\nEstimate\nLower .95\nUpper .95\n\n\n\n\nNagelkerke R2\n0.177\n0.116\n0.321\n0.121\nNA\nNA\n\n\n\n\n\n\n\n\n\nCox-Snell\n\n# CS_app = Apparent estimate of the Cox-Snell statistic:\n# Point estimate and 95% CI bounds:\nCS_app &lt;- c(Nagelkerke, Nagelkerke.LL, Nagelkerke.UL)*Max_CS_app\n\n# Adjusted for optimism:\nCS_adj &lt;- Nagelkerke_adj*Max_CS_app\n\nres_CS &lt;- matrix(\n  c(CS_app,\n    CS_adj,\n    NA,\n    NA),\n  \n  nrow = 1,\n  ncol = 6,\n  byrow = T,\n  \n  dimnames = list(\n    c(\"Cox-Snell R2\"),\n    rep(c(\"Estimate\", \"Lower .95 \", \"Upper .95\"), 2)\n  )\n)\n\nres_CS %&gt;% \n  make_kable() %&gt;% \n  add_header_above(c(\" \" = 1, \n                     \"Apparent\" = 3, \n                     \"Internal\" = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApparent\n\n\nInternal\n\n\n\n\nEstimate\nLower .95\nUpper .95\nEstimate\nLower .95\nUpper .95\n\n\n\n\nCox-Snell R2\n0.113\n0.074\n0.205\n0.077\nNA\nNA\n\n\n\n\n\n\n\n\n\nBrier score\n\n# Brier score:\nBrier &lt;- as.vector(M0$stats[\"Brier\"])\n# Bootstrap CI for Brier score:\ng &lt;- rms::bootcov(M0, stat = 'Brier', seed = 4, B = 1000)\n# The CI limits:\ng.quan &lt;- quantile(g$boot.stats, c(.025, .975))\n# Lower CI bound:\nBrier.LL &lt;- g.quan[1]\n# Upper CI bound:\nBrier.UL &lt;- g.quan[2]\n\nBrier_app &lt;- c(Brier, Brier.LL, Brier.UL)\n\n# Adjusted for optimism via rms:\nBrier_adj &lt;- M0.boot[\"B\", \"index.corrected\"]\n\nres_Brier &lt;- matrix(\n  c(Brier_app,\n    Brier_adj,\n    NA,\n    NA),\n  \n  nrow = 1,\n  ncol = 6,\n  byrow = T,\n  \n  dimnames = list(\n    c(\"Brier\"),\n    rep(c(\"Estimate\", \"Lower .95 \", \"Upper .95\"), 2)\n  )\n)\n\nres_Brier %&gt;% \n  make_kable() %&gt;% \n  add_header_above(c(\" \" = 1, \n                     \"Apparent\" = 3, \n                     \"Internal\" = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApparent\n\n\nInternal\n\n\n\n\nEstimate\nLower .95\nUpper .95\nEstimate\nLower .95\nUpper .95\n\n\n\n\nBrier\n0.14\n0.111\n0.16\n0.148\nNA\nNA\n\n\n\n\n\n\n\n\n\nCalibration intercept\nAlso called Calibration-in-the-large\n\nCcurves_app &lt;- calPerf$Calibration$Intercept\n\n# Corrected for optimism:\n# summary(mod_iv)[\"Corrected\", \"Intercept\"]\n\n\nres_cil &lt;- matrix(c(\n  # From the CalibrationCurves package:\n  Ccurves_app[\"Point estimate\"],\n  Ccurves_app[\"Lower confidence limit\"],\n  Ccurves_app[\"Upper confidence limit\"],\n  \n  # Corrected for optimism:\n  summary(mod_iv)[\"Corrected\", \"Intercept\"],\n  NA,\n  NA\n  ),\n  nrow = 1,\n  ncol = 6, \n  byrow = TRUE,\n  dimnames = list(\n    c(\"Calibration-in-the-large\"),\n    rep(c(\"Estimate\", \"Lower .95 \", \"Upper .95\"), 2)\n  )\n)\n\n\nres_cil %&gt;% \n  make_kable() %&gt;% \n  add_header_above(c(\" \" = 1, \n                     \"Apparent\" = 3, \n                     \"Internal\" = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApparent\n\n\nInternal\n\n\n\n\nEstimate\nLower .95\nUpper .95\nEstimate\nLower .95\nUpper .95\n\n\n\n\nCalibration-in-the-large\n0\n-0.275\n0.275\n-0.181\nNA\nNA\n\n\n\n\n\n\n\n\n\nCalibration slope\n\n# Via the CalibrationCurves package:\nCC_slope_app &lt;- calPerf$Calibration$Slope\n\n\n# Table the results:\nres_weak_cal &lt;- matrix(c(\n  CC_slope_app[\"Point estimate\"],\n  CC_slope_app[\"Lower confidence limit.2.5 %\"],\n  CC_slope_app[\"Upper confidence limit.97.5 %\"],\n  # Adjusting for optimism:\n  summary(mod_iv)[\"Corrected\", \"Slope\"],\n  NA,\n  NA\n  ),\n  nrow = 1,\n  ncol = 6, \n  byrow = TRUE,\n  dimnames = list(\n    c(\"Calibration slope\"),\n    rep(c(\"Estimate\", \"Lower .95 \", \"Upper .95\"), 2))\n)\n\nres_weak_cal %&gt;% \n  make_kable() %&gt;% \n  add_header_above(c(\" \" = 1, \"Apparent\" = 3, \"Internal\" = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApparent\n\n\nInternal\n\n\n\n\nEstimate\nLower .95\nUpper .95\nEstimate\nLower .95\nUpper .95\n\n\n\n\nCalibration slope\n1\n0.664\n1.34\n0.836\nNA\nNA\n\n\n\n\n\n\n\n\n\nCalibration plot\nLogistic regression models are typically well-calibrated on the development data; and we see aspects of that here.\nHowever, the model does have some issues.\n\n# Using ggplot2:\nLR1CC &lt;- CalibrationCurves::valProbggplot(pred, X.sel$wm)$ggPlot\n\nLR1CC\n\n\n\n\nCalibration plot for a logistic regression model for predicting the presence of white mold in snap bean.\n\n\n\n\n\n\nPredicted probabilities\n\ntibble::tibble(response = pred, truth = X.sel$wm) %&gt;%\n  ggplot(aes(response)) +\n  geom_histogram(fill = \"orange\", col = \"orange3\", bins = 20) +\n  facet_wrap(~ as.factor(truth), ncol = 1, \n             labeller = as_labeller(c(\"0\" = \"No White Mold\", \"1\" = \"White Mold Present\"))) +\n  geom_rug(col = \"blue\", alpha = 0.5) + \n  labs(x = \"Probability estimate of white mold\", y = \"No. of fields\") +\n  theme_bw()\n\n\n\n\nHistogram of fitted probabilities for a logistic regression model fit to a subset of predictors identified by the VSURF algorithm.\n\n\n\n\n\n\nICI\n\nApparent\n\n# As a background check:\n# Get the apparent estimate using CalibrationCurves, which generates a flexible calibration curve based on loess as the default.\n\ncalPerf$stats[\"Eavg\"]\n\nThe calculations are done by hand, making heavy use of code by Daniele Giardiello. A CI for the estimate of the apparent ICI is calculated via bootstrap. Complicated enough as it is. No internal validation (correction for optimism) here; better (easier) obtained with pminternal.\n\n# Calibration measures ICI, E50, E90 based on secondary logistic regression\n# Create a copy of the X.sel data frame, with pred added as a column:\nXmod &lt;-\n  X.sel %&gt;% \n  dplyr::mutate(pred = pred)\n\n# Use loess to fit the flexible calibration curve:\nfit_cal &lt;- loess(wm ~ pred, \n                 data = Xmod, \n                 span = 0.75, \n                 degree = 2, \n                 family = \"gaussian\")\n\ncal_obs &lt;- predict(fit_cal)\n\ndt_cal &lt;- \n  cbind.data.frame(\n    \"obs\" = cal_obs,\n    \"pred\" = pred)\n\n\nres_calmeas &lt;-\n  c(\n    \"ICI\" = mean(abs(dt_cal$obs - dt_cal$pred)),\n    \"E50\" = median(abs(dt_cal$obs - dt_cal$pred)),\n    \"E90\" = unname(quantile(abs(dt_cal$obs - dt_cal$pred), \n                            probs = .90))\n)\n\n\n## Bootstrap confidence intervals for the calibration  measures (ICI, E50, E90)\nalpha &lt;- .05\n# Set B = 2000 although it takes more time:\nB &lt;- 2000 \nset.seed(2022)\n# Using stratification on wm because of the imbalance between the classes:\nvboot &lt;- rsample::bootstraps(X.sel, times = B, strata = wm)\n\n# Bootstrap calibration measures\nnumsum_boot &lt;- function(split) {\n  \n  pred &lt;- suppressWarnings(predict(M0,\n                  type = \"fitted.ind\",\n                  newdata = rsample::analysis(split)\n                  ) )\n\n  v_cal &lt;- loess(wm ~ pred, \n                 data = rsample::analysis(split), \n                 span = 0.75, \n                 degree = 2, \n                 family = \"gaussian\")\n\n  cal_obs_boot &lt;- predict(v_cal)\n  \n  # Save objects needed\n  db_cal_boot &lt;- data.frame(\n    \"obs\" = cal_obs_boot,\n    \"pred\" = pred\n    )\n  \n  absdiff_boot &lt;- abs(db_cal_boot$obs - db_cal_boot$pred)\n\n  res_cal_boot &lt;- data.frame(\n    \"ICI\" = mean(absdiff_boot),\n    \"E50\" = quantile(absdiff_boot, probs = .5),\n    \"E90\" = quantile(absdiff_boot, probs = .9)\n  )\n  }  # end of function\n\n# Example of use:\n# (bar &lt;- purrr::pluck(vboot, \"splits\", 1) %&gt;% numsum_boot())\n\ntictoc::tic()\nnumsum_b &lt;- \n  vboot %&gt;% \n  dplyr::mutate(num_cal_boot = purrr::map(splits, numsum_boot),\n                \n                ICI = purrr::map_dbl(num_cal_boot, ~ .x$ICI),\n         \n                E50 = purrr::map_dbl(num_cal_boot, ~ .x$E50),\n         \n                E90 = purrr::map_dbl(num_cal_boot, ~ .x$E90)\n         )\n\ntictoc::toc()  # 9 sec\n\n9.69 sec elapsed\n\n# numsum_b is a tibble with 2,000 rows\n# Now we need to process the results to get the bootstrap CIs:\nalpha &lt;- .05\nres_numcal &lt;- matrix(c(res_calmeas[\"ICI\"],\n                       quantile(numsum_b$ICI, probs = alpha / 2),\n                       quantile(numsum_b$ICI, \n                                probs = 1 - alpha / 2),\n                       \n                       res_calmeas[\"E50\"],\n                       quantile(numsum_b$E50, probs = alpha / 2),\n                       quantile(numsum_b$E50, probs = 1 - alpha / 2),\n                       \n                       res_calmeas[\"E90\"],\n                       quantile(numsum_b$E90, probs = alpha / 2),\n                       quantile(numsum_b$E90, probs = 1 - alpha / 2)\n                       ),\n                     nrow = 1,\n                     ncol = 9,\n                     byrow = T,\n                     dimnames = list(\n                       c(\"Apparent\"),\n                       rep(\n                         c(\"Estimate\", \"Lower.95\", \"Upper.95\"),\n                         3))\n)\n\n\n# Present the results:\nres_numcal %&gt;% \n  make_kable()  %&gt;% \n  add_header_above(c(\" \" = 1,  \n                     \"ICI\" = 3, \n                     \"E50\" = 3,\n                     \"E90\" = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nICI\n\n\nE50\n\n\nE90\n\n\n\n\nEstimate\nLower.95\nUpper.95\nEstimate\nLower.95\nUpper.95\nEstimate\nLower.95\nUpper.95\n\n\n\n\nApparent\n0.031\n0.017\n0.069\n0.021\n0.009\n0.06\n0.059\n0.032\n0.142\n\n\n\n\n\n\n\n\n\nInternal validation\n\n# ICI adjusted for optimism:\n\n# oc = optimism-corrected\noptsum &lt;-\n  c(\n    ICI.app = summary(mod_iv)[\"Apparent\", \"Eavg\"],\n    ICI.oc = summary(mod_iv)[\"Corrected\", \"Eavg\"],\n    E50.app = summary(mod_iv)[\"Apparent\", \"E50\"],\n    E50.oc = summary(mod_iv)[\"Corrected\", \"E50\"],\n    E90.app = summary(mod_iv)[\"Apparent\", \"E90\"],\n    E90.oc = summary(mod_iv)[\"Corrected\", \"E90\"]\n    )\n\nres_optcal &lt;- matrix(c(optsum[\"ICI.app\"],\n                       optsum[\"ICI.oc\"],\n                       \n                       optsum[\"E50.app\"],\n                       optsum[\"E50.oc\"],\n                       \n                       optsum[\"E90.app\"],\n                       optsum[\"E90.oc\"]\n                       ),\n                     nrow = 1,\n                     ncol = 6,\n                     byrow = T,\n                     dimnames = list(\n                       c(\"Value\"),\n                       rep(c(\"Apparent\", \"Corrected\"), 3))\n                     )\n\n# Present the results:\nres_optcal %&gt;% \n  make_kable()  %&gt;% \n  add_header_above(c(\" \" = 1,  \n                     \"ICI\" = 2, \n                     \"E50\" = 2,\n                     \"E90\" = 2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nICI\n\n\nE50\n\n\nE90\n\n\n\n\nApparent\nCorrected\nApparent\nCorrected\nApparent\nCorrected\n\n\n\n\nValue\n0.027\n0.033\n0.014\n0.021\n0.057\n0.073\n\n\n\n\n\n\n\n\n\n\nC-statistic\nEquivalent to the AUROC in the logistic regression model setting.\nThe CI is a bit wide, as would be expected for a small dataset. Correction for optimism still indicates a good model (at least by this measure).\n\n# With rms::bootcov\n# C-statistic (apparent)\n# as.vector(M0$stats[\"C\"])\n\n# Bootstrap CI for the C-statistic:\ng &lt;- rms::bootcov(M0, stat = 'C', seed = 4, B = 1000)\n# The CI limits:\ng.quan &lt;- quantile(g$boot.stats, c(.025, .975))\n# Lower CI bound:\n# g.quan[1]\n# Upper CI bound:\n# g.quan[2]\n\n# Optimism-corrected:\n# The 1st row in M0.boot is Dxy, which is Somer's D. It is a transformed version of the C-statistic\nC_adj &lt;- 0.5*(M0.boot[1, 5] + 1)\n\nres_C &lt;- matrix(c(as.vector(M0$stats[\"C\"]),\n                  g.quan[1],\n                  g.quan[2],\n                  C_adj,\n                  NA,\n                  NA\n                  ),\n                nrow = 1,\n                ncol = 6,\n                byrow = T,\n                dimnames = list(\n                  c(\"\"),\n                  rep(c(\"Estimate\", \"Lower.95\", \"Upper.95\"), 2))\n                )\n\n# Present the results:\nres_C %&gt;% \n  make_kable()  %&gt;% \n  add_header_above(c(\" \" = 1,  \n                     \"Apparent\" = 3, \n                     \"Internal\" = 3\n                     )\n                   )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApparent\n\n\nInternal\n\n\n\n\nEstimate\nLower.95\nUpper.95\nEstimate\nLower.95\nUpper.95\n\n\n\n\n\n0.728\n0.681\n0.812\n0.696\nNA\nNA\n\n\n\n\n\n\n\n\n\nSe, Sp, YI\n\n# 1. Prepare predictions dataframe:\nlr_pred &lt;-\n  data.frame(truth = X.sel$wm, response = pred)\n\n# 2. Calculate apparent performance\nlr_apparent_perf &lt;- calc_metrics_cutpointr(lr_pred)\n\n# 3. Bootstrap process\nfmla &lt;- formula(paste(\"wm\", \"~\", \n                      paste(c(\"sm_40dap_to_49dap\",\n                              \"t2m_mean_to_4dap\",\n                              \"stsm_35dap_to_44dap\",\n                              \"sm_5dap_to_15dap\", \n                              \"log_silt_clay\", \n                              \"rainto35dap\", \n                              \"rain36to50dap\"),\n                            collapse = \"+\")))\n\n\nlr_boot_fxn &lt;- function(i) {\n  # Print the current iteration\n  cat(\"Currently processing iteration: \", i, \"\\n\")\n  \n  # a. Generate bootstrap sample\n  boot_sample &lt;- rsample::bootstraps(X.sel, times = 1, strata = wm)$splits[[1]] %&gt;%\n    rsample::analysis()\n  \n  # b. Fit model on bootstrap sample with error handling\n  boot_model &lt;- tryCatch(\n    expr = {\n       rms::lrm(fmla, data = boot_sample, x = T, y = T)\n    },\n    error = function(e) {\n      cat(\"Error occurred in iteration\", i, \":\", e$message, \"\\n\")\n      return(NULL) # Return NULL if an error occurs\n    }\n  )\n  \n  # If boot_model is NULL, skip this iteration\n  if (is.null(boot_model)) {\n    cat(\"Skipping iteration\", i, \"due to error.\\n\")\n    return(NULL)\n  }\n  \n  \n  # c. Calculate metrics on bootstrap sample (training performance)\n  suppressWarnings(boot_pred &lt;- predict(boot_model, type = \"fitted.ind\"))\n  \n  boot_pred_df &lt;- data.frame(truth = boot_sample$wm, response = boot_pred)\n  \n  train_perf &lt;- calc_metrics_cutpointr(boot_pred_df)\n  \n  # d & e. Apply bootstrap model to original dataset and calculate metrics (test performance)\n  suppressWarnings(test_pred &lt;- predict(boot_model, newdata = X.sel, type = \"fitted.ind\"))\n  \n  test_pred_df &lt;- data.frame(truth = X.sel$wm, response = test_pred)\n  \n  test_perf &lt;- calc_metrics_cutpointr(test_pred_df)\n  \n  # f. Calculate optimism\n  optimism &lt;- purrr::map2_dbl(train_perf, test_perf, \\(x, y) x-y)\n  \n  tibble(\n    iteration = i,\n    metric = names(train_perf),\n    train = unlist(train_perf),\n    test = unlist(test_perf),\n    optimism = optimism)\n  }\n\n\n# Run lr_boot_fxn in parallel:\nset.seed(178)\nmy.seeds &lt;- c(1:5000)\n\nplan(list(tweak(multisession, workers = 5), sequential))\ntictoc::tic()\nbootstrap_results &lt;- \n  furrr::future_map(my.seeds, lr_boot_fxn, .options = furrr_options(seed = TRUE)) %&gt;% \n  purrr::list_rbind()\ntictoc::toc()  # 43 sec\n\nplan(sequential)\n\n# 4. Apparent performance bootstrap confidence intervals\nlr_apparent_CI &lt;-\n  bootstrap_results %&gt;%\n  dplyr::group_by(metric) %&gt;%\n  dplyr::summarize(app_LCI = quantile(train, .025), app_UCI = quantile(train, .975))\n\n\n# 5. Average optimism across all bootstrap samples\navg_optimism &lt;- \n  bootstrap_results %&gt;%\n  dplyr::group_by(metric) %&gt;%\n  dplyr::summarize(avg_optimism = mean(optimism))\n\n# 6. Calculate optimism-corrected performance\nlr_corrected_perf &lt;- \n  tibble(\n    metric = names(lr_apparent_perf),\n    apparent = unlist(lr_apparent_perf)\n    ) %&gt;%\n  dplyr::left_join(lr_apparent_CI, by = \"metric\") %&gt;%\n  dplyr::left_join(avg_optimism, by = \"metric\") %&gt;%\n  dplyr::mutate(corrected = apparent - avg_optimism)\n\n\n# Save the results:\nsave(lr_pred, bootstrap_results, lr_apparent_perf, lr_corrected_perf, file = here::here(\"Modeling\", \"LRIoptimism.RData\"))\n\n\n# lr_pred, bootstrap_results, lr_apparent_perf, lr_corrected_perf\nload(here::here(\"Modeling\", \"LRIoptimism.RData\"))  \n\nlr_corrected_perf %&gt;% \n  make_kable()\n\n\n\n\n\nmetric\napparent\napp_LCI\napp_UCI\navg_optimism\ncorrected\n\n\n\n\n1\nYI\n0.353\n0.311\n0.546\n0.066\n0.287\n\n\n2\nSe\n0.562\n0.438\n0.877\n0.063\n0.499\n\n\n3\nSp\n0.792\n0.512\n0.947\n0.003\n0.789\n\n\n4\nAUC\n0.728\n0.678\n0.812\n0.034\n0.694\n\n\n5\nbrier\n0.140\n0.119\n0.150\n-0.007\n0.147",
    "crumbs": [
      "Data analysis",
      "Models for white mold"
    ]
  },
  {
    "objectID": "code_modeling.html#lr-model-ii",
    "href": "code_modeling.html#lr-model-ii",
    "title": "Models for white mold",
    "section": "LR model II",
    "text": "LR model II\nAs the SHAP plots indicate nonlinearity in the feature contributions, we’ll model the features using restricted cubic splines. This will be done via the rms package.\nRestrict to the top 4 predictors:\n\nt2m_mean_to_4dap\nsm_40dap_to_49dap\nstsm_35dap_to_44dap\nrain36to50dap\n\nConduct an internal validation, as all the data will be used for model building (as we don’t have much data or wm cases, it’s wasteful not to use all the data to build the model). Internal validation will correct for optimism in the model build.\n\n\n\n\n\nload(here::here(\"Modeling\", \"rf_tuned_sel.RData\"))  # rf.tuned.sel, X.sel, selected_vars \n# rm(list = ls()[!(ls() %in% c(\"X.sel\", \"make_kable\"))])\n\n\n# Makes life easier to have wm as a numeric (0,1) instead of as a factor:\nX.sel &lt;- \n  X.sel %&gt;% \n  dplyr::mutate(wm = as.numeric(wm)-1)\n\n# Create a datadist object\ndd &lt;- rms::datadist(X.sel)\n\noptions(datadist = \"dd\")\n\nfmla &lt;- formula(paste(\"wm\", \"~\", \n                      paste(c(\"rcs(t2m_mean_to_4dap, 3)\",\n                              \"rcs(sm_40dap_to_49dap, 3)\",\n                              \"rcs(stsm_35dap_to_44dap, 3)\", \n                              \"rcs(rain36to50dap, 3)\"),\n                            collapse = \"+\")))\n\n\n\n# NOTE: getting this warning message, and have not been able to solve it. \n# From what I read, this is a software bug. Therefore, I am wrapping the code in suppressWarnings...\n# Warning message:\n# In formula.character(object, env = baseenv()) :\n#   Using formula(x) is deprecated when x is a character vector of length &gt; 1.\n#   Consider formula(paste(x, collapse = \" \")) instead.\n\n# Set x = T and y = T to be able to compute bootstraps later for assessing optimism:\nM0 &lt;- rms::lrm(fmla, data = X.sel, x = T, y = T)\n\nsuppressWarnings(pred &lt;- predict(M0, type = \"fitted.ind\"))\n\n# No. obs:\nn &lt;- as.vector(M0$stats[\"Obs\"])\n\n# LR = Likelihood Ratio\nLR &lt;- as.vector(M0$stats[\"Model L.R.\"])\n\n# Max CS_app:\nMax_CS_app &lt;- as.vector(1 - exp(-M0$deviance[1]/n))\n\n# Model summary:\nprint(M0, coef = T)\n\n# The no. of predictor parameters:\nas.vector(M0$stats[\"d.f.\"])\n\n## IF you have the original development data, you can adjust for optimism via bootstrapping:\nset.seed(2022)\nM0.boot &lt;- rms::validate(M0, B = 1000)\n\n\n# Set up for model performance statistics via the CalibrationCurves package:\ncalPerf &lt;- suppressGraphics({\n  CalibrationCurves::val.prob.ci.2(pred, X.sel$wm)\n})\n\n\n# Get set up for using the `pminternal` package.  See the file Troubleshooting.R for more details. Basically, to avoid error messages and conflicts, we (i) do not load `pminternal`, (ii) write custom model fitting and predict functions.\nmodel_fun &lt;- function(data, ...) {\n  glm(wm ~ rms::rcs(t2m_mean_to_4dap, 3) + \n        rms::rcs(sm_40dap_to_49dap, 3) + \n        rms::rcs(stsm_35dap_to_44dap, 3) +\n        rms::rcs(rain36to50dap, 3), \n      data = data, family = \"binomial\")\n}\n\n\npred_fun &lt;- function(model, data, ...) {\n  predict(model, newdata = data, type = \"response\")\n}\n\n# for calibration plot:\neval &lt;- seq(min(pred), max(pred), length.out = 200)\n\nset.seed(2022)\n# Takes ~ 32 sec\nmod_iv &lt;- pminternal::validate(method = \"boot_optimism\", \n                               data = X.sel, \n                               outcome = \"wm\", \n                               model_fun = model_fun, \n                               pred_fun = pred_fun, \n                               B = 2000, \n                               cores = 4,\n                               # Here we use loess:\n                               calib_args = list(smooth = \"loess\", eval = eval))\n\n\nModel summary\n\n# Model summary:\nprint(M0, coef = T)\n\nLogistic Regression Model\n\nrms::lrm(formula = fmla, data = X.sel, x = T, y = T)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs           356    LR chi2      71.55      R2       0.286    C       0.798    \n 0            283    d.f.             8      R2(8,356)0.163    Dxy     0.594    \n 1             73    Pr(&gt; chi2) &lt;0.0001    R2(8,174.1)0.306    gamma   0.596    \nmax |deriv| 8e-05                            Brier    0.129    tau-a   0.195    \n\n                     Coef    S.E.    Wald Z Pr(&gt;|Z|)\nIntercept            -3.9761  4.8076 -0.83  0.4082  \nt2m_mean_to_4dap     -0.1647  0.1280 -1.29  0.1983  \nt2m_mean_to_4dap'     0.5193  0.1316  3.95  &lt;0.0001 \nsm_40dap_to_49dap     6.9182 14.0293  0.49  0.6219  \nsm_40dap_to_49dap'   -8.4035 10.7696 -0.78  0.4352  \nstsm_35dap_to_44dap   0.0082  0.0327  0.25  0.8013  \nstsm_35dap_to_44dap' -0.0453  0.0700 -0.65  0.5170  \nrain36to50dap         0.0768  0.0206  3.73  0.0002  \nrain36to50dap'       -0.1091  0.0275 -3.97  &lt;0.0001 \n\n\n\n\nFitted model formula\nOutput a function for obtaining predictions on the logit sale from the fitted LR model.\n\nFunction(M0)\n\nfunction (t2m_mean_to_4dap = 20.353918, sm_40dap_to_49dap = 0.30222176, \n    stsm_35dap_to_44dap = 65.915896, rain36to50dap = 46.65) \n{\n    -3.9761487 - 0.16469932 * t2m_mean_to_4dap + 0.010732301 * \n        pmax(t2m_mean_to_4dap - 15.749826, 0)^3 - 0.031740014 * \n        pmax(t2m_mean_to_4dap - 20.353918, 0)^3 + 0.021007713 * \n        pmax(t2m_mean_to_4dap - 22.70603, 0)^3 + 6.9181724 * \n        sm_40dap_to_49dap - 204.29019 * pmax(sm_40dap_to_49dap - \n        0.18214171, 0)^3 + 500.78374 * pmax(sm_40dap_to_49dap - \n        0.30222176, 0)^3 - 296.49355 * pmax(sm_40dap_to_49dap - \n        0.3849594, 0)^3 + 0.0082354322 * stsm_35dap_to_44dap - \n        8.732913e-06 * pmax(stsm_35dap_to_44dap - 50.395775, \n            0)^3 + 1.1130272e-05 * pmax(stsm_35dap_to_44dap - \n        65.915896, 0)^3 - 2.3973588e-06 * pmax(stsm_35dap_to_44dap - \n        122.45139, 0)^3 + 0.076765708 * rain36to50dap - 1.6521394e-05 * \n        pmax(rain36to50dap - 17.05, 0)^3 + 2.5989608e-05 * pmax(rain36to50dap - \n        46.65, 0)^3 - 9.4682141e-06 * pmax(rain36to50dap - 98.3, \n        0)^3\n}\n&lt;environment: 0x0000026f0568dea8&gt;\n\n\n\n\nModel fit statistics\nWe’ll focus on statistics that Riley and colleagues have suggested should be reported for logistic regression models.\n\nNo. predictor variables\n\nlength(M0$Design$name) %&gt;% \n  make_kable(col.names = \"No. predictors\")\n\n\n\n\n\nNo. predictors\n\n\n\n\n1\n4\n\n\n\n\n\n\n\n\n\nNo. predictor parameters\n\nas.vector(M0$stats[\"d.f.\"]) %&gt;% \n  make_kable(col.names = \"No. predictor params\")\n\n\n\n\n\nNo. predictor params\n\n\n\n\n1\n8\n\n\n\n\n\n\n\n\n\nEvents per predictor param\n\n(sum(M0$y)/as.vector(M0$stats[\"d.f.\"])) %&gt;% \n  make_kable(col.names = \"EPP\")\n\n\n\n\n\nEPP\n\n\n\n\n1\n9.12\n\n\n\n\n\n\n\n\n\nMax Cox-Snell and LR\n\ntibble::tibble(`Max Cox-Snell (app)` = Max_CS_app, `Likelihood ratio` = LR) %&gt;% \n  make_kable()\n\n\n\n\n\nMax Cox-Snell (app)\nLikelihood ratio\n\n\n\n\n1\n0.637\n71.5\n\n\n\n\n\n\n\n\n\nNagelkerke\n\nNagelkerke &lt;- as.vector(M0$stats[\"R2\"])\n\n# Bootstrap CI for Nagelkerke R2:\ng &lt;- rms::bootcov(M0, stat = 'R2', seed = 4, B = 1000)\n# The CI limits:\ng.quan &lt;- quantile(g$boot.stats, c(.025, .975))\n# Lower CI bound:\nNagelkerke.LL &lt;- g.quan[1]\n# Upper CI bound:\nNagelkerke.UL &lt;- g.quan[2]\n\n# Internal validation: optimism-adjusted Nagelkerke's R2 from the bootstrap validation results:\nNagelkerke_adj &lt;- M0.boot[2, 5]\n\nres_Nagelkerke &lt;- matrix(\n  c(Nagelkerke,\n    Nagelkerke.LL,\n    Nagelkerke.UL,\n    Nagelkerke_adj,\n    NA,\n    NA),\n  \n  nrow = 1,\n  ncol = 6,\n  byrow = T,\n  \n  dimnames = list(\n    c(\"Nagelkerke R2\"),\n    rep(c(\"Estimate\", \"Lower .95 \", \"Upper .95\"), 2)\n  )\n)\n\nres_Nagelkerke %&gt;% \n  make_kable() %&gt;% \n  add_header_above(c(\" \" = 1, \n                     \"Apparent\" = 3, \n                     \"Internal\" = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApparent\n\n\nInternal\n\n\n\n\nEstimate\nLower .95\nUpper .95\nEstimate\nLower .95\nUpper .95\n\n\n\n\nNagelkerke R2\n0.286\n0.208\n0.429\n0.228\nNA\nNA\n\n\n\n\n\n\n\n\n\nCox-Snell\n\n# CS_app = Apparent estimate of the Cox-Snell statistic:\n# Point estimate and 95% CI bounds:\nCS_app &lt;- c(Nagelkerke, Nagelkerke.LL, Nagelkerke.UL)*Max_CS_app\n\n# Adjusted for optimism:\nCS_adj &lt;- Nagelkerke_adj*Max_CS_app\n\nres_CS &lt;- matrix(\n  c(CS_app,\n    CS_adj,\n    NA,\n    NA),\n  \n  nrow = 1,\n  ncol = 6,\n  byrow = T,\n  \n  dimnames = list(\n    c(\"Cox-Snell R2\"),\n    rep(c(\"Estimate\", \"Lower .95 \", \"Upper .95\"), 2)\n  )\n)\n\nres_CS %&gt;% \n  make_kable() %&gt;% \n  add_header_above(c(\" \" = 1, \n                     \"Apparent\" = 3, \n                     \"Internal\" = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApparent\n\n\nInternal\n\n\n\n\nEstimate\nLower .95\nUpper .95\nEstimate\nLower .95\nUpper .95\n\n\n\n\nCox-Snell R2\n0.182\n0.133\n0.274\n0.145\nNA\nNA\n\n\n\n\n\n\n\n\n\nBrier score\n\n# Brier score:\nBrier &lt;- as.vector(M0$stats[\"Brier\"])\n# Bootstrap CI for Brier score:\ng &lt;- rms::bootcov(M0, stat = 'Brier', seed = 4, B = 1000)\n# The CI limits:\ng.quan &lt;- quantile(g$boot.stats, c(.025, .975))\n# Lower CI bound:\nBrier.LL &lt;- g.quan[1]\n# Upper CI bound:\nBrier.UL &lt;- g.quan[2]\n\nBrier_app &lt;- c(Brier, Brier.LL, Brier.UL)\n\n# Adjusted for optimism via rms:\nBrier_adj &lt;- M0.boot[\"B\", \"index.corrected\"]\n\nres_Brier &lt;- matrix(\n  c(Brier_app,\n    Brier_adj,\n    NA,\n    NA),\n  \n  nrow = 1,\n  ncol = 6,\n  byrow = T,\n  \n  dimnames = list(\n    c(\"Brier\"),\n    rep(c(\"Estimate\", \"Lower .95 \", \"Upper .95\"), 2)\n  )\n)\n\nres_Brier %&gt;% \n  make_kable() %&gt;% \n  add_header_above(c(\" \" = 1, \n                     \"Apparent\" = 3, \n                     \"Internal\" = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApparent\n\n\nInternal\n\n\n\n\nEstimate\nLower .95\nUpper .95\nEstimate\nLower .95\nUpper .95\n\n\n\n\nBrier\n0.129\n0.103\n0.147\n0.136\nNA\nNA\n\n\n\n\n\n\n\n\n\nCalibration intercept\nAlso called Calibration-in-the-large\n\nCcurves_app &lt;- calPerf$Calibration$Intercept\n\n# Corrected for optimism:\n# summary(mod_iv)[\"Corrected\", \"Intercept\"]\n\n\nres_cil &lt;- matrix(c(\n  # From the CalibrationCurves package:\n  Ccurves_app[\"Point estimate\"],\n  Ccurves_app[\"Lower confidence limit\"],\n  Ccurves_app[\"Upper confidence limit\"],\n  \n  # Corrected for optimism:\n  summary(mod_iv)[\"Corrected\", \"Intercept\"],\n  NA,\n  NA\n  ),\n  nrow = 1,\n  ncol = 6, \n  byrow = TRUE,\n  dimnames = list(\n    c(\"Calibration-in-the-large\"),\n    rep(c(\"Estimate\", \"Lower .95 \", \"Upper .95\"), 2)\n  )\n)\n\n\nres_cil %&gt;% \n  make_kable() %&gt;% \n  add_header_above(c(\" \" = 1, \n                     \"Apparent\" = 3, \n                     \"Internal\" = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApparent\n\n\nInternal\n\n\n\n\nEstimate\nLower .95\nUpper .95\nEstimate\nLower .95\nUpper .95\n\n\n\n\nCalibration-in-the-large\n0\n-0.291\n0.291\n-0.153\nNA\nNA\n\n\n\n\n\n\n\n\n\nCalibration slope\n\n# Via the CalibrationCurves package:\nCC_slope_app &lt;- calPerf$Calibration$Slope\n\n\n# Table the results:\nres_weak_cal &lt;- matrix(c(\n  CC_slope_app[\"Point estimate\"],\n  CC_slope_app[\"Lower confidence limit.2.5 %\"],\n  CC_slope_app[\"Upper confidence limit.97.5 %\"],\n  # Adjusting for optimism:\n  summary(mod_iv)[\"Corrected\", \"Slope\"],\n  NA,\n  NA\n  ),\n  nrow = 1,\n  ncol = 6, \n  byrow = TRUE,\n  dimnames = list(\n    c(\"Calibration slope\"),\n    rep(c(\"Estimate\", \"Lower .95 \", \"Upper .95\"), 2))\n)\n\nres_weak_cal %&gt;% \n  make_kable() %&gt;% \n  add_header_above(c(\" \" = 1, \"Apparent\" = 3, \"Internal\" = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApparent\n\n\nInternal\n\n\n\n\nEstimate\nLower .95\nUpper .95\nEstimate\nLower .95\nUpper .95\n\n\n\n\nCalibration slope\n1\n0.725\n1.27\n0.857\nNA\nNA\n\n\n\n\n\n\n\n\n\nCalibration plot\nLogistic regression models are typically well-calibrated on the development data; and we see aspects of that here.\nHowever, the model does have some issues. Very low and high risks are overestimated. Intermediate risks are underestimated.\n\nLRII model\n\n# Using ggplot2:\nLR2CC &lt;- CalibrationCurves::valProbggplot(pred, X.sel$wm)$ggPlot\n\nLR2CC\n\n\n\n\nCalibration plot for a logistic regression model for predicting the presence of white mold in snap bean.\n\n\n\n\n\n\nComparison to the LRI model\n\n(LR1CC + LR2CC) +\n  plot_annotation(tag_levels = \"A\") +\n   plot_layout(nrow = 2, byrow = TRUE, guides = \"collect\") & \n  cowplot::theme_half_open(font_size = 12) &\n  theme(legend.position = \"bottom\", \n        legend.direction = \"horizontal\")\n\n\n\n\n\n\n\nggsave(\"figs/LR_calibration_plot.png\", dpi = 600, height = 7, width = 5, bg = \"white\" )\nggsave(\"figs/LR_calibration_plot.pdf\", dpi = 600, height = 7, width = 5, bg = \"white\" )\n\n\n\n\nPredicted probabilities\n\ntibble::tibble(response = pred, truth = X.sel$wm) %&gt;%\n  ggplot(aes(response)) +\n  geom_histogram(fill = \"orange\", col = \"orange3\", bins = 20) +\n  facet_wrap(~ as.factor(truth), ncol = 1, \n             labeller = as_labeller(c(\"0\" = \"No White Mold\", \"1\" = \"White Mold Present\"))) +\n  geom_rug(col = \"blue\", alpha = 0.5) + \n  labs(x = \"Probability estimate of white mold\", y = \"No. of fields\") +\n  theme_bw()\n\n\n\n\nHistogram of fitted probabilities for a logistic regression model fit to a subset of predictors identified by the VSURF algorithm.\n\n\n\n\n\n\nICI\n\nApparent\n\n# As a background check:\n# Get the apparent estimate using CalibrationCurves, which generates a flexible calibration curve based on loess as the default.\n\ncalPerf$stats[\"Eavg\"]\n\nThe calculations are done by hand, making heavy use of code by Daniele Giardiello. A CI for the estimate of the apparent ICI is calculated via bootstrap. Complicated enough as it is. No internal validation (correction for optimism) here; better (easier) obtained with pminternal.\n\n# Calibration measures ICI, E50, E90 based on secondary logistic regression\n# Create a copy of the X.sel data frame, with pred added as a column:\nXmod &lt;-\n  X.sel %&gt;% \n  dplyr::mutate(pred = pred)\n\n# Use loess to fit the flexible calibration curve:\nfit_cal &lt;- loess(wm ~ pred, \n                 data = Xmod, \n                 span = 0.75, \n                 degree = 2, \n                 family = \"gaussian\")\n\ncal_obs &lt;- predict(fit_cal)\n\ndt_cal &lt;- \n  cbind.data.frame(\n    \"obs\" = cal_obs,\n    \"pred\" = pred)\n\n\nres_calmeas &lt;-\n  c(\n    \"ICI\" = mean(abs(dt_cal$obs - dt_cal$pred)),\n    \"E50\" = median(abs(dt_cal$obs - dt_cal$pred)),\n    \"E90\" = unname(quantile(abs(dt_cal$obs - dt_cal$pred), \n                            probs = .90))\n)\n\n\n## Bootstrap confidence intervals for the calibration  measures (ICI, E50, E90)\nalpha &lt;- .05\n# Set B = 2000 although it takes more time:\nB &lt;- 2000 \nset.seed(2022)\n# Using stratification on wm because of the imbalance between the classes:\nvboot &lt;- rsample::bootstraps(X.sel, times = B, strata = wm)\n\n# Bootstrap calibration measures\nnumsum_boot &lt;- function(split) {\n  \n  pred &lt;- suppressWarnings(predict(M0,\n                  type = \"fitted.ind\",\n                  newdata = rsample::analysis(split)\n                  ) )\n\n  v_cal &lt;- loess(wm ~ pred, \n                 data = rsample::analysis(split), \n                 span = 0.75, \n                 degree = 2, \n                 family = \"gaussian\")\n\n  cal_obs_boot &lt;- predict(v_cal)\n  \n  # Save objects needed\n  db_cal_boot &lt;- data.frame(\n    \"obs\" = cal_obs_boot,\n    \"pred\" = pred\n    )\n  \n  absdiff_boot &lt;- abs(db_cal_boot$obs - db_cal_boot$pred)\n\n  res_cal_boot &lt;- data.frame(\n    \"ICI\" = mean(absdiff_boot),\n    \"E50\" = quantile(absdiff_boot, probs = .5),\n    \"E90\" = quantile(absdiff_boot, probs = .9)\n  )\n  }  # end of function\n\n# Example of use:\n# (bar &lt;- purrr::pluck(vboot, \"splits\", 1) %&gt;% numsum_boot())\n\n\nnumsum_b &lt;- \n  vboot %&gt;% \n  dplyr::mutate(num_cal_boot = purrr::map(splits, numsum_boot),\n                \n                ICI = purrr::map_dbl(num_cal_boot, ~ .x$ICI),\n         \n                E50 = purrr::map_dbl(num_cal_boot, ~ .x$E50),\n         \n                E90 = purrr::map_dbl(num_cal_boot, ~ .x$E90)\n         )\n\n# numsum_b is a tibble with 2,000 rows\n# Now we need to process the results to get the bootstrap CIs:\nalpha &lt;- .05\nres_numcal &lt;- matrix(c(res_calmeas[\"ICI\"],\n                       quantile(numsum_b$ICI, probs = alpha / 2),\n                       quantile(numsum_b$ICI, \n                                probs = 1 - alpha / 2),\n                       \n                       res_calmeas[\"E50\"],\n                       quantile(numsum_b$E50, probs = alpha / 2),\n                       quantile(numsum_b$E50, probs = 1 - alpha / 2),\n                       \n                       res_calmeas[\"E90\"],\n                       quantile(numsum_b$E90, probs = alpha / 2),\n                       quantile(numsum_b$E90, probs = 1 - alpha / 2)\n                       ),\n                     nrow = 1,\n                     ncol = 9,\n                     byrow = T,\n                     dimnames = list(\n                       c(\"Apparent\"),\n                       rep(\n                         c(\"Estimate\", \"Lower.95\", \"Upper.95\"),\n                         3))\n)\n\n\n# Present the results:\nres_numcal %&gt;% \n  make_kable()  %&gt;% \n  add_header_above(c(\" \" = 1,  \n                     \"ICI\" = 3, \n                     \"E50\" = 3,\n                     \"E90\" = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nICI\n\n\nE50\n\n\nE90\n\n\n\n\nEstimate\nLower.95\nUpper.95\nEstimate\nLower.95\nUpper.95\nEstimate\nLower.95\nUpper.95\n\n\n\n\nApparent\n0.018\n0.016\n0.056\n0.017\n0.009\n0.051\n0.034\n0.03\n0.132\n\n\n\n\n\n\n\n\n\nInternal validation\n\n# ICI adjusted for optimism:\n\n# oc = optimism-corrected\noptsum &lt;-\n  c(\n    ICI.app = summary(mod_iv)[\"Apparent\", \"Eavg\"],\n    ICI.oc = summary(mod_iv)[\"Corrected\", \"Eavg\"],\n    E50.app = summary(mod_iv)[\"Apparent\", \"E50\"],\n    E50.oc = summary(mod_iv)[\"Corrected\", \"E50\"],\n    E90.app = summary(mod_iv)[\"Apparent\", \"E90\"],\n    E90.oc = summary(mod_iv)[\"Corrected\", \"E90\"]\n    )\n\nres_optcal &lt;- matrix(c(optsum[\"ICI.app\"],\n                       optsum[\"ICI.oc\"],\n                       \n                       optsum[\"E50.app\"],\n                       optsum[\"E50.oc\"],\n                       \n                       optsum[\"E90.app\"],\n                       optsum[\"E90.oc\"]\n                       ),\n                     nrow = 1,\n                     ncol = 6,\n                     byrow = T,\n                     dimnames = list(\n                       c(\"Value\"),\n                       rep(c(\"Apparent\", \"Corrected\"), 3))\n                     )\n\n# Present the results:\nres_optcal %&gt;% \n  make_kable()  %&gt;% \n  add_header_above(c(\" \" = 1,  \n                     \"ICI\" = 2, \n                     \"E50\" = 2,\n                     \"E90\" = 2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nICI\n\n\nE50\n\n\nE90\n\n\n\n\nApparent\nCorrected\nApparent\nCorrected\nApparent\nCorrected\n\n\n\n\nValue\n0.017\n0.022\n0.015\n0.016\n0.036\n0.05\n\n\n\n\n\n\n\n\n\n\nC-statistic\nEquivalent to the AUROC in the logistic regression model setting.\nThe CI is a bit wide, as would be expected for a small dataset. Correction for optimism still indicates a good model (at least by this measure).\n\n# With rms::bootcov\n# C-statistic (apparent)\n# as.vector(M0$stats[\"C\"])\n\n# Bootstrap CI for the C-statistic:\ng &lt;- rms::bootcov(M0, stat = 'C', seed = 4, B = 1000)\n# The CI limits:\ng.quan &lt;- quantile(g$boot.stats, c(.025, .975))\n# Lower CI bound:\n# g.quan[1]\n# Upper CI bound:\n# g.quan[2]\n\n# Optimism-corrected:\n# The 1st row in M0.boot is Dxy, which is Somer's D. It is a transformed version of the C-statistic\nC_adj &lt;- 0.5*(M0.boot[1, 5] + 1)\n\nres_C &lt;- matrix(c(as.vector(M0$stats[\"C\"]),\n                  g.quan[1],\n                  g.quan[2],\n                  C_adj,\n                  NA,\n                  NA\n                  ),\n                nrow = 1,\n                ncol = 6,\n                byrow = T,\n                dimnames = list(\n                  c(\"\"),\n                  rep(c(\"Estimate\", \"Lower.95\", \"Upper.95\"), 2))\n                )\n\n# Present the results:\nres_C %&gt;% \n  make_kable()  %&gt;% \n  add_header_above(c(\" \" = 1,  \n                     \"Apparent\" = 3, \n                     \"Internal\" = 3\n                     )\n                   )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApparent\n\n\nInternal\n\n\n\n\nEstimate\nLower.95\nUpper.95\nEstimate\nLower.95\nUpper.95\n\n\n\n\n\n0.798\n0.755\n0.869\n0.773\nNA\nNA\n\n\n\n\n\n\n\n\n\nSe, Sp, YI\n\n# 1. Prepare predictions dataframe:\nlr_pred &lt;-\n  data.frame(truth = X.sel$wm, response = pred)\n\n# 2. Calculate apparent performance\nlr_apparent_perf &lt;- calc_metrics_cutpointr(lr_pred)\n\n# 3. Bootstrap process\nfmla &lt;- formula(paste(\"wm\", \"~\", \n                      paste(c(\"rms::rcs(t2m_mean_to_4dap, 3)\",\n                              \"rms::rcs(sm_40dap_to_49dap, 3)\",\n                              \"rms::rcs(stsm_35dap_to_44dap, 3)\", \n                              \"rms::rcs(rain36to50dap, 3)\"),\n                            collapse = \"+\")))\n\n\nlr_boot_fxn &lt;- function(i) {\n  # Print the current iteration\n  cat(\"Currently processing iteration: \", i, \"\\n\")\n  \n  # a. Generate bootstrap sample\n  boot_sample &lt;- rsample::bootstraps(X.sel, times = 1, strata = wm)$splits[[1]] %&gt;%\n    rsample::analysis()\n  \n  # b. Fit model on bootstrap sample with error handling\n  boot_model &lt;- tryCatch(\n    expr = {\n       rms::lrm(fmla, data = boot_sample, x = T, y = T)\n    },\n    error = function(e) {\n      cat(\"Error occurred in iteration\", i, \":\", e$message, \"\\n\")\n      return(NULL) # Return NULL if an error occurs\n    }\n  )\n  \n  # If boot_model is NULL, skip this iteration\n  if (is.null(boot_model)) {\n    cat(\"Skipping iteration\", i, \"due to error.\\n\")\n    return(NULL)\n  }\n  \n  \n  # c. Calculate metrics on bootstrap sample (training performance)\n  suppressWarnings(boot_pred &lt;- predict(boot_model, type = \"fitted.ind\"))\n  \n  boot_pred_df &lt;- data.frame(truth = boot_sample$wm, response = boot_pred)\n  \n  train_perf &lt;- calc_metrics_cutpointr(boot_pred_df)\n  \n  # d & e. Apply bootstrap model to original dataset and calculate metrics (test performance)\n  suppressWarnings(test_pred &lt;- predict(boot_model, newdata = X.sel, type = \"fitted.ind\"))\n  \n  test_pred_df &lt;- data.frame(truth = X.sel$wm, response = test_pred)\n  \n  test_perf &lt;- calc_metrics_cutpointr(test_pred_df)\n  \n  # f. Calculate optimism\n  optimism &lt;- purrr::map2_dbl(train_perf, test_perf, \\(x, y) x-y)\n  \n  tibble(\n    iteration = i,\n    metric = names(train_perf),\n    train = unlist(train_perf),\n    test = unlist(test_perf),\n    optimism = optimism)\n  }\n\n\n# Run lr_boot_fxn in parallel:\nset.seed(5987)\nmy.seeds &lt;- c(1:5000)\n\nplan(list(tweak(multisession, workers = 5), sequential))\ntictoc::tic()\nbootstrap_results &lt;- \n  furrr::future_map(my.seeds, lr_boot_fxn, .options = furrr_options(seed = TRUE)) %&gt;% \n  purrr::list_rbind()\ntictoc::toc()  # 50 sec\n\nplan(sequential)\n\n\n# 4. Apparent performance bootstrap confidence intervals\nlr_apparent_CI &lt;-\n  bootstrap_results %&gt;%\n  dplyr::group_by(metric) %&gt;%\n  dplyr::summarize(app_LCI = quantile(train, .025), app_UCI = quantile(train, .975))\n\n\n# 5. Average optimism across all bootstrap samples\navg_optimism &lt;- \n  bootstrap_results %&gt;%\n  dplyr::group_by(metric) %&gt;%\n  dplyr::summarize(avg_optimism = mean(optimism))\n\n# 6. Calculate optimism-corrected performance\nlr_corrected_perf &lt;- \n  tibble(\n    metric = names(lr_apparent_perf),\n    apparent = unlist(lr_apparent_perf)\n    ) %&gt;%\n  dplyr::left_join(lr_apparent_CI, by = \"metric\") %&gt;%\n  dplyr::left_join(avg_optimism, by = \"metric\") %&gt;%\n  dplyr::mutate(corrected = apparent - avg_optimism)\n\n\n# Save the results:\nsave(lr_pred, lr_apparent_perf, bootstrap_results, lr_corrected_perf, file = here::here(\"Modeling\", \"LRIIoptimism.RData\"))\n\n\n# lr_pred, lr_apparent_perf, bootstrap_results, lr_corrected_perf\nload(here::here(\"Modeling\", \"LRIIoptimism.RData\"))  \n\nlr_corrected_perf %&gt;% \n  make_kable()\n\n\n\n\n\nmetric\napparent\napp_LCI\napp_UCI\navg_optimism\ncorrected\n\n\n\n\n1\nYI\n0.461\n0.410\n0.637\n0.060\n0.401\n\n\n2\nSe\n0.836\n0.575\n0.932\n0.049\n0.786\n\n\n3\nSp\n0.625\n0.565\n0.908\n0.010\n0.615\n\n\n4\nAUC\n0.798\n0.754\n0.867\n0.027\n0.771\n\n\n5\nbrier\n0.129\n0.108\n0.140\n-0.007\n0.136",
    "crumbs": [
      "Data analysis",
      "Models for white mold"
    ]
  },
  {
    "objectID": "code_modeling.html#session-info",
    "href": "code_modeling.html#session-info",
    "title": "Models for white mold",
    "section": "Session Info",
    "text": "Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Portuguese_Brazil.utf8  LC_CTYPE=Portuguese_Brazil.utf8   \n[3] LC_MONETARY=Portuguese_Brazil.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Portuguese_Brazil.utf8    \n\ntime zone: America/Sao_Paulo\ntzcode source: internal\n\nattached base packages:\n [1] splines   grid      tools     compiler  parallel  stats     graphics \n [8] grDevices datasets  utils     methods   base     \n\nother attached packages:\n  [1] cards_0.4.0             multcomp_1.4-26         fastmatch_1.1-6        \n  [4] cowplot_1.1.3           rlang_1.1.4             insight_1.2.0          \n  [7] scales_1.3.0            mvtnorm_1.3-2           viridisLite_0.4.2      \n [10] listenv_0.9.1           MatrixModels_0.5-3      globals_0.16.3         \n [13] Rcpp_1.0.13             munsell_0.5.1           systemfonts_1.1.0      \n [16] rpart_4.1.23            cli_3.6.3               codetools_0.2-20       \n [19] evaluate_1.0.1          yaml_2.3.10             stringi_1.8.4          \n [22] RConics_1.1.1           xfun_0.48               svglite_2.1.3          \n [25] bookdown_0.41           gridExtra_2.3           tidyselect_1.2.1       \n [28] renv_1.1.2              BBmisc_1.13             pillar_1.9.0           \n [31] markdown_1.13           utf8_1.2.4              xml2_1.3.6             \n [34] hms_1.1.3               data.table_1.16.2       tzdb_0.4.0             \n [37] gtable_0.3.5            generics_0.1.3          cluster_2.1.6          \n [40] nlme_3.1-164            Metrics_0.1.4           glue_1.8.0             \n [43] nnet_7.3-19             future.apply_1.11.2     parallelMap_1.5.1      \n [46] foreign_0.8-86          quantreg_5.98           SparseM_1.84-2         \n [49] R.utils_2.12.3          R.oo_1.27.0             R.methodsS3_1.8.2      \n [52] highr_0.11              backports_1.5.0         htmlTable_2.4.3        \n [55] doParallel_1.0.17       foreach_1.5.2           withr_3.0.2            \n [58] here_1.0.1              timechange_0.3.0        fansi_1.0.6            \n [61] yardstick_1.3.1         randomForest_4.7-1.2    labeling_0.4.3         \n [64] textshaping_0.4.0       rprojroot_2.0.4         colorspace_2.1-1       \n [67] digest_0.6.37           fastmap_1.2.0           R6_2.5.1               \n [70] Matrix_1.7-0            pkgconfig_2.0.3         iterators_1.0.14       \n [73] lifecycle_1.0.4         commonmark_1.9.2        gt_0.11.1              \n [76] zoo_1.8-12              sandwich_3.1-1          plyr_1.8.9             \n [79] DiceKriging_1.6.0       htmlwidgets_1.6.4       parallelly_1.38.0      \n [82] sass_0.4.9              Formula_1.2-5           xgboost_1.7.8.1        \n [85] haven_2.5.4             polspline_1.1.25        htmltools_0.5.8.1      \n [88] base64enc_0.1-3         vctrs_0.6.5             ragg_1.3.3             \n [91] rmarkdown_2.28          farver_2.1.2            TH.data_1.1-2          \n [94] MASS_7.3-60.2           survival_3.6-4          magrittr_2.0.3         \n [97] jsonlite_1.8.9          rstudioapi_0.17.0       RColorBrewer_1.1-3     \n[100] R.devices_2.17.2        CalibrationCurves_2.0.3 rms_6.9-0              \n[103] Hmisc_5.2-1             tictoc_1.2.1            shapviz_0.9.6          \n[106] kernelshap_0.7.0        lattice_0.22-6          patchwork_1.3.0        \n[109] predtools_0.0.3         VSURF_1.2.0             pdp_0.8.2              \n[112] vip_0.4.1               pROC_1.18.5             bayestestR_0.15.3      \n[115] furrr_0.3.1             future_1.34.0           cutpointr_1.2.0        \n[118] tuneRanger_0.7          lhs_1.2.0               mlrMBO_1.1.5.1         \n[121] smoof_1.6.0.3           checkmate_2.3.2         mlr_2.19.2             \n[124] ParamHelpers_1.14.1     ranger_0.17.0           kableExtra_1.4.0       \n[127] gtsummary_2.0.4         labelled_2.13.0         rsample_1.2.1          \n[130] lubridate_1.9.3         forcats_1.0.0           stringr_1.5.1          \n[133] dplyr_1.1.4             purrr_1.0.2             readr_2.1.5            \n[136] tidyr_1.3.1             tibble_3.2.1            ggplot2_3.5.1          \n[139] tidyverse_2.0.0         iml_0.11.3              knitr_1.48             \n\nloaded via a namespace (and not attached):\n[1] pmcalibration_0.1.0 pminternal_0.0.1    mgcv_1.9-1         \n[4] chk_0.9.2           pbapply_1.7-2",
    "crumbs": [
      "Data analysis",
      "Models for white mold"
    ]
  },
  {
    "objectID": "code_fda_exploratory.html",
    "href": "code_fda_exploratory.html",
    "title": "Exploratory analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggthemes)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(patchwork)\n\n# tidyfun is currently not on CRAN. You can install the development version from GitHub with:\n# # install.packages(\"pak\")\n# pak::pak(\"tidyfun/tidyfun\")\nlibrary(tidyfun)\n# my modifications to the geom-spaghetti function (updated ggplot linewidth instead of size):\nsource(here::here(\"FunctionalDataAnalysis\", \"FDAExploratory\", \"geom-spaghetti.R\"))\n\n# theme_half_open is part of the cowplot themes\n# theme_set(theme_half_open(font_size = 12))\n\n\nwm_load &lt;- readr::read_csv(here::here(\"Data\", \"data_model_plus_weather_filtered.csv\"), show_col_types = FALSE)\n\n\n# Simplify things. Keep only the weather-related variables and others needed for calculations\nwm_data &lt;-\n  wm_load %&gt;%\n  dplyr::select(subject, date, planting.date, sampling.date, wm, d2m, t2m_mean, t2m_max, t2m_min, st, sp, sm, rh) %&gt;%\n  # Do the filtering steps before doing any calculations or feature engineering:\n  # wm_load has identical data for each of the sampling dates, which is why there is a filtering step on sampling.date.\n  dplyr::group_by(subject) %&gt;% \n  dplyr::filter(sampling.date == max(sampling.date)) %&gt;%\n  dplyr::ungroup() %&gt;% \n  # Add the response variable (wm present or absent; binary):\n  dplyr::group_by(subject) %&gt;% \n  dplyr::mutate(wm = (mean(wm, na.rm = T) &gt; 0)*1) %&gt;% \n  # wm as a factor:\n  dplyr::mutate(wm = factor(wm, levels = c(0, 1))) %&gt;%\n  dplyr::ungroup() %&gt;% \n  dplyr::filter(!is.na(wm)) %&gt;% \n  # Calculate dap (as a numeric):\n  dplyr::mutate(dap = as.numeric(date - planting.date)) %&gt;%\n  # Convert temperatures from Kelvin to degrees Celsius:\n  dplyr::mutate(across(d2m:st, ~ .x - 273.15)) %&gt;%\n  # dewpoint depression:\n  dplyr::mutate(dpd = t2m_mean - d2m) %&gt;%\n  # surface pressure in kPa:\n  dplyr::mutate(sp = sp/1000) %&gt;%\n  # Ratio of soil temperature to soil moisture:\n  dplyr::mutate(stsm = st/sm) %&gt;%\n  # Difference in max and min temperatures:\n  dplyr::mutate(tdiff = t2m_max - t2m_min) %&gt;%\n  # estimate GDD (base 0). NB: base 0 is reasonable for snap bean; see the Jenni et al. (2000) paper. I think we want GDD to start accumulating the day after planting date onwards...\n  # I use the day after planting, because we don't know exactly what time of the day the field was planted.\n  dplyr::mutate(gddi = ifelse(dap &lt;= 0, 0, (t2m_max + t2m_min)*0.5 - 0)) %&gt;%\n  dplyr::group_by(subject) %&gt;% \n  # We don't want the gddi column after creating gdd:\n  dplyr::mutate(gdd = cumsum(gddi), .keep = \"unused\") %&gt;%\n  dplyr::ungroup() %&gt;%\n  # Keep only the columns you need:\n  dplyr::select(subject, dap, wm:gdd)\n\n\n# To avoid problems with domain-related calculations in fda, check on the range of dap for each subject.\nwm_data %&gt;%\n  dplyr::group_by(subject) %&gt;%\n  dplyr::summarise(min_dap = min(dap), max_dap = max(dap)) %&gt;%\n  # min_dap is -30 for all subjects, but max_dap varies:\n  # print(n = Inf)\n  # dplyr::pull(max_dap) %&gt;%\n  # range() # 50 to 77\n  ggplot(., aes(x = max_dap)) +\n  geom_histogram(binwidth = 1, fill = \"#69b3a2\", colour = \"black\", alpha = 0.8) +\n  theme_bw() +\n  xlab(\"Max dap\") +\n  ylab(\"Frequency\")\n\n# The empirical cdf for max_dap:\nwm_data %&gt;%\n  dplyr::group_by(subject) %&gt;%\n  dplyr::summarise(min_dap = min(dap), max_dap = max(dap)) %&gt;%\n  ggplot(., aes(max_dap)) + \n  stat_ecdf(geom = \"step\") +\n  theme_bw() +\n  labs(x = \"Max dap\", y = \"ECDF\")\n\n\n# Create a tf object:\nfoo_df &lt;-\n  wm_data %&gt;%\n  dplyr::select(subject, dap, wm, d2m) %&gt;% \n  # Not restricting the dap range (max_dap is variable by subject):\n  tf_nest(d2m, .id = subject, .arg = dap)\n\n# Plot the data for a few subjects:  \nfoo_df %&gt;% \n  dplyr::filter(subject %in% 1:5) %&gt;%\n  ggplot(aes(tf = d2m, colour = factor(wm))) + \n  geom_spaghetti(alpha = 1)\n\n\n# The mean for each wm group, then a lowess smooth.\n# Of course, you have to choose a proper span parameter (f argument) to avoid over-smoothing. \n# tf_smooth uses f = .15 as the default instead of the typical .75\nfoo_df %&gt;% \n  dplyr::group_by(wm) %&gt;% \n  dplyr::summarize(mean_d2m = mean(d2m)) %&gt;% \n  dplyr::mutate(smooth_mean = tf_smooth(mean_d2m, method = \"lowess\", f = 0.15)) %&gt;%\n  ggplot(aes(tf = smooth_mean, color = wm)) +\n  geom_spaghetti(linewidth = 1.25, alpha = 1)\n\n# Using a spline basis representation to smooth the means:\nfoo_df %&gt;% \n  dplyr::group_by(wm) %&gt;% \n  dplyr::summarize(mean_d2m = mean(d2m)) %&gt;% \n  dplyr::mutate(smooth_mean = tfb(mean_d2m)) %&gt;%\n  ggplot(aes(tf = smooth_mean, color = wm)) +\n  geom_spaghetti(linewidth = 1.25, alpha = 1) +\n  scale_x_continuous(breaks = seq(-30, 70, by = 10)) +\n  annotate(\"rect\", ymin = -Inf, ymax = Inf, xmin = 35, xmax = 50, \n           fill = \"steelblue\", alpha = 0.2) +\n  geom_vline(xintercept = 0, color = \"gray\", linetype = \"dashed\") +\n  scale_color_colorblind(labels = c( \"Absent\",\"Present\")) +\n  theme_bw() +\n  labs(x = \"Days relative to sowing\",\n       y = \"foo\",\n       color = \"White mold\") +\n  theme(legend.position = \"bottom\")\n\n\n# Next, want to plot the difference between the wm curves\nx &lt;-\n  foo_df %&gt;% \n  group_by(wm) %&gt;% \n  summarize(mean_d2m = mean(d2m)) %&gt;% \n  # Will use a lowess smoother here instead of spline basis:\n  mutate(smooth_mean = tf_smooth(mean_d2m, method = \"lowess\", f = 0.15)) %&gt;%\n  # mutate(smooth_mean = tfb(mean_d2m))\n  # Prepping to unnest:\n  select(-mean_d2m) %&gt;%\n  tf_unnest(cols = \"smooth_mean\") %&gt;%\n  # These steps are to be able to get the difference between the two mean curves\n  pivot_wider(names_from = wm, values_from = smooth_mean_value, names_prefix = \"wm=\") %&gt;%\n  mutate(diff = `wm=1` - `wm=0`)\n\n\ntibble(id = 1, dap = x$smooth_mean_arg, diff = x$diff) %&gt;%\n  # Get back into tfd format:\n  tf_nest(diff, .id = id, .arg = dap) %&gt;%\n  ggplot(aes(tf = diff)) +\n  geom_spaghetti(linewidth = 1.25, alpha = 1, color = \"black\") +\n  geom_hline(yintercept = 0, color = \"gray\", linetype = \"dashed\") +\n  theme_bw() +\n  labs(x = \"Days relative to sowing\",\n       y = \"Difference\",\n       title = \"foo\")\n\n\n# GOAL: Generalize the code for plotting the mean curves and difference between the mean curves:\n\nwm.tfd &lt;- function(x) {\n  # Create a tidy functional data object for a weather variable\n  # Args:\n  #  x = unquoted variable name\n  # Returns:\n  #  a tidy functional data object\n  #\n  .x = enquo(x)\n  \n  # Create a tf object:\n  df &lt;-\n    wm_data %&gt;%\n    dplyr::select(subject, dap, wm, !!.x) %&gt;% \n    # Not restricting the dap range (max_dap is variable by subject):\n    tf_nest(!!.x, .id = subject, .arg = dap)\n  \n  return(df)\n}\n\n\nwm.curves &lt;- function(x, .ylab = NULL, ...) {\n  # Plot the smoothed mean curves for wm = 0 and wm = 1\n  # Args:\n  #  x = unquoted variable name\n  # Returns:\n  #  a ggplot of the smoothed mean curves for wm = 0 and wm = 1\n  .x = enquo(x)\n  \n  # Create a tf object:\n  suppressMessages(\n    wm_data %&gt;%\n      dplyr::select(subject, dap, wm, !!.x) %&gt;% \n      # Create a tf object:\n      # Not restricting the dap range (max_dap is variable by subject):\n      tf_nest(!!.x, .id = subject, .arg = dap) %&gt;%\n      dplyr::group_by(wm) %&gt;%\n      # Get the means:\n      dplyr::summarize(var_mean = mean(!!.x)) %&gt;%\n      # We used suppressMessages() to suppress output generated by the tfb arg:\n      # Using a spline basis representation to smooth the means:\n      dplyr::mutate(smooth_mean = tfb(var_mean)) %&gt;%\n      ggplot(aes(tf = smooth_mean, color = wm)) +\n      geom_spaghetti(linewidth = 2, alpha = 1) +\n      scale_x_continuous(breaks = seq(-30, 70, by = 10)) +\n      annotate(\"rect\", ymin = -Inf, ymax = Inf, xmin = 35, xmax = 50,\n               fill = \"steelblue\", alpha = 0.2) +\n      geom_vline(xintercept = 0, color = \"gray\", linetype = \"dashed\") +\n      ggthemes::scale_color_colorblind(labels = c( \"Absent\",\"Present\")) +\n      theme_bw() +\n      labs(x = \"Days relative to planting\",\n           y = .ylab,\n           color = \"White mold\") +\n      theme(axis.title.y = element_text(size = 8)) +\n      theme(\n        axis.text.x = element_text(size = 7),  # Decrease x-axis tick label size\n        axis.text.y = element_text(size = 7)  # Decrease y-axis tick label size\n        ) +\n      theme(legend.position = \"bottom\")\n    )\n} # end function wm.curves\n\n\nwm.diff.curve &lt;- function(x, .span = 0.15, my.title = NULL) {\n  # Plot the difference between the wm curves\n  # Args:\n  #  x = unquoted variable name\n  #  .span = span for the lowess smoother\n  # Returns:\n  #  a ggplot of the smoothed difference between the wm = 0 and wm = 1 mean curves\n  \n  .x = enquo(x)\n  \n  df &lt;- wm.tfd(!!.x)\n  \n  z &lt;-\n    suppressMessages(\n      df %&gt;% \n      dplyr::group_by(wm) %&gt;% \n      dplyr::summarize(var_mean = mean(!!.x)) %&gt;% \n      # Will use a lowess smoother here instead of spline basis:\n      dplyr::mutate(smooth_mean = tf_smooth(var_mean, method = \"lowess\", f = .span)) %&gt;%\n      # dplyr::mutate(smooth_mean = tfb(var_mean)) %&gt;%\n      # Prepping to unnest:\n      dplyr::select(-var_mean) %&gt;%\n      tf_unnest(cols = \"smooth_mean\") %&gt;%\n      # These steps are to be able to get the difference between the two mean curves\n      tidyr::pivot_wider(names_from = wm, values_from = smooth_mean_value, \n                         names_prefix = \"wm=\") %&gt;%\n      dplyr::mutate(diff = `wm=1` - `wm=0`) )\n  \n  tibble(id = 1, dap = z$smooth_mean_arg, diff = z$diff) %&gt;%\n    # Get back into tfd format:\n    tf_nest(diff, .id = id, .arg = dap) %&gt;%\n    ggplot(aes(tf = diff)) +\n    geom_spaghetti(linewidth = 1.25, alpha = 1, color = \"black\") +\n    scale_x_continuous(breaks = seq(-30, 70, by = 10)) +\n    annotate(\"rect\", ymin = -Inf, ymax = Inf, xmin = 35, xmax = 50, \n           fill = \"steelblue\", alpha = 0.2) +\n    geom_hline(yintercept = 0, color = \"gray\", linetype = \"dashed\") +\n    geom_vline(xintercept = 0, color = \"gray\", linetype = \"dashed\") +\n    theme_bw() +\n    labs(x = \"Days relative to sowing\",\n         y = \"Difference\",\n         title = my.title)\n  }  # end function wm.diff.curve\n\n\n# Example of use:\n# wm.diff.curve(d2m, .span = 0.15)",
    "crumbs": [
      "Functional data analysis",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "code_fda_exploratory.html#dew-point",
    "href": "code_fda_exploratory.html#dew-point",
    "title": "Exploratory analysis",
    "section": "Dew point",
    "text": "Dew point\n\nMeans\n\n# d2m = dewpoint temperature (2m)\nwm.curves(d2m, .ylab = \"Dew point (°C)\")\n\n\n\n\n\n\n\n\n\n\nDifference\n\nwm.diff.curve(d2m, .span = 0.2, my.title = \"Dew point\")",
    "crumbs": [
      "Functional data analysis",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "code_fda_exploratory.html#mean-air-temperature",
    "href": "code_fda_exploratory.html#mean-air-temperature",
    "title": "Exploratory analysis",
    "section": "Mean air temperature",
    "text": "Mean air temperature\n\nMeans\n\n# t2m_mean = mean air temperature (2m)\nwm.curves(t2m_mean, .ylab = \"Mean air temperature (°C)\")\n\n\n\n\n\n\n\n\n\n\nDifference\n\nwm.diff.curve(t2m_mean, .span = 0.2, my.title = \"Mean air temperature\")",
    "crumbs": [
      "Functional data analysis",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "code_fda_exploratory.html#max-air-temperature",
    "href": "code_fda_exploratory.html#max-air-temperature",
    "title": "Exploratory analysis",
    "section": "Max air temperature",
    "text": "Max air temperature\n\nMeans\n\n# t2m_max = max air temperature (2m)\nwm.curves(t2m_max, .ylab = \"Max. air temperature (°C)\")\n\n\n\n\n\n\n\n\n\n\nDifference\n\nwm.diff.curve(t2m_max, .span = .2, my.title = \"Max. air temperature\")",
    "crumbs": [
      "Functional data analysis",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "code_fda_exploratory.html#min-air-temperature",
    "href": "code_fda_exploratory.html#min-air-temperature",
    "title": "Exploratory analysis",
    "section": "Min air temperature",
    "text": "Min air temperature\n\nMeans\n\n# t2m_min = min air temperature (2m)\nwm.curves(t2m_min, .ylab = \"Min. air temperature (°C)\")\n\n\n\n\n\n\n\n\n\n\nDifference\n\nwm.diff.curve(t2m_min, my.title = \"Min. air temperature\")",
    "crumbs": [
      "Functional data analysis",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "code_fda_exploratory.html#max---min-air-temperature",
    "href": "code_fda_exploratory.html#max---min-air-temperature",
    "title": "Exploratory analysis",
    "section": "Max - Min air temperature",
    "text": "Max - Min air temperature\n\nMeans\n\nwm.curves(tdiff, .ylab = \"Max. - Min. air temperature (°C)\")\n\n\n\n\n\n\n\n\n\n\nDifference\n\nwm.diff.curve(tdiff, my.title = \"Max. - Min. air temperature\")",
    "crumbs": [
      "Functional data analysis",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "code_fda_exploratory.html#dewpoint-depression",
    "href": "code_fda_exploratory.html#dewpoint-depression",
    "title": "Exploratory analysis",
    "section": "Dewpoint depression",
    "text": "Dewpoint depression\n\nMeans\n\n# dpd = dewpoint depression\nwm.curves(dpd, .ylab = \"Dew point depression (°C)\")\n\n\n\n\n\n\n\n\n\n\nDifference\n\nwm.diff.curve(dpd, my.title = \"Dew point depression\")",
    "crumbs": [
      "Functional data analysis",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "code_fda_exploratory.html#soil-temperature",
    "href": "code_fda_exploratory.html#soil-temperature",
    "title": "Exploratory analysis",
    "section": "Soil temperature",
    "text": "Soil temperature\n\nMeans\n\n# st = soil temperature \nwm.curves(st, .ylab = \"Soil temperature (°C)\")\n\n\n\n\n\n\n\n\n\n\nDifference\n\nwm.diff.curve(st, my.title = \"Soil temperature\")",
    "crumbs": [
      "Functional data analysis",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "code_fda_exploratory.html#soil-moisture",
    "href": "code_fda_exploratory.html#soil-moisture",
    "title": "Exploratory analysis",
    "section": "Soil moisture",
    "text": "Soil moisture\n\nMeans\n\n# sm = soil moisture\nwm.curves(sm, .ylab = \"Soil Moisture (m³/m³)\")\n\n\n\n\n\n\n\n\n\n\nDifference\n\nwm.diff.curve(sm, my.title = \"Soil Moisture\")",
    "crumbs": [
      "Functional data analysis",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "code_fda_exploratory.html#ratio-of-soil-temperature-soil-moisture",
    "href": "code_fda_exploratory.html#ratio-of-soil-temperature-soil-moisture",
    "title": "Exploratory analysis",
    "section": "Ratio of soil temperature: soil moisture",
    "text": "Ratio of soil temperature: soil moisture\n\nMeans\n\n# stsm = Ratio of soil temperature: soil moisture\nwm.curves(stsm, .ylab = \"Soil temperature: Soil moisture\")\n\n\n\n\n\n\n\n\n\n\nDifference\n\nwm.diff.curve(stsm, my.title = \"Soil temperature: Soil moisture\")",
    "crumbs": [
      "Functional data analysis",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "code_fda_exploratory.html#surface-pressure",
    "href": "code_fda_exploratory.html#surface-pressure",
    "title": "Exploratory analysis",
    "section": "Surface pressure",
    "text": "Surface pressure\n\nMeans\n\n# sp = surface pressure\nwm.curves(sp, .ylab = \"Surface pressure (kPa)\")\n\n\n\n\n\n\n\n\n\n\nDifference\n\nwm.diff.curve(sp, .span = .2, my.title = \"Surface pressure\")",
    "crumbs": [
      "Functional data analysis",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "code_fda_exploratory.html#relative-humidity",
    "href": "code_fda_exploratory.html#relative-humidity",
    "title": "Exploratory analysis",
    "section": "Relative humidity",
    "text": "Relative humidity\n\nMeans\n\n# rh = relative humidity\nwm.curves(rh, .ylab = \"Relative Humidity (%)\")\n\n\n\n\n\n\n\n\n\n\nDifference\n\nwm.diff.curve(rh, .span = .2, my.title = \"Relative Humidity\")",
    "crumbs": [
      "Functional data analysis",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "code_fda_exploratory.html#growing-degree-days",
    "href": "code_fda_exploratory.html#growing-degree-days",
    "title": "Exploratory analysis",
    "section": "Growing degree days",
    "text": "Growing degree days\n\nMeans\n\n# gdd = growing degree days\nwm.curves(gdd, .ylab = \"Growing degree days\")\n\n\n\n\n\n\n\n\n\n\nDifference\n\nwm.diff.curve(gdd, .span= 0.3, my.title = \"Growing degree days\")",
    "crumbs": [
      "Functional data analysis",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "code_fda_exploratory.html#session-info",
    "href": "code_fda_exploratory.html#session-info",
    "title": "Exploratory analysis",
    "section": "Session Info",
    "text": "Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Portuguese_Brazil.utf8  LC_CTYPE=Portuguese_Brazil.utf8   \n[3] LC_MONETARY=Portuguese_Brazil.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Portuguese_Brazil.utf8    \n\ntime zone: America/Sao_Paulo\ntzcode source: internal\n\nattached base packages:\n[1] grid      stats     graphics  grDevices datasets  utils     methods  \n[8] base     \n\nother attached packages:\n [1] tidyfun_0.0.98  tf_0.3.4        patchwork_1.3.0 gridExtra_2.3  \n [5] ggthemes_5.1.0  lubridate_1.9.3 forcats_1.0.0   stringr_1.5.1  \n [9] dplyr_1.1.4     purrr_1.0.2     readr_2.1.5     tidyr_1.3.1    \n[13] tibble_3.2.1    ggplot2_3.5.1   tidyverse_2.0.0 knitr_1.48     \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5       xfun_0.48          htmlwidgets_1.6.4  GGally_2.2.1      \n [5] lattice_0.22-6     tzdb_0.4.0         vctrs_0.6.5        tools_4.4.1       \n [9] generics_0.1.3     parallel_4.4.1     fansi_1.0.6        pkgconfig_2.0.3   \n[13] Matrix_1.7-0       checkmate_2.3.2    RColorBrewer_1.1-3 lifecycle_1.0.4   \n[17] compiler_4.4.1     farver_2.1.2       textshaping_0.4.0  munsell_0.5.1     \n[21] htmltools_0.5.8.1  yaml_2.3.10        pracma_2.4.4       crayon_1.5.3      \n[25] pillar_1.9.0       nlme_3.1-164       ggstats_0.7.0      tidyselect_1.2.1  \n[29] digest_0.6.37      mvtnorm_1.3-2      stringi_1.8.4      labeling_0.4.3    \n[33] splines_4.4.1      cowplot_1.1.3      rprojroot_2.0.4    fastmap_1.2.0     \n[37] here_1.0.1         colorspace_2.1-1   cli_3.6.3          magrittr_2.0.3    \n[41] utf8_1.2.4         withr_3.0.2        scales_1.3.0       backports_1.5.0   \n[45] bit64_4.5.2        timechange_0.3.0   rmarkdown_2.28     bit_4.5.0         \n[49] ragg_1.3.3         zoo_1.8-12         hms_1.1.3          evaluate_1.0.1    \n[53] mgcv_1.9-1         rlang_1.1.4        Rcpp_1.0.13        glue_1.8.0        \n[57] renv_1.1.2         vroom_1.6.5        rstudioapi_0.17.0  jsonlite_1.8.9    \n[61] R6_2.5.1           plyr_1.8.9         systemfonts_1.1.0",
    "crumbs": [
      "Functional data analysis",
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "code_canopy_closure_estimation.html",
    "href": "code_canopy_closure_estimation.html",
    "title": "Canopy closure",
    "section": "",
    "text": "Estimate canopy closure at 35 dap",
    "crumbs": [
      "Data sources",
      "Canopy closure"
    ]
  },
  {
    "objectID": "code_canopy_closure_estimation.html#load-and-process-the-data",
    "href": "code_canopy_closure_estimation.html#load-and-process-the-data",
    "title": "Canopy closure",
    "section": "Load and process the data",
    "text": "Load and process the data\n\n# The observational (survey) matrix:\nload(here::here(\"Data\", \"Survey.RData\"))  # df\n\n\n# Preliminary exploration of the variables:\ndf1 &lt;- \n  df %&gt;% \n  # Filter out the PA fields (Potter county):\n  dplyr::filter(! county == \"Potter\") %&gt;% \n  # Agronomic variables from the observational data to potentially use to predict canopy closure.\n  dplyr::select(subject, soil.type, drainage.class, hydro.group, vg) %&gt;%\n  dplyr::filter(complete.cases(.)) %&gt;% \n  dplyr::distinct(.)\n  \n\nsummary(df1)\n\n# soil.type: 106 groups!! Too many, no clear way to collapse. DO NOT USE..\ndf1 %&gt;% dplyr::count(soil.type)\n\n# drainage.class: imbalance in groups. Collapse into well-drained and poorly-drained\ndf1 %&gt;% dplyr::count(drainage.class)\n\n# hydro.group: Collapse the dual categories into group D\ndf1 %&gt;% dplyr::count(hydro.group)\n\nrm(df1)\n\n\n# The agronomic data we'll use for predicting canopy closure\nagron &lt;-\n  df %&gt;% \n  dplyr::filter(! county == \"Potter\") %&gt;% \n  # Filter out the missing canopy closure, location data:\n  dplyr::filter(!is.na(can.closure), !is.na(latitude), !is.na(longitude)) %&gt;% \n  # Collapse drainage.class into two groups:\n  dplyr::mutate(drainage = \n                  forcats::fct_collapse(drainage.class,\n                                        `Poorly_Drained` = c(\"Somewhat poorly drained\", \"Poorly drained\", \"Very poorly drained\"),\n                                        `Well_Drained` = c(\"Somewhat excessively drained\", \"Well drained\", \"Moderately well drained\"))) %&gt;%\n  # Collapse the dual categories of hydro.group into group D (natural condition):\n  dplyr::mutate(hydrol = \n                  forcats::fct_collapse(hydro.group,\n                                        `A` = \"A\",\n                                        `B` = \"B\",\n                                        `C` = \"C\",\n                                        `D` = c(\"D\", \"A/D\", \"B/D\", \"C/D\"))) %&gt;%\n  # If dap is &gt;60, then consider the field beyond the optimal harvest time (60 dap): Create a binary variable to represent this:\n  dplyr::mutate(harv.optim = ifelse(dap &lt;= 60, 0, 1)) %&gt;% \n  dplyr::mutate(harv.optim = factor(harv.optim, levels = c(0, 1), labels = c(\"Yes\", \"No\"))) %&gt;% \n  # Selecting vars with no missing values, and which don't have a lot of small obs in categories:\n  dplyr::select(subject, planting.date, sampling.date, drainage, hydrol, year, cd, harv.optim, can.closure) %&gt;% \n  # Removal of duplicated rows:\n  dplyr::distinct()\n\n# No duplicated rows:\nfind.dup.rows(agron)\n\n# A tibble: 0 × 9\n# ℹ 9 variables: subject &lt;int&gt;, planting.date &lt;date&gt;, sampling.date &lt;date&gt;,\n#   drainage &lt;fct&gt;, hydrol &lt;fct&gt;, year &lt;fct&gt;, cd &lt;fct&gt;, harv.optim &lt;fct&gt;,\n#   can.closure &lt;dbl&gt;\n\n\n\n## Examine the `agron` data frame:\n# All the rows are complete:\nsummary(agron)\n# Most of the observational data rows (1081 out of 1194) are within the optimal harvest period:\nagron %&gt;% dplyr::count(harv.optim)\n\n# The distribution of the number of times a field was observed:\n# Most fields were observed 1-4 times during a season: \nagron %&gt;% \n  dplyr::add_count(subject) %&gt;%\n  dplyr::group_by(subject) %&gt;%\n  dplyr::summarise(times_obs = mean(n)) %&gt;%\n  dplyr::group_by(times_obs) %&gt;%\n  dplyr::summarise(count = n()) %&gt;% \n  ggplot(., aes(x = times_obs, y = count)) +\n  geom_point(size = 3) +\n  geom_segment(aes(x = times_obs, xend = times_obs, y = 0, yend = count)) +\n  scale_x_continuous(breaks = 1:10) +\n  labs(x = \"Number of Times Observed\", \n       y = \"Number of Subjects\") +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank())\n\n# 375 subjects:\nagron %&gt;% dplyr::distinct(subject) %&gt;% nrow()  \n\n\nsoils &lt;- \n  read.csv(here::here(\"Data\", \"extracted_soil_data.csv\")) %&gt;% \n  dplyr::select(-longitude, -latitude)\n\n# The descriptions of the vars are here:\n# https://github.com/lhmrosso/XPolaris/\nnames(soils)\n\n [1] \"ph\"      \"om\"      \"clay\"    \"sand\"    \"silt\"    \"bd\"      \"hb\"     \n [8] \"n\"       \"alpha\"   \"ksat\"    \"lambda\"  \"theta_r\" \"theta_s\" \"subject\"\n\nsummary(soils)\n\n       ph             om             clay           sand           silt     \n Min.   :4.72   Min.   :0.179   Min.   : 2.6   Min.   :10.8   Min.   :11.3  \n 1st Qu.:5.86   1st Qu.:0.491   1st Qu.:12.7   1st Qu.:22.1   1st Qu.:40.4  \n Median :6.01   Median :0.523   Median :14.2   Median :29.0   Median :45.1  \n Mean   :5.96   Mean   :0.529   Mean   :14.6   Mean   :31.1   Mean   :43.2  \n 3rd Qu.:6.13   3rd Qu.:0.547   3rd Qu.:16.8   3rd Qu.:33.2   3rd Qu.:48.6  \n Max.   :6.45   Max.   :1.545   Max.   :44.5   Max.   :79.0   Max.   :62.6  \n       bd             hb               n            alpha       \n Min.   :0.61   Min.   :-0.009   Min.   :1.28   Min.   :-0.522  \n 1st Qu.:1.16   1st Qu.: 0.269   1st Qu.:1.33   1st Qu.:-0.316  \n Median :1.19   Median : 0.302   Median :1.35   Median :-0.298  \n Mean   :1.21   Mean   : 0.291   Mean   :1.35   Mean   :-0.288  \n 3rd Qu.:1.24   3rd Qu.: 0.318   3rd Qu.:1.36   3rd Qu.:-0.265  \n Max.   :1.46   Max.   : 0.523   Max.   :1.50   Max.   : 0.013  \n      ksat            lambda         theta_r          theta_s     \n Min.   :-0.046   Min.   :0.272   Min.   :0.0284   Min.   :0.449  \n 1st Qu.: 0.246   1st Qu.:0.312   1st Qu.:0.0521   1st Qu.:0.531  \n Median : 0.307   Median :0.324   Median :0.0544   Median :0.550  \n Mean   : 0.327   Mean   :0.326   Mean   :0.0540   Mean   :0.544  \n 3rd Qu.: 0.389   3rd Qu.:0.332   3rd Qu.:0.0567   3rd Qu.:0.563  \n Max.   : 1.327   Max.   :0.410   Max.   :0.1156   Max.   :0.770  \n    subject   \n Min.   :  1  \n 1st Qu.: 96  \n Median :196  \n Mean   :215  \n 3rd Qu.:344  \n Max.   :440  \n\n# We'll focus on the following:\n# ph = Soil pH in water\n# om = Soil organic matter (%)\n# clay = Clay (%)\n# sand = Sand (%)\n# silt = Silt (%)\n\n# A problem: sand, silt, clay DO NOT add to 100. This is because the POLARIS database is a probabilistic construct.\nsoils %&gt;% \n  dplyr::mutate(x = clay + sand + silt) %&gt;% \n  dplyr::pull(x) %&gt;% \n  summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   71.3    86.9    88.5    88.9    91.0   108.0 \n\nsoils &lt;- \n  soils %&gt;% \n  # Rescale sand, silt, clay so that they add to 100 while respecting the proportionality among them\n  dplyr::mutate(scaling_factor = 100/(clay+sand+silt)) %&gt;% \n  dplyr::mutate(across(c(clay, sand, silt), ~ .x*scaling_factor)) %&gt;% \n  dplyr::select(subject, ph, om, sand, silt, clay) %&gt;% \n  # We want log ratios for sand, silt, clay as they are compositional.\n  # Will use clay as the reference:\n  dplyr::mutate(log_sand_clay = log(sand/clay)) %&gt;% \n  dplyr::mutate(log_silt_clay = log(silt/clay)) %&gt;% \n  dplyr::select(subject, ph, om, log_sand_clay, log_silt_clay)\n\n# No duplicated rows:\nfind.dup.rows(soils)\n\n# A tibble: 0 × 5\n# ℹ 5 variables: subject &lt;int&gt;, ph &lt;dbl&gt;, om &lt;dbl&gt;, log_sand_clay &lt;dbl&gt;,\n#   log_silt_clay &lt;dbl&gt;\n\n\n\nwm_load &lt;- readr::read_csv(here::here(\"Data\", \"data_model_plus_weather_filtered.csv\"), show_col_types = FALSE)\n\nwm_data &lt;-\n  wm_load %&gt;%\n  dplyr::select(subject, date, planting.date, t2m_max, t2m_mean, t2m_min, sm, rh) %&gt;%\n  # wm_load has identical data for each of the sampling dates. This will filter out duplicated rows.\n  dplyr::distinct() %&gt;% \n  dplyr::arrange(subject, planting.date, date) %&gt;% \n  # Calculate dap (as a numeric):\n  dplyr::mutate(dap = as.numeric(date - planting.date)) %&gt;%\n  # Convert temperatures from Kelvin to degrees Celsius:\n  dplyr::mutate(across(c(t2m_max:t2m_min), ~ .x - 273.15)) %&gt;%\n  # Estimate GDD (base 0). NB: base 0 is reasonable for snap bean; see the Jenni et al. (2000) paper.\n  # I think we want GDD to start accumulating the day after planting date onwards...\n  # I use the day after planting, because we don't know exactly what time of the day the field was planted.\n  dplyr::mutate(gdd = ifelse(dap &lt;= 0, 0, (t2m_max + t2m_min)*0.5 - 0)) %&gt;%\n  # Calculate saturation vapor pressure (es):\n  dplyr::mutate(es = 0.61078 * exp((17.27 * t2m_mean) / (t2m_mean + 237.3))) %&gt;% \n  # Calculate actual vapor pressure (ea):\n  dplyr::mutate(ea = (rh / 100) * es) %&gt;% \n  # Calculate VPD (kPa):\n  dplyr::mutate(vpd = es - ea) %&gt;% \n  dplyr::select(subject, date, gdd, sm, vpd)\n\nsummary(wm_data)\n\n    subject         date                 gdd             sm       \n Min.   :  1   Min.   :2006-04-24   Min.   : 0.0   Min.   :0.062  \n 1st Qu.: 99   1st Qu.:2006-09-12   1st Qu.: 0.0   1st Qu.:0.249  \n Median :195   Median :2007-07-22   Median :18.5   Median :0.302  \n Mean   :215   Mean   :2007-08-06   Mean   :13.4   Mean   :0.294  \n 3rd Qu.:344   3rd Qu.:2008-06-11   3rd Qu.:21.7   3rd Qu.:0.351  \n Max.   :440   Max.   :2008-09-29   Max.   :29.2   Max.   :0.434  \n      vpd       \n Min.   :0.016  \n 1st Qu.:0.422  \n Median :0.624  \n Mean   :0.633  \n 3rd Qu.:0.809  \n Max.   :1.835  \n\n# No duplicate rows:\nfind.dup.rows(wm_data)  \n\n# A tibble: 0 × 5\n# ℹ 5 variables: subject &lt;dbl&gt;, date &lt;date&gt;, gdd &lt;dbl&gt;, sm &lt;dbl&gt;, vpd &lt;dbl&gt;\n\n# For each subject, we have planting date and sampling date\n# Our goal is to estimate, at each sampling date:\n# - GDD (accumulated)\n# - sm (mean from planting to dap)\n# - vpd (mean from planting to dap) \n\nsmry.vars &lt;- function(i) {\n  # Create summary variables between planting date and the sampling date for:\n  # GDD (accumulated)\n  # sm (mean from planting to dap)\n  # vpd (mean from planting to dap) \n  \n  # Args:\n  #  i = a numeric value for pulling a row in the ith position\n  # Returns:\n  #  a data frame of the estimated variables\n  \n  # Use the `agron` dataframe to get the subject, planting.date and sampling.date\n  foo &lt;- \n    agron %&gt;% \n    dplyr::slice(i) %&gt;% \n    dplyr::select(subject, planting.date, sampling.date)\n  \n  # Prep from wm_data:\n  prepped_dat &lt;-\n    wm_data %&gt;% \n    dplyr::filter(subject == foo$subject, date &gt;= foo$planting.date, date &lt;= foo$sampling.date) %&gt;% \n    dplyr::arrange(subject, date)\n  \n  \n  # The cumulative gdd from the planting date to the sampling date:\n  c_gdd &lt;- \n    prepped_dat %&gt;% \n    dplyr::mutate(x = cumsum(gdd)) %&gt;%\n    dplyr::pull(x) %&gt;%\n    dplyr::last()\n  \n  # The mean sm (from planting to sampling date):\n  mean_sm &lt;- \n    prepped_dat %&gt;% \n    dplyr::summarise(x = mean(sm)) %&gt;%\n    dplyr::pull(x)\n  \n  # The mean vpd (from planting to sampling date):\n  mean_vpd &lt;- \n    prepped_dat %&gt;% \n    dplyr::summarise(x = mean(vpd)) %&gt;%\n    dplyr::pull(x)\n  \n  # Data frame of the values:\n  data.frame(subject = foo$subject, planting.date = foo$planting.date, sampling.date = foo$sampling.date, c_gdd, mean_sm, mean_vpd)\n  } # end function smry.vars\n\n# Examples of use:\n# smry.vars(254)  \n# smry.vars(255)\n\n# Now apply the function across all the rows of the `agron` data frame and bind the results into a single data frame:\nenv_data &lt;- purrr::map(1:nrow(agron), smry.vars) %&gt;% dplyr::bind_rows()\n\nsummary(env_data)\n\n    subject    planting.date        sampling.date            c_gdd     \n Min.   :  1   Min.   :2006-05-24   Min.   :2006-06-23   Min.   : 191  \n 1st Qu.:124   1st Qu.:2007-06-08   1st Qu.:2007-07-20   1st Qu.: 763  \n Median :196   Median :2007-07-23   Median :2007-09-07   Median : 962  \n Mean   :228   Mean   :2007-09-13   Mean   :2007-10-29   Mean   : 942  \n 3rd Qu.:341   3rd Qu.:2008-06-06   3rd Qu.:2008-07-18   3rd Qu.:1143  \n Max.   :440   Max.   :2008-08-02   Max.   :2008-09-29   Max.   :1626  \n    mean_sm         mean_vpd    \n Min.   :0.079   Min.   :0.328  \n 1st Qu.:0.260   1st Qu.:0.574  \n Median :0.292   Median :0.652  \n Mean   :0.286   Mean   :0.667  \n 3rd Qu.:0.344   3rd Qu.:0.751  \n Max.   :0.396   Max.   :1.019  \n\n# No duplicate rows:\nfind.dup.rows(env_data)\n\n# A tibble: 0 × 6\n# ℹ 6 variables: subject &lt;int&gt;, planting.date &lt;date&gt;, sampling.date &lt;date&gt;,\n#   c_gdd &lt;dbl&gt;, mean_sm &lt;dbl&gt;, mean_vpd &lt;dbl&gt;\n\n\n\n# Load the sunshine duration and rain vars (the `wm_wvars` dataframe):\nload(here::here(\"Openmeteo\", \"wm_WeatherVars.RData\"))  # wm_wvars\n\n# No duplicated rows:\nfind.dup.rows(wm_wvars)\n\n# A tibble: 0 × 6\n# ℹ 6 variables: env &lt;chr&gt;, subject &lt;int&gt;, planting.date &lt;date&gt;,\n#   sampling.date &lt;date&gt;, sundur &lt;dbl&gt;, rain &lt;dbl&gt;\n\n\n\nrm(df, wm_load, smry.vars)\n\nNEXT: Join the separate dataframes together.\n\nnames(agron)\n\n[1] \"subject\"       \"planting.date\" \"sampling.date\" \"drainage\"     \n[5] \"hydrol\"        \"year\"          \"cd\"            \"harv.optim\"   \n[9] \"can.closure\"  \n\nnames(soils)\n\n[1] \"subject\"       \"ph\"            \"om\"            \"log_sand_clay\"\n[5] \"log_silt_clay\"\n\nnames(env_data)\n\n[1] \"subject\"       \"planting.date\" \"sampling.date\" \"c_gdd\"        \n[5] \"mean_sm\"       \"mean_vpd\"     \n\nnames(wm_wvars)\n\n[1] \"env\"           \"subject\"       \"planting.date\" \"sampling.date\"\n[5] \"sundur\"        \"rain\"         \n\n# Joining of the different data frames to arrive at the finalized matrix:\ncc.df &lt;- \n  dplyr::left_join(agron, soils, by = \"subject\") %&gt;% \n  dplyr::left_join(., env_data, by = c(\"subject\", \"planting.date\", \"sampling.date\")) %&gt;% \n  dplyr::left_join(., wm_wvars %&gt;% select(-env), by = c(\"subject\", \"planting.date\", \"sampling.date\")) %&gt;% \n  # Calculate dap:\n  dplyr::mutate(dap = as.numeric(sampling.date - planting.date), .after = \"sampling.date\")\n\n# Just some data checks:\nnames(cc.df)\n\n [1] \"subject\"       \"planting.date\" \"sampling.date\" \"dap\"          \n [5] \"drainage\"      \"hydrol\"        \"year\"          \"cd\"           \n [9] \"harv.optim\"    \"can.closure\"   \"ph\"            \"om\"           \n[13] \"log_sand_clay\" \"log_silt_clay\" \"c_gdd\"         \"mean_sm\"      \n[17] \"mean_vpd\"      \"sundur\"        \"rain\"         \n\nsummary(cc.df)\n\n    subject    planting.date        sampling.date             dap      \n Min.   :  1   Min.   :2006-05-24   Min.   :2006-06-23   Min.   : 9.0  \n 1st Qu.:124   1st Qu.:2007-06-08   1st Qu.:2007-07-20   1st Qu.:37.0  \n Median :196   Median :2007-07-23   Median :2007-09-07   Median :46.0  \n Mean   :228   Mean   :2007-09-13   Mean   :2007-10-29   Mean   :45.8  \n 3rd Qu.:341   3rd Qu.:2008-06-06   3rd Qu.:2008-07-18   3rd Qu.:56.0  \n Max.   :440   Max.   :2008-08-02   Max.   :2008-09-29   Max.   :77.0  \n           drainage   hydrol    year                 cd      harv.optim\n Well_Drained  :888   A:197   2006:174   Central Lakes:318   Yes:1081  \n Poorly_Drained:306   B: 58   2007:595   Great Lakes  :876   No : 113  \n                      C:170   2008:425                                 \n                      D:769                                            \n                                                                       \n                                                                       \n  can.closure          ph             om        log_sand_clay  \n Min.   :  0.0   Min.   :4.72   Min.   :0.179   Min.   :-0.99  \n 1st Qu.: 30.5   1st Qu.:5.83   1st Qu.:0.486   1st Qu.: 0.28  \n Median : 43.2   Median :6.01   Median :0.523   Median : 0.76  \n Mean   : 42.7   Mean   :5.95   Mean   :0.527   Mean   : 0.81  \n 3rd Qu.: 53.3   3rd Qu.:6.12   3rd Qu.:0.548   3rd Qu.: 1.03  \n Max.   :147.3   Max.   :6.45   Max.   :1.545   Max.   : 3.41  \n log_silt_clay        c_gdd         mean_sm         mean_vpd         sundur   \n Min.   :-0.147   Min.   : 191   Min.   :0.079   Min.   :0.328   Min.   :127  \n 1st Qu.: 1.033   1st Qu.: 763   1st Qu.:0.260   1st Qu.:0.574   1st Qu.:439  \n Median : 1.111   Median : 962   Median :0.292   Median :0.652   Median :531  \n Mean   : 1.106   Mean   : 942   Mean   :0.286   Mean   :0.667   Mean   :530  \n 3rd Qu.: 1.182   3rd Qu.:1143   3rd Qu.:0.344   3rd Qu.:0.751   3rd Qu.:629  \n Max.   : 1.587   Max.   :1626   Max.   :0.396   Max.   :1.019   Max.   :922  \n      rain    \n Min.   : 10  \n 1st Qu.: 98  \n Median :130  \n Mean   :142  \n 3rd Qu.:179  \n Max.   :364  \n\nskimr::skim(cc.df)\n\n\nData summary\n\n\nName\ncc.df\n\n\nNumber of rows\n1194\n\n\nNumber of columns\n19\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nDate\n2\n\n\nfactor\n5\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nplanting.date\n0\n1\n2006-05-24\n2008-08-02\n2007-07-23\n135\n\n\nsampling.date\n0\n1\n2006-06-23\n2008-09-29\n2007-09-07\n144\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndrainage\n0\n1\nFALSE\n2\nWel: 888, Poo: 306\n\n\nhydrol\n0\n1\nFALSE\n4\nD: 769, A: 197, C: 170, B: 58\n\n\nyear\n0\n1\nFALSE\n3\n200: 595, 200: 425, 200: 174\n\n\ncd\n0\n1\nFALSE\n2\nGre: 876, Cen: 318\n\n\nharv.optim\n0\n1\nFALSE\n2\nYes: 1081, No: 113\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsubject\n0\n1\n227.58\n126.35\n1.00\n124.00\n195.50\n341.00\n440.00\n▃▇▃▅▆\n\n\ndap\n0\n1\n45.78\n11.99\n9.00\n37.00\n46.00\n56.00\n77.00\n▁▅▇▇▁\n\n\ncan.closure\n0\n1\n42.74\n22.55\n0.00\n30.48\n43.18\n53.34\n147.32\n▃▇▂▁▁\n\n\nph\n0\n1\n5.95\n0.27\n4.72\n5.83\n6.01\n6.12\n6.45\n▁▁▂▇▅\n\n\nom\n0\n1\n0.53\n0.11\n0.18\n0.49\n0.52\n0.55\n1.55\n▁▇▁▁▁\n\n\nlog_sand_clay\n0\n1\n0.81\n0.81\n-0.99\n0.28\n0.76\n1.03\n3.41\n▁▇▆▂▁\n\n\nlog_silt_clay\n0\n1\n1.11\n0.18\n-0.15\n1.03\n1.11\n1.18\n1.59\n▁▁▁▇▂\n\n\nc_gdd\n0\n1\n941.79\n252.20\n191.34\n762.62\n962.17\n1143.18\n1625.66\n▁▅▇▇▁\n\n\nmean_sm\n0\n1\n0.29\n0.06\n0.08\n0.26\n0.29\n0.34\n0.40\n▁▃▂▇▆\n\n\nmean_vpd\n0\n1\n0.67\n0.12\n0.33\n0.57\n0.65\n0.75\n1.02\n▁▆▇▅▁\n\n\nsundur\n0\n1\n530.28\n128.54\n127.23\n438.80\n531.19\n628.70\n922.31\n▁▅▇▆▁\n\n\nrain\n0\n1\n142.13\n60.53\n9.70\n97.70\n129.60\n179.10\n363.90\n▂▇▅▂▁\n\n\n\n\n# No. subjects: 375\nunique(cc.df$subject) %&gt;% length()\n\n[1] 375\n\n\n\nsave(cc.df, file = here::here(\"CanopyClosure\", \"canclos.RData\"))",
    "crumbs": [
      "Data sources",
      "Canopy closure"
    ]
  },
  {
    "objectID": "code_canopy_closure_estimation.html#data-dictionary",
    "href": "code_canopy_closure_estimation.html#data-dictionary",
    "title": "Canopy closure",
    "section": "Data dictionary",
    "text": "Data dictionary\n\n\nccdf_dictionary &lt;- ccdf_labelled |&gt; \n  generate_dictionary()\n\nccdf_dictionary |&gt; \n  make_kable()\n\n\n\n\n\npos\nvariable\nlabel\ncol_type\nmissing\nlevels\nvalue_labels\n\n\n\n\n1\n1\nsubject\nSnap bean field\nint\n0\nNULL\nNULL\n\n\n2\n2\nplanting.date\nField's planting date (pd)\ndate\n0\nNULL\nNULL\n\n\n3\n3\nsampling.date\nDate on which the field was observed/sampled (sd)\ndate\n0\nNULL\nNULL\n\n\n4\n4\ndap\nNo. days after planting\ndbl\n0\nNULL\nNULL\n\n\n5\n5\ndrainage\nSoil drainage class\nfct\n0\nWell_Drained , Poorly_Drained\nNULL\n\n\n6\n6\nhydrol\nSoil hydrological group\nfct\n0\nA, B, C, D\nNULL\n\n\n7\n7\nyear\nYear (growing season)\nfct\n0\n2006, 2007, 2008\nNULL\n\n\n8\n8\ncd\nClimate division\nfct\n0\nCentral Lakes, Great Lakes\nNULL\n\n\n9\n9\nharv.optim\nField harvested &lt;=60 dap\nfct\n0\nYes, No\nNULL\n\n\n10\n10\ncan.closure\nCanopy gap (cm)\ndbl\n0\nNULL\nNULL\n\n\n11\n11\nph\nSoil pH\ndbl\n0\nNULL\nNULL\n\n\n12\n12\nom\nSoil organic matter content (%)\ndbl\n0\nNULL\nNULL\n\n\n13\n13\nlog_sand_clay\nLogratio sand:clay\ndbl\n0\nNULL\nNULL\n\n\n14\n14\nlog_silt_clay\nLogratio silt:clay\ndbl\n0\nNULL\nNULL\n\n\n15\n15\nc_gdd\nThe cumulative growing degree days from pd to sd\ndbl\n0\nNULL\nNULL\n\n\n16\n16\nmean_sm\nMean vsw (m³/m³) from pd to sd\ndbl\n0\nNULL\nNULL\n\n\n17\n17\nmean_vpd\nMean vapor pressure deficit (kPa) from pd to sd\ndbl\n0\nNULL\nNULL\n\n\n18\n18\nsundur\nTotal sunshine duration (hours) from pd to sd\ndbl\n0\nNULL\nNULL\n\n\n19\n19\nrain\nTotal rain (mm) from pd to sd\ndbl\n0\nNULL\nNULL",
    "crumbs": [
      "Data sources",
      "Canopy closure"
    ]
  },
  {
    "objectID": "code_canopy_closure_estimation.html#the-variables",
    "href": "code_canopy_closure_estimation.html#the-variables",
    "title": "Canopy closure",
    "section": "The variables",
    "text": "The variables\n\nccdf_labelled |&gt; \n  dplyr::select(-subject, -planting.date, -sampling.date, -can.closure) |&gt;\n  # Arrange names so that categorical variables are first:\n  dplyr::select(year, drainage, hydrol, cd, harv.optim, dap, ph:rain) |&gt;\n  tbl_summary() |&gt; \n  bold_labels()\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 1,1941\n\n\n\n\nYear (growing season)\n\n\n\n\n    2006\n174 (15%)\n\n\n    2007\n595 (50%)\n\n\n    2008\n425 (36%)\n\n\nSoil drainage class\n\n\n\n\n    Well_Drained\n888 (74%)\n\n\n    Poorly_Drained\n306 (26%)\n\n\nSoil hydrological group\n\n\n\n\n    A\n197 (16%)\n\n\n    B\n58 (4.9%)\n\n\n    C\n170 (14%)\n\n\n    D\n769 (64%)\n\n\nClimate division\n\n\n\n\n    Central Lakes\n318 (27%)\n\n\n    Great Lakes\n876 (73%)\n\n\nField harvested &lt;=60 dap\n1,081 (91%)\n\n\nNo. days after planting\n46 (37, 56)\n\n\nSoil pH\n6.01 (5.83, 6.12)\n\n\nSoil organic matter content (%)\n0.52 (0.49, 0.55)\n\n\nLogratio sand:clay\n0.76 (0.28, 1.04)\n\n\nLogratio silt:clay\n1.11 (1.03, 1.18)\n\n\nThe cumulative growing degree days from pd to sd\n962 (763, 1,143)\n\n\nMean vsw (m³/m³) from pd to sd\n0.29 (0.26, 0.34)\n\n\nMean vapor pressure deficit (kPa) from pd to sd\n0.65 (0.57, 0.75)\n\n\nTotal sunshine duration (hours) from pd to sd\n531 (439, 629)\n\n\nTotal rain (mm) from pd to sd\n130 (98, 179)\n\n\n\n1 n (%); Median (Q1, Q3)",
    "crumbs": [
      "Data sources",
      "Canopy closure"
    ]
  },
  {
    "objectID": "code_canopy_closure_estimation.html#tuning-a-ranger-model",
    "href": "code_canopy_closure_estimation.html#tuning-a-ranger-model",
    "title": "Canopy closure",
    "section": "Tuning a ranger model",
    "text": "Tuning a ranger model\n\nlibrary(ranger)\nlibrary(tuneRanger)\nlibrary(mlr)\n\n# Set seed for reproducibility:\nset.seed(14092)\n\n# For tuneRanger, a mlr task has to be created:\ncc.task &lt;- mlr::makeRegrTask(data = x, target = \"can.closure\")\n\n\n## with tuneRanger (following Probst et al and the documentation)\n# Rough Estimation of the Tuning time\nestimateTimeTuneRanger(cc.task)\n\nApproximated time for tuning: 2M 18S\n\n# Tuning process:\nrf3 &lt;- tuneRanger(cc.task, num.trees = 1000)\n\n# Save the fitted model so that you don't have to re-tune:\nsave(rf3, file = here::here(\"CanopyClosure\", \"tunedRF.RData\"))\n\n\n# Load the fitted model:\nload(here::here(\"CanopyClosure\", \"tunedRF.RData\"))  # rf3\n\n# Mean of best 5 % of the results\nrf3\n\nRecommended parameter settings: \n  mtry min.node.size sample.fraction\n1    3             2           0.874\nResults: \n  mse exec.time\n1 114     0.192\n\n# Model with the new tuned hyperparameters\nrf3$model\n\nModel for learner.id=regr.ranger; learner.class=regr.ranger\nTrained on: task.id = x; obs = 1194; features = 15\nHyperparameters: num.threads=24,verbose=FALSE,respect.unordered.factors=order,mtry=3,min.node.size=2,sample.fraction=0.874,num.trees=1e+03,replace=FALSE\n\n# recommended parameters\nrf3$recommended.pars\n\n  mtry min.node.size sample.fraction mse exec.time\n1    3             2           0.874 114     0.192\n\n# the OOB RMSE\nsqrt(rf3$model$learner.model$prediction.error)\n\n[1] 10.7\n\n# Prediction\nfitted.vals &lt;- predict(rf3$model, newdata = x)$data\nclass(rf3$model)\n\n[1] \"WrappedModel\"\n\n# The predicted values vs actual values on the data:\n# There is slight under-prediction at high values of can.clos, and over-prediction at low values of can.clos\nfitted.vals %&gt;%\n  ggplot(., aes(x = truth, y = response)) + \n  geom_point(color = \"orange\", alpha = 0.5) + \n  coord_fixed(xlim = c(0, 150), ylim = c(0, 150)) +\n  # The fitted line is blue:\n  geom_smooth(method = lm, formula = 'y ~ x') +\n  geom_abline(slope = 1, intercept = 0, color = \"black\") +\n  theme_bw() +\n  ylab(\"Predicted canopy gap (cm)\") + \n  xlab(\"Actual canopy gap (cm)\") + \n  theme(axis.text.y = element_text(size = 12),\n        axis.text.x = element_text(size = 12, hjust = 0.5),\n        axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14))\n\n\n\n\n\n\n\n\n\nresiduals &lt;- x$can.closure - fitted.vals\n\n# RMSE: 2.52\nsqrt(sum(residuals^2)/nrow(x))\n\n[1] 2.52\n\n\n\n# Prediction of can.clos at 35 dap entails:\n#   - setting dap = 35\n#   - setting harv.optim = 0\n#   - calculating c_gdd, mean_sm, mean_vpd, sundur, rain at 35 dap\n#   - placing all these values in the test data frame\n\n# Make the necessary adjustments to the `agron` data frame:\nagron &lt;-\n  agron %&gt;% \n  dplyr::select(-sampling.date, -can.closure, -harv.optim) %&gt;% \n  # Removal of duplicated rows:\n  dplyr::distinct()\n\n# No duplicates:\nfind.dup.rows(agron)\n\n# A tibble: 0 × 6\n# ℹ 6 variables: subject &lt;int&gt;, planting.date &lt;date&gt;, drainage &lt;fct&gt;,\n#   hydrol &lt;fct&gt;, year &lt;fct&gt;, cd &lt;fct&gt;\n\n###--- New env_data dataframe needed here ---###\n# remove the old env_data object\nrm(env_data)\n\n# Create the new env_data dataframe which holds the estimates of the environ vars to 35 dap:\n# wm_data has to be loaded (from above)\n\nsmry.vars &lt;- function(i) {\n  # Create summary variables between planting date and 35 dap for:\n  # GDD (accumulated to 35 dap)\n  # sm (mean from planting to 35 dap)\n  # vpd (mean from planting to 35 dap) \n  \n  # Args:\n  #  i = a numeric for the ith row\n  # Returns:\n  #  a data frame with the estimated vars representing conditions from planting to 35 dap\n  \n  # Use the `agron` dataframe to get the subject, planting.date and sampling.date\n  foo &lt;- \n    agron %&gt;% \n    dplyr::slice(i) %&gt;% \n    dplyr::select(subject, planting.date) %&gt;% \n    dplyr::mutate(end.date = planting.date + 35)\n  \n  # Prep from wm_data:\n  prepped_dat &lt;-\n    wm_data %&gt;% \n    dplyr::filter(subject == foo$subject, date &gt;= foo$planting.date, date &lt;= foo$end.date) %&gt;% \n    dplyr::arrange(subject, date)\n  \n  \n  # The cumulative gdd from the planting date to 35 dap:\n  c_gdd &lt;- \n    prepped_dat %&gt;% \n    dplyr::mutate(x = cumsum(gdd)) %&gt;%\n    dplyr::pull(x) %&gt;%\n    dplyr::last()\n  \n  # The mean sm (from planting to 35 dap):\n  mean_sm &lt;- \n    prepped_dat %&gt;% \n    dplyr::summarise(x = mean(sm)) %&gt;%\n    dplyr::pull(x)\n  \n  # The mean vpd (from planting to 35 dap):\n  mean_vpd &lt;- \n    prepped_dat %&gt;% \n    dplyr::summarise(x = mean(vpd)) %&gt;%\n    dplyr::pull(x)\n  \n  # Data frame of the values:\n  data.frame(subject = foo$subject, planting.date = foo$planting.date, c_gdd, mean_sm, mean_vpd)\n  } # end function smry.vars\n\n# Now apply the function across all the rows of the `agron` data frame and bind the results into a single data frame:\nenv_data &lt;- purrr::map(1:nrow(agron), smry.vars) %&gt;% dplyr::bind_rows() %&gt;% dplyr::distinct()\n\n# No duplicated rows:\nfind.dup.rows(env_data)\n\n# A tibble: 0 × 5\n# ℹ 5 variables: subject &lt;int&gt;, planting.date &lt;date&gt;, c_gdd &lt;dbl&gt;,\n#   mean_sm &lt;dbl&gt;, mean_vpd &lt;dbl&gt;\n\n###--- ---------------------------------- ---###\n\n# Load the sunshine duration and rain vars at 35 dap (the `wm_wvars_35dap` dataframe):\nload(here::here(\"Openmeteo\", \"wm_WeatherVars_35dap.RData\"))  # wm_wvars_35dap\n\n# There are no duplicate rows:\nfind.dup.rows(wm_wvars_35dap)\n\n# A tibble: 0 × 5\n# ℹ 5 variables: env &lt;chr&gt;, subject &lt;int&gt;, planting.date &lt;date&gt;, sundur &lt;dbl&gt;,\n#   rain &lt;dbl&gt;\n\n# Joining of the different data frames to arrive at the finalized matrix:\ncc.df.35dap &lt;- \n  dplyr::left_join(agron, soils, by = \"subject\") %&gt;% \n  dplyr::left_join(., env_data, by = c(\"subject\", \"planting.date\")) %&gt;% \n  dplyr::left_join(., wm_wvars_35dap %&gt;% select(-env), by = c(\"subject\", \"planting.date\")) %&gt;% \n  dplyr::mutate(dap = 35, harv.optim = 0) %&gt;% \n  dplyr::mutate(harv.optim = factor(harv.optim, levels = c(0, 1), labels = c(\"Yes\", \"No\"))) %&gt;% \n  dplyr::select(-planting.date)\n \n\n# Checks:\nfind.dup.rows(cc.df.35dap)  # none\n\n# A tibble: 0 × 16\n# ℹ 16 variables: subject &lt;int&gt;, drainage &lt;fct&gt;, hydrol &lt;fct&gt;, year &lt;fct&gt;,\n#   cd &lt;fct&gt;, ph &lt;dbl&gt;, om &lt;dbl&gt;, log_sand_clay &lt;dbl&gt;, log_silt_clay &lt;dbl&gt;,\n#   c_gdd &lt;dbl&gt;, mean_sm &lt;dbl&gt;, mean_vpd &lt;dbl&gt;, sundur &lt;dbl&gt;, rain &lt;dbl&gt;,\n#   dap &lt;dbl&gt;, harv.optim &lt;fct&gt;\n\nsummary(cc.df.35dap)\n\n    subject              drainage   hydrol    year                 cd     \n Min.   :  1   Well_Drained  :277   A: 56   2006: 98   Central Lakes:101  \n 1st Qu.: 96   Poorly_Drained: 98   B: 17   2007:153   Great Lakes  :274  \n Median :196                        C: 58   2008:124                      \n Mean   :215                        D:244                                 \n 3rd Qu.:344                                                              \n Max.   :440                                                              \n       ph             om        log_sand_clay   log_silt_clay        c_gdd    \n Min.   :4.72   Min.   :0.179   Min.   :-0.99   Min.   :-0.147   Min.   :570  \n 1st Qu.:5.86   1st Qu.:0.491   1st Qu.: 0.27   1st Qu.: 1.033   1st Qu.:699  \n Median :6.01   Median :0.523   Median : 0.74   Median : 1.099   Median :725  \n Mean   :5.96   Mean   :0.529   Mean   : 0.73   Mean   : 1.099   Mean   :725  \n 3rd Qu.:6.13   3rd Qu.:0.548   3rd Qu.: 0.96   3rd Qu.: 1.168   3rd Qu.:761  \n Max.   :6.45   Max.   :1.545   Max.   : 3.41   Max.   : 1.587   Max.   :816  \n    mean_sm         mean_vpd         sundur         rain            dap    \n Min.   :0.082   Min.   :0.343   Min.   :357   Min.   : 36.0   Min.   :35  \n 1st Qu.:0.271   1st Qu.:0.570   1st Qu.:400   1st Qu.: 81.2   1st Qu.:35  \n Median :0.299   Median :0.641   Median :420   Median :113.6   Median :35  \n Mean   :0.294   Mean   :0.661   Mean   :417   Mean   :122.5   Mean   :35  \n 3rd Qu.:0.347   3rd Qu.:0.751   3rd Qu.:436   3rd Qu.:161.1   3rd Qu.:35  \n Max.   :0.390   Max.   :0.963   Max.   :476   Max.   :241.7   Max.   :35  \n harv.optim\n Yes:375   \n No :  0   \n           \n           \n           \n           \n\nnames(cc.df.35dap)\n\n [1] \"subject\"       \"drainage\"      \"hydrol\"        \"year\"         \n [5] \"cd\"            \"ph\"            \"om\"            \"log_sand_clay\"\n [9] \"log_silt_clay\" \"c_gdd\"         \"mean_sm\"       \"mean_vpd\"     \n[13] \"sundur\"        \"rain\"          \"dap\"           \"harv.optim\"   \n\nnames(x)\n\n [1] \"dap\"           \"drainage\"      \"hydrol\"        \"year\"         \n [5] \"cd\"            \"harv.optim\"    \"can.closure\"   \"ph\"           \n [9] \"om\"            \"log_sand_clay\" \"log_silt_clay\" \"c_gdd\"        \n[13] \"mean_sm\"       \"mean_vpd\"      \"sundur\"        \"rain\"         \n\n# Load the fitted model:\nload(here::here(\"CanopyClosure\", \"tunedRF.RData\"))  # rf3\n\n\n# Prediction:\nfitted.vals &lt;- predict(rf3$model, newdata = cc.df.35dap)$data\nsummary(fitted.vals$response)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   24.5    44.0    50.7    51.7    55.3   124.3 \n\n# Add the predicted can.clos at 35 dap to the cc.df.35dap dataframe:\ncc.df.35dap &lt;-\n  cc.df.35dap %&gt;% \n  dplyr::mutate(cc35 = fitted.vals$response) %&gt;% \n  # Well, all we really need from this is subject and cc35:\n  dplyr::select(subject, cc35)\n  \nsummary(cc.df.35dap)\n\n    subject         cc35      \n Min.   :  1   Min.   : 24.5  \n 1st Qu.: 96   1st Qu.: 44.0  \n Median :196   Median : 50.7  \n Mean   :215   Mean   : 51.7  \n 3rd Qu.:344   3rd Qu.: 55.3  \n Max.   :440   Max.   :124.3  \n\n# And that should do it for getting a var representing canopy closure at 35 dap.\n# Save the final result:\nsave(cc.df.35dap, file = here::here(\"CanopyClosure\", \"cc.df.35dap.RData\"))\n\n\n# Did all the work to get the cc var at 35 dap lead to anything that could be meaningfully used?\n\n# The observational (survey) matrix:\nload(here::here(\"Data\", \"Survey.RData\"))  # df\n\n# Load the canopy closure estimates at 35 dap:\nload(here::here(\"CanopyClosure\", \"cc.df.35dap.RData\"))  # cc.df.35dap\n\nx &lt;-\n  df %&gt;% \n  # Filter out the PA fields (Potter county):\n  dplyr::filter(! county == \"Potter\") %&gt;% \n  dplyr::select(subject, latitude, longitude, sampling.date, wm, vg) %&gt;% \n  dplyr::filter(!is.na(latitude), !is.na(longitude)) %&gt;% \n  dplyr::arrange(subject, sampling.date) %&gt;% \n  dplyr::group_by(subject) %&gt;%\n  # The last sampling date for each field:\n  dplyr::slice_max(sampling.date, n = 1, with_ties = FALSE) %&gt;%\n  dplyr::ungroup() %&gt;% \n  dplyr::filter(!is.na(wm)) %&gt;% \n  dplyr::mutate(wm = ifelse(wm &gt; 0, 1, 0)) %&gt;% \n  # Add cc35 to the main dataframe:\n  dplyr::left_join(cc.df.35dap, by = \"subject\") \n\nsummary(x)\n\n# On average, fields with wm have more closed canopies:\nx %&gt;% \n  dplyr::group_by(wm) %&gt;% \n  dplyr::summarise(n = n(), mean = mean(cc35), median = median(cc35), sd = sd(cc35))\n\n\nx %&gt;% \n  dplyr::add_count(vg, wm) %&gt;%\n  dplyr::group_by(vg, wm) %&gt;% \n  dplyr::summarise(mean_cc35 = mean(cc35), n = mean(n))\n\n# But its value as a predictor may be limited...\nx %&gt;%\n  ggplot(aes(cc35)) +\n  geom_histogram(col = \"white\", bins = 30) +\n  facet_wrap(~ as.factor(wm), ncol = 1, \n             labeller = as_labeller(c(\"0\" = \"No White Mold\", \"1\" = \"White Mold Present\"))) +\n  geom_rug(col = \"blue\", alpha = 0.5) + \n  labs(x = \"Canopy gap at 35 dap (cm)\") +\n  theme_bw()",
    "crumbs": [
      "Data sources",
      "Canopy closure"
    ]
  },
  {
    "objectID": "code_canopy_closure_estimation.html#session-info",
    "href": "code_canopy_closure_estimation.html#session-info",
    "title": "Canopy closure",
    "section": "Session Info",
    "text": "Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Portuguese_Brazil.utf8  LC_CTYPE=Portuguese_Brazil.utf8   \n[3] LC_MONETARY=Portuguese_Brazil.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Portuguese_Brazil.utf8    \n\ntime zone: America/Sao_Paulo\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices datasets  utils     methods  \n[8] base     \n\nother attached packages:\n [1] tuneRanger_0.7      lhs_1.2.0           mlrMBO_1.1.5.1     \n [4] smoof_1.6.0.3       checkmate_2.3.2     mlr_2.19.2         \n [7] ParamHelpers_1.14.1 ranger_0.17.0       gtsummary_2.0.4    \n[10] labelled_2.13.0     kableExtra_1.4.0    lubridate_1.9.3    \n[13] forcats_1.0.0       stringr_1.5.1       dplyr_1.1.4        \n[16] purrr_1.0.2         readr_2.1.5         tidyr_1.3.1        \n[19] tibble_3.2.1        ggplot2_3.5.1       tidyverse_2.0.0    \n[22] knitr_1.48         \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1  viridisLite_0.4.2 farver_2.1.2      fastmap_1.2.0    \n [5] XML_3.99-0.18     digest_0.6.37     timechange_0.3.0  lifecycle_1.0.4  \n [9] survival_3.6-4    parallelMap_1.5.1 magrittr_2.0.3    compiler_4.4.1   \n[13] rlang_1.1.4       sass_0.4.9        tools_4.4.1       utf8_1.2.4       \n[17] yaml_2.3.10       gt_0.11.1         data.table_1.16.2 skimr_2.1.5      \n[21] labeling_0.4.3    htmlwidgets_1.6.4 bit_4.5.0         here_1.0.1       \n[25] xml2_1.3.6        repr_1.1.7        withr_3.0.2       grid_4.4.1       \n[29] fansi_1.0.6       colorspace_2.1-1  scales_1.3.0      BBmisc_1.13      \n[33] cli_3.6.3         rmarkdown_2.28    crayon_1.5.3      generics_0.1.3   \n[37] rstudioapi_0.17.0 tzdb_0.4.0        commonmark_1.9.2  splines_4.4.1    \n[41] base64enc_0.1-3   vctrs_0.6.5       Matrix_1.7-0      jsonlite_1.8.9   \n[45] hms_1.1.3         bit64_4.5.2       systemfonts_1.1.0 DiceKriging_1.6.0\n[49] glue_1.8.0        stringi_1.8.4     gtable_0.3.5      munsell_0.5.1    \n[53] pillar_1.9.0      htmltools_0.5.8.1 R6_2.5.1          rprojroot_2.0.4  \n[57] vroom_1.6.5       evaluate_1.0.1    lattice_0.22-6    markdown_1.13    \n[61] haven_2.5.4       highr_0.11        backports_1.5.0   cards_0.4.0      \n[65] renv_1.1.2        Rcpp_1.0.13       fastmatch_1.1-6   nlme_3.1-164     \n[69] svglite_2.1.3     mgcv_1.9-1        xfun_0.48         pkgconfig_2.0.3",
    "crumbs": [
      "Data sources",
      "Canopy closure"
    ]
  },
  {
    "objectID": "code_weather_white-mold.html",
    "href": "code_weather_white-mold.html",
    "title": "Weather data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(raster)\nlibrary(spatstat)\nlibrary(KrigR)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(sp)\nlibrary(viridis)\nlibrary(ggthemes)\nlibrary(terra)\nlibrary(MetBrewer)\n\nlibrary(ncdf4)",
    "crumbs": [
      "Data sources",
      "Weather data"
    ]
  },
  {
    "objectID": "code_weather_white-mold.html#packages",
    "href": "code_weather_white-mold.html#packages",
    "title": "Weather data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(raster)\nlibrary(spatstat)\nlibrary(KrigR)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(sp)\nlibrary(viridis)\nlibrary(ggthemes)\nlibrary(terra)\nlibrary(MetBrewer)\n\nlibrary(ncdf4)",
    "crumbs": [
      "Data sources",
      "Weather data"
    ]
  },
  {
    "objectID": "code_weather_white-mold.html#white-mold-data",
    "href": "code_weather_white-mold.html#white-mold-data",
    "title": "Weather data",
    "section": "White mold data",
    "text": "White mold data\n\n\n\n\n\n\nAbout the Data\n\n\n\nIf you have not downloaded the data necessary to run these analysis, please refer to getting started section before runnig the code below\n\n\n\nwm_data = read.csv(\"data_white-mold/WhiteMoldSurveyWrangledData.csv\")\n\n\nRemoving missing coordinates\n\nwm_data2 = wm_data %&gt;% \n  filter(!is.na(latitude)) \n\n\n\nNames of the counties in the dataset\n\ncounties = unique(wm_data2$county)\ncounties\n\n[1] \"Genesee\"    \"Niagara\"    \"Orleans\"    \"Livingston\" \"Wyoming\"   \n[6] \"Ontario\"    \"Yates\"      \"Chautauqua\" \"Monroe\"    \n\ncentral_ny = c(\"wyoming\", \"livingston\",\"ontario\",\"yates\")\n\n\n\nPloting the location of each field\n\n#covert the names to lowercase\nmap_snap_fun = function(yearr){\ncounties_lc = tolower(counties)\n\nny_map_data = map_data('county', region = 'new york')\n\nsnap_map = ny_map_data %&gt;% \n  filter(subregion %in% counties_lc) %&gt;%\n  mutate(region2 = case_when(subregion %in% central_ny ~ \"Central lakes\",\n                            !subregion %in% central_ny ~ \"Great lakes\")) %&gt;% \n  ggplot()+\n  geom_polygon(data = ny_map_data,\n               aes(x=long, y = lat, group = group),\n               fill= \"white\",\n               color = \"black\",\n               size =0.3)+\n  geom_polygon(aes(x=long, y = lat, group = group,\n                   # fill=region2\n                   ),\n               fill= \"#fafced\",\n               color = \"black\",\n               size =0.3)+\n  geom_point(data = wm_data2 %&gt;% \n               group_by(subject) %&gt;% \n               filter(dap == max(dap),\n                       !is.na(wm)) %&gt;%\n               ungroup() %&gt;% \n               filter(year == yearr) %&gt;% \n               arrange(wm),\n                size = 1,\n             aes(longitude,latitude, color = wm&gt;0))+\n  coord_map(xlim = c(-80,-76.7),\n            ylim = c(41.8, 43.5))+\n  # scale_fill_manual(values = c(\"gray85\", \"gray70\"))+\n  # scale_color_colorblind(labels = c(\"Absent\", \"Present\"))+\n  scale_color_manual(labels = c(\"Absent\", \"Present\"),\n                     values = c(\"#009E73\", \"#E69F00\"))+\n  guides(color = guide_legend(override.aes = list(size=3)),\n         fill = \"none\")+\n  theme_minimal()+\n  labs(x = \"Longitude\",\n       y = \"Latitude\",\n       fill = \"Climate division\",\n       color = \"White mold\",\n       title = paste(yearr))+\n  # facet_wrap(~year, ncol = 1)+\n  theme(legend.position = \"right\",\n        legend.text = element_text(size=7),\n        axis.title = element_text(size=7),\n        axis.text = element_text(size=7),\n        plot.title = element_text(size=10))\nsnap_map\n}\n\n\nny_map_data = map_data('county', region = 'new york')\ncounties_lc = tolower(counties)\nny_map = ny_map_data %&gt;% \n  filter(subregion %in% counties_lc) %&gt;% \n  mutate(region2 = case_when(subregion %in% central_ny ~ \"Central lakes\",\n                            !subregion %in% central_ny ~ \"Great lakes\")) %&gt;% \n  ggplot()+\n  geom_polygon(data = ny_map_data,\n               aes(x=long, y = lat, group = group),\n               fill= \"white\",\n               color = \"black\",\n               size =0.3\n               )+\n  geom_polygon(aes(x=long, y = lat, group = group,\n                   # fill=region2\n                   ),\n               fill= \"#fafced\",\n               color = \"black\",\n               size =0.3)+\n  annotate(\"rect\", xmin = -80, xmax = -76.5,\n                   ymin = 41.8, ymax = 43.5,\n           color = \"black\",\n           size = 0.3,\n           alpha = 0)+\n  # scale_size_manual(values = c(0.1,0.3))+\n  # coord_map(xlim = c(-80,-73),\n            # ylim = c(40, 45))+\n  coord_map()+\n   # scale_fill_manual(values = c(\"gray85\", \"gray70\"))+\n  theme_void()+\n  labs(x = \"Longitude\",\n       y = \"Latitude\",\n       fill = \"Climate division\")+\n  theme(legend.position = c(0.1,0.8))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\nny_map\n\n\n\n\n\n\n\n\n\n  (map_snap_fun(yearr = \"2006\") +\n   map_snap_fun(yearr = \"2007\")+\n   map_snap_fun(yearr = \"2008\")+plot_layout(ncol = 2, guides = 'collect')&\n     theme(legend.position = \"bottom\",\n           legend.justification = c(\"left\",\"center\"))) +\n  (ny_map+plot_layout(guides = 'keep')) +\n\n  # plot_layout(ncol = 2)+\n  plot_annotation(tag_levels = \"A\")&\n  theme(legend.text = element_text(size=6),\n        legend.title = element_text(size=8),\n        legend.key.size = unit(0.4, 'cm'),\n        plot.title = element_text(face = \"bold\"),\n        plot.tag = element_text(face = \"bold\"))\n\n\n\n\n\n\n\nggsave(\"figs/maps/maps_fields.png\", dpi= 900, height = 6, width = 7, bg = \"white\")\nggsave(\"figs/maps/maps_fields.pdf\", dpi= 900, height = 6, width = 7, bg = \"white\")",
    "crumbs": [
      "Data sources",
      "Weather data"
    ]
  },
  {
    "objectID": "code_weather_white-mold.html#era5",
    "href": "code_weather_white-mold.html#era5",
    "title": "Weather data",
    "section": "ERA5",
    "text": "ERA5\n\nERA5-Land hourly data from 1950 to present Description\n\n0.1° x 0.1°; Native resolution is 9 km\nGlobal\nHourly\nJanuary 1950 to present\n\nERA5 hourly data on pressure levels from 1979 to present Description\n\nReanalysis: 0.25° x 0.25°\nGlobal\nHourly\n1979 to present\n\nAgrometeorological indicators from 1979 to present derived from reanalysis Description\n\n0.1° x 0.1°\nGlobal\nDaily\n1979 to present\n\nThere is a package for downloading the data using R: link",
    "crumbs": [
      "Data sources",
      "Weather data"
    ]
  },
  {
    "objectID": "code_weather_white-mold.html#importing-data",
    "href": "code_weather_white-mold.html#importing-data",
    "title": "Weather data",
    "section": "Importing data",
    "text": "Importing data\nHere I load the data for each data variable and gather into a single raster object.\n\n2m temperature\n\n\n\n\n\n\n\nt2m_all = raster::stack(\"data_era5/era5_NY_2006-2008.nc\", varname = \"t2m\")\n\n\n\n2m dewpoint temperature\n\n\n\n\n\n\n\nd2m_all = raster::stack(\"data_era5/era5_NY_2006-2008.nc\", varname = \"d2m\")\n\n\n\nSurface pressure\n\n\n\n\n\n\n\nsp_all = raster::stack(\"data_era5/era5_NY_2006-2008.nc\", varname = \"sp\")\n\n\n\nsoil moisture\n\n\n\n\n\n\n\nswvl1_all = raster::stack(\"data_era5/era5_NY_2006-2008.nc\", varname = \"swvl1\")\n\n\n\nsoil temperature\n\nstl1_all = raster::stack(\"data_era5/era5_NY_2006-2008_2.nc\", varname = \"stl1\")",
    "crumbs": [
      "Data sources",
      "Weather data"
    ]
  },
  {
    "objectID": "code_weather_white-mold.html#kringing-test",
    "href": "code_weather_white-mold.html#kringing-test",
    "title": "Weather data",
    "section": "Kringing Test",
    "text": "Kringing Test\n\nWeather data\nHere I load the temperature data, select one layer (time and day) and plot it together with the New York map\n\nr &lt;- raster::stack(\"data_era5/era5_NY_2006-2008.nc\", varname = \"t2m\")\n# a = r$X2008.05.01.00.00.00\na = subset(r,1)\nraster::plot(a)\npoints(wm_data2$longitude, wm_data2$latitude)\nmaps::map('county', region = 'new york', col = \"#5E610B\", add = TRUE)\n\n\n\n\n\n\n\nlength(names(r))\n\n[1] 15408\n\n\n\n\nNY shapefile\nWe will need the New York shapefile to download the digital elevation map, which should be used for kriging. I also filter only the counties that have been surveyed for white mold.\n\nDir.StateShp &lt;- file.path(\"data_era5/test\")# file.path(\"shape_files/nc_files\")\n\n # ny_shape1 = readOGR(\"shape_files/cugir-007865/cugir-007865/cty036.shp\")\n ny_shape1 = sf::read_sf(\"shape_files/cugir-007865/cugir-007865/cty036.shp\")\n\n ny_shape = ny_shape1[ny_shape1$NAME %in% c(unique(wm_data2$county)\n                                            # \"Yates\", \"Steuben\",\"Allegany\", \"Cattaraugus\",\"Erie\"\n                                            ),] \n\n\nggplot()+\n  geom_sf(data =ny_shape )+\n  geom_point(aes(wm_data2$longitude, wm_data2$latitude))+\n  theme_nothing()\n\n\n\n\n\n\n\n\n\n\nMasking\nIn this section, we create a mask to filter the dataset, retaining only data for the selected counties. The process involves the following steps:\n\nConvert the selected county shapes into a simple feature object for visualization.\n\n\nny_shape_st = st_as_sfc(ny_shape[1])\nplot(ny_shape_st)\n\n\n\n\n\n\n\n\n\nMerge the geometries to create a single boundary.\n\n\nny_shape_st_nobound = st_union(ny_shape_st)\nplot(ny_shape_st_nobound)\n\n\n\n\n\n\n\n\n\nApply a buffer around the selected area to ensure a smooth mask transition.\n\n\nny_shape_buffer = st_sf(st_buffer(ny_shape_st_nobound,dist = 10000, max_cells = 500))\nplot(ny_shape_buffer)\n\n\n\n\n\n\n\n\n\nUse the buffer to mask the raster, setting all selected cells to 1.\n\n\na11 =  terra::mask(a, ny_shape_buffer, updatevalue=NA, touches =T)\na11_values = values(a11)\na11_values[!is.na(values(a11))] = 1\nvalues(a11) = a11_values\n\nplot(a11)\n\n\n\n\n\n\n\n\n\nConvert the masked raster into a polygon. This will serve as the mask for filtering all datasets consistently.\n\n\na21 = raster::rasterToPolygons(a11, dissolve=T)\n\n\nApply the mask to filter the raster dataset, ensuring only the relevant regions remain.\n\n\na2 = crop(mask(a,a21, touches =T),a21)\n\n\nVisualize the results:\n\nThe first plot shows the initial masked raster.\nThe second plot shows the mask polygon.\nThe third plot displays the filtered raster dataset.\n\n\n\npar(mfrow=c(1,3))\nplot(a11, main = \"NULL raster file\")\nplot(a21, main = \"Mask\")\nplot(a2, main = \"Masked raster data\")\n\n\n\n\n\n\n\n\n\nori_data_df = as.data.frame(a2, xy = T) \ncolnames(ori_data_df)[3] &lt;- \"values\"\nori_g= ori_data_df %&gt;% \n  filter(!is.na(values)) %&gt;% \n  ggplot()+\n  geom_tile(aes(x, y, fill = values))+\n  geom_sf(data = ny_shape1,\n              # aes(x=long, y = lat, group = group),\n               fill= NA,\n               color = \"black\")+\n  scale_fill_viridis(option=\"B\")+\n  coord_sf(xlim = c(-80,-76.8), ylim = c(41.8,43.5))+\n  geom_point(data = wm_data2,\n             shape = 21,\n             color = \"white\",\n             fill = \"black\", \n             aes(longitude,latitude))+\n  labs(x = \"Longitude\",\n       y = \"Latitude\",\n       title= \"Era5 original\")+\n  theme_minimal()\nori_g\n\n\n\n\n\n\n\n\n\n\nDownloading Digital elevation model (DEM)\nFunction to plot the DEM\n\nsource(\"functions/Plot_Covs.R\")\n\nDownloading DEM\n\n# Covs_ls &lt;- download_DEM(\n#   Train_ras = a2,\n#   Target_res = .02,\n#   Shape = ny_shape,\n#   Dir = Dir.StateShp,\n#   Keep_Temporary = TRUE\n# )\n\nCovs_ls &lt;- CovariateSetup(\n  Training =  as(a2, \"SpatRaster\"),\n  Target = .02,\n  Extent = a21,\n  Dir = \"data_era5\",\n  Keep_Global = TRUE\n)\n\n###### Downloading global covariate data\n\n\n[1] \"GMTED2010 covariate data already downloaded.\"\n\n\n###### Resampling Data\n\n# nc &lt;- nc_open(\"data_era5/GMTED2010.nc\")\n# names(nc[['var']])\n# Covs_ls = raster::stack(x = \"data_era5/GMTED2010.nc\", varname = \"GMTED2010\")\n# names(Covs_ls)\n# Plot_Covs(Covs_ls)\nPlot.Covariates(Covs_ls)\n\nWarning: Raster pixels are placed at uneven horizontal intervals and will be shifted\nℹ Consider using `geom_tile()` instead.\nRaster pixels are placed at uneven horizontal intervals and will be shifted\nℹ Consider using `geom_tile()` instead.\n\n\n\n\n\n\n\n\n# class(a2)\n\n\nVisualization\n…Using ggplot2\n\nas.data.frame(Covs_ls[[2]], xy = T) %&gt;%\n  mutate(DEM = GMTED2010) |&gt; \n  filter(!is.na(DEM)) %&gt;% \n  ggplot() +\n  geom_tile(aes(x,y,fill = DEM))+\n  geom_sf(data = ny_shape1,\n              # aes(x=long, y = lat, group = group),\n               fill= NA,\n               color = \"black\")+\n   geom_point(data = wm_data2, size = 0.1,\n             aes(longitude,latitude))+\n  scale_fill_gradientn(colors=met.brewer(\"Homer2\", direction = -1))+\n  coord_sf(xlim = c(-80,-76.8), ylim = c(41.8,43.5))+\n  labs(x = \"Longitude\",\n       y = \"Latitude\",\n       title= \"Era5 original\")+\n  theme_void()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nggsave(\"figs/maps/elevation.png\", dpi = 600, height = 5, width = 7, bg = \"white\")\n\n\n\n\nKringing\nThis function performs kriging of the weather variables as function of the DEM data\n\na2_SpatRaster = as(a2, \"SpatRaster\")\nKrigStart &lt;- Sys.time() \n\nState_Krig &lt;- Kriging(\n  Data =a2_SpatRaster, # what to krig\n  Covariates_training  = Covs_ls[[1]], # covariates at training resolution\n  Covariates_target  = Covs_ls[[2]], # covariates at target resolution\n  Equation = \"GMTED2010\", # the covariate(s) we want to use\n  Keep_Temporary = FALSE, # delete temporary krigs of layers\n  nmax = 40, # degree of localisation\n  Cores = 1, # we want to krig using three cores to speed this process up\n  FileName = \"State_Shape\", # save the finished file as this _t2m_2008.nc\n  Dir = \"data_era5/kriging\" # where to save the output\n)\n\n###### Checking your Kriging Specification\n\n\n[1] \"A file with the name State_Shape_Kriged.nc already exists in data_era5/kriging.\"\n[1] \"Loading this file for you from the disk.\"\n[1] \"A file with the name State_Shape_StDev.nc already exists in data_era5/kriging.\"\n[1] \"Loading this file for you from the disk.\"\n\nKrigStop &lt;- Sys.time() \nKrigStop-KrigStart\n\nTime difference of 0.116817 secs\n\nPlot.Kriged(State_Krig)\n\n\n\n\n\n\n\n# plot(Covs_ls[[1]])\n\n\n\nVisualization\n\nKrigs = State_Krig$Prediction\n# Krigs = State_Krig$Kriging_SE\nKrig_df &lt;- as.data.frame(Krigs[[1]], xy = TRUE)\ncolnames(Krig_df)[3] &lt;- \"values\"\n\n\n\nkrig_g = Krig_df %&gt;% \n  filter(!is.na(values)) %&gt;% \n  ggplot()+\n  geom_tile(aes(x, y, fill = values))+\n  geom_sf(data = ny_shape1,\n              # aes(x=long, y = lat, group = group),\n               fill= NA,\n               color = \"black\")+\n  scale_fill_viridis(option=\"B\")+\n  # scale_color_viridis(option=\"B\")+\n  geom_point(data = wm_data2,\n             shape = 21,\n             color = \"white\",\n             fill = \"black\",\n             aes(longitude,latitude))+\n  coord_sf(xlim = c(-80,-76.8), ylim = c(41.8,43.5))+\n  labs(x = \"Longitude\",\n       y = \"Latitude\",\n       title= \"Era5 kriged\")\n\nori_g + krig_g +\n  plot_layout(ncol = 1) &\ntheme_minimal()",
    "crumbs": [
      "Data sources",
      "Weather data"
    ]
  },
  {
    "objectID": "code_weather_white-mold.html#daily-summaries",
    "href": "code_weather_white-mold.html#daily-summaries",
    "title": "Weather data",
    "section": "Daily summaries",
    "text": "Daily summaries\n\npar(mfrow=c(1,3))\nplot(mean(t2m_all[[1:24]]))\nmaps::map('county', region = 'new york', col = \"#5E610B\", add = TRUE)\nplot(min(t2m_all[[1:24]]))\nmaps::map('county', region = 'new york', col = \"#5E610B\", add = TRUE)\nplot(max(t2m_all[[1:24]]))\nmaps::map('county', region = 'new york', col = \"#5E610B\", add = TRUE)\n\n\n\n\n\n\n\n\n\n# days_in_month(month)\nyear = 2006:2008\nmonth = 4:10\nday = 01:31\nhour = as.numeric(seq(0,23,1))\ndata_era5 = expand_grid(year, month, day) %&gt;% \n  dplyr::filter(month != 4 | day != 31,\n         month != 6 | day != 31,\n         month != 9 | day != 31)%&gt;% \n  unite(date, year, month, day,sep=\"-\",remove = F ) %&gt;% \n  mutate(date = as.Date(date))\n\nFunction for calculating the summaries\n\nmean_raster = function(i, stack_obj){\n  mean(stack_obj[[i:(i+23)]])\n}\nmin_raster = function(i, stack_obj){\n  min(stack_obj[[i:(i+23)]])\n}\nmax_raster = function(i, stack_obj){\n  max(stack_obj[[i:(i+23)]])\n}\n\n\nLapply\n\ndays_i = seq(1,length(data_era5$day)*24,by = 24)\ntime_start = Sys.time()\n\n# dew point\naa1 = lapply(days_i, mean_raster, stack_obj= d2m_all)\nd2m_mean_daily_stack = crop(mask(stack(aa1), a21),a21)\nwriteCDF(rast(d2m_mean_daily_stack),\n         \"data_era5/daily_summaries/d2m_mean_daily.nc\",\n         overwrite=TRUE)\n# temperature\n### mean\naa2 = lapply(days_i, mean_raster, stack_obj= t2m_all)\nt2m_mean_daily_stack = crop(mask(stack(aa2), a21),a21)\nwriteCDF(rast(t2m_mean_daily_stack),\n         \"data_era5/daily_summaries/t2m_mean_daily.nc\",\n         overwrite=TRUE)\n### minimum\naa3 = lapply(days_i, min_raster, stack_obj= t2m_all)\nt2m_min_daily_stack = crop(mask(stack(aa3), a21),a21)\nwriteCDF(rast(t2m_min_daily_stack),\n         \"data_era5/daily_summaries/t2m_min_daily.nc\",\n         overwrite=TRUE)\n### maximum\naa4 = lapply(days_i, max_raster, stack_obj= t2m_all)\nt2m_max_daily_stack = crop(mask(stack(aa4), a21),a21)\nwriteCDF(rast(t2m_max_daily_stack),\n         \"data_era5/daily_summaries/t2m_max_daily.nc\",\n         overwrite=TRUE)\n\n\n# Surface pressure\naa5 = lapply(days_i, mean_raster, stack_obj= sp_all)\nsp_mean_daily_stack = crop(mask(stack(aa5), a21),a21)\nwriteCDF(rast(sp_mean_daily_stack),\n         \"data_era5/daily_summaries/sp_mean_daily.nc\",\n         overwrite=TRUE)\n\n# Soil moisture\naa6 = lapply(days_i, mean_raster, stack_obj= swvl1_all)\nswvl1_mean_daily_stack = crop(mask(stack(aa6), a21),a21)\nwriteCDF(rast(swvl1_mean_daily_stack),\n         \"data_era5/daily_summaries/swvl1_mean_daily.nc\",\n         overwrite=TRUE)\n\n# soil temperature\naa7 = lapply(days_i, mean_raster, stack_obj= stl1_all)\nstl1_mean_daily_stack = crop(mask(stack(aa7), a21),a21)\nwriteCDF(rast(stl1_mean_daily_stack),\n         \"data_era5/daily_summaries/stl1_mean_daily.nc\",\n         overwrite=TRUE)\n\ntime_end = Sys.time()\ntime_end -time_start\n\n\nplot(d2m_mean_daily_stack$X1)\n# plot(stl1_mean_daily_stack$X1)\nmaps::map('county', region = 'new york',  add = TRUE)\n\n\n\n\n\n\n\n\n\n\nDownload DEM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKriging the daily summaries\n\nDir.krigd_var &lt;- file.path(\"data_era5/kriged\")\n\n\nDew point\n\nd2m_mean_daily_Krig = Kriging(\n  Data = as(d2m_mean_daily_stack, \"SpatRaster\"), # what to krig\n  Covariates_training  = Covs_ls[[1]], # covariates at training resolution\n  Covariates_target  = Covs_ls[[2]], # covariates at target resolution\n  Equation = \"GMTED2010\", # the covariate(s) we want to use\n  Keep_Temporary = FALSE, # delete temporary krigs of layers\n  nmax = 40, # degree of localisation\n  Cores = 5, # we want to krig using three cores to speed this process up\n  FileName = \"d2m_mean_daily_krig.nc\", # save the finished file as this _t2m_2008.nc\n  Dir = Dir.krigd_var#\"data_era5/kriging\" # where to save the output\n)\n\n\n\nMean Temeperature\n\nt2m_mean_daily_Krig = Kriging(\n  Data = as(t2m_mean_daily_stack, \"SpatRaster\"), # what to krig\n  Covariates_training  = Covs_ls[[1]], # covariates at training resolution\n  Covariates_target  = Covs_ls[[2]], # covariates at target resolution\n  Equation = \"GMTED2010\", # the covariate(s) we want to use\n  Keep_Temporary = FALSE, # delete temporary krigs of layers\n  nmax = 40, # degree of localisation\n  Cores = 5, # we want to krig using three cores to speed this process up\n  FileName = \"t2m_mean_daily_Krig.nc\", # save the finished file as this _t2m_2008.nc\n  Dir = Dir.krigd_var#\"data_era5/kriging\" # where to save the output\n)\n\n\n\nMinimum temperature\n\nt2m_min_daily_Krig = Kriging(\n  Data = as(t2m_min_daily_stack, \"SpatRaster\"), # what to krig\n  Covariates_training  = Covs_ls[[1]], # covariates at training resolution\n  Covariates_target  = Covs_ls[[2]], # covariates at target resolution\n  Equation = \"GMTED2010\", # the covariate(s) we want to use\n  Keep_Temporary = FALSE, # delete temporary krigs of layers\n  nmax = 40, # degree of localisation\n  Cores = 5, # we want to krig using three cores to speed this process up\n  FileName = \"t2m_min_daily_Krig.nc\", # save the finished file as this _t2m_2008.nc\n  Dir = Dir.krigd_var#\"data_era5/kriging\" # where to save the output\n)\n\n\n\nMaximum temperature\n\nt2m_max_daily_Krig = Kriging(\n  Data = as(t2m_max_daily_stack, \"SpatRaster\"), # what to krig\n  Covariates_training  = Covs_ls[[1]], # covariates at training resolution\n  Covariates_target  = Covs_ls[[2]], # covariates at target resolution\n  Equation = \"GMTED2010\", # the covariate(s) we want to use\n  Keep_Temporary = FALSE, # delete temporary krigs of layers\n  nmax = 40, # degree of localisation\n  Cores = 5, # we want to krig using three cores to speed this process up\n  FileName = \"t2m_max_daily_Krig.nc\", # save the finished file as this _t2m_2008.nc\n  Dir = Dir.krigd_var#\"data_era5/kriging\" # where to save the output\n)\n\n\n\nSurface pressure\n\nsp_mean_daily_Krig = Kriging(\n  Data = as(sp_mean_daily_stack, \"SpatRaster\"), # what to krig\n  Covariates_training  = Covs_ls[[1]], # covariates at training resolution\n  Covariates_target  = Covs_ls[[2]], # covariates at target resolution\n  Equation = \"GMTED2010\", # the covariate(s) we want to use\n  Keep_Temporary = FALSE, # delete temporary krigs of layers\n  nmax = 40, # degree of localisation\n  Cores = 5, # we want to krig using three cores to speed this process up\n  FileName = \"sp_mean_daily_Krig.nc\", # save the finished file as this _t2m_2008.nc\n  Dir = Dir.krigd_var#\"data_era5/kriging\" # where to save the output\n)\n\n\n\nSoil temperature\n\nstl1_mean_daily_Krig = Kriging(\n  Data = as(stl1_mean_daily_stack, \"SpatRaster\"), # what to krig\n  Covariates_training  = Covs_ls[[1]], # covariates at training resolution\n  Covariates_target  = Covs_ls[[2]], # covariates at target resolution\n  Equation = \"GMTED2010\", # the covariate(s) we want to use\n  Keep_Temporary = FALSE, # delete temporary krigs of layers\n  nmax = 40, # degree of localisation\n  Cores = 5, # we want to krig using three cores to speed this process up\n  FileName = \"stl1_mean_daily_Krig.nc\", # save the finished file as this _t2m_2008.nc\n  Dir = Dir.krigd_var#\"data_era5/kriging\" # where to save the output\n)\n\n\n\nSoil moisture\n\nswvl1_mean_daily_Krig = Kriging(\n  Data = as(swvl1_mean_daily_stack, \"SpatRaster\"), # what to krig\n  Covariates_training  = Covs_ls[[1]], # covariates at training resolution\n  Covariates_target  = Covs_ls[[2]], # covariates at target resolution\n  Equation = \"GMTED2010\", # the covariate(s) we want to use\n  Keep_Temporary = FALSE, # delete temporary krigs of layers\n  nmax = 40, # degree of localisation\n  Cores = 5, # we want to krig using three cores to speed this process up\n  FileName = \"swvl1_mean_daily_Krig.nc\", # save the finished file as this _t2m_2008.nc\n  Dir = Dir.krigd_var#\"data_era5/kriging\" # where to save the output\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd2m_mean_daily_Krig = stack(\"data_era5/kriged/d2m_mean_daily_Krig.nc\")\nt2m_mean_daily_Krig = stack(\"data_era5/kriged/t2m_mean_daily_Krig.nc\")\nt2m_min_daily_Krig = stack(\"data_era5/kriged/t2m_min_daily_Krig.nc\")\nt2m_max_daily_Krig = stack(\"data_era5/kriged/t2m_max_daily_Krig.nc\")\nsp_mean_daily_Krig = stack(\"data_era5/kriged/sp_mean_daily_Krig.nc\")\nswvl1_mean_daily_Krig = stack(\"data_era5/kriged/swvl1_mean_daily_Krig.nc\")\nstl1_mean_daily_Krig = stack(\"data_era5/kriged/stl1_mean_daily_Krig.nc\")\n\n\n\n\nPloting the kingued maps (first layer)\n\npar(mfrow=c(2,4))\nplot(t2m_mean_daily_Krig$X1, main = \"Mean Temp\")\nplot(t2m_min_daily_Krig$X1, main = \"Min Temp\")\nplot(t2m_max_daily_Krig$X1, main = \"Max Temp\")\nplot(d2m_mean_daily_Krig$X1, main = \"Dew Point\")\nplot(sp_mean_daily_Krig$X1, main = \"Surface Pressure\")\nplot(swvl1_mean_daily_Krig$X1, main = \"Soil Moisture\")\nplot(stl1_mean_daily_Krig$X1, main = \"Soil Temp\")\n\n\n\n\n\n\n\n\nUsing ggplot\n\n# t2m_max_daily_stack\nKrigs = t2m_max_daily_stack$X1\nKrig_df &lt;- as.data.frame(Krigs[[1]], xy = TRUE)\ncolnames(Krig_df)[3] &lt;- \"values\"\n\nmax_t_ori = Krig_df %&gt;% \n  filter(!is.na(values)) %&gt;% \n  ggplot()+\n  geom_sf(data = ny_shape1,\n              # aes(x=long, y = lat, group = group),\n               fill= \"white\",\n               color = \"black\")+\n  geom_tile(aes(x, y, fill = values-273.15))+\n  geom_sf(data = ny_shape1,\n              # aes(x=long, y = lat, group = group),\n               fill= NA,\n               color = \"black\")+\n  scale_fill_viridis(option=\"B\", limits = c(5,18),breaks =seq(5, 18, by = 3))+\n  # scale_color_viridis(option=\"B\")+\n  geom_point(data = wm_data2,\n             shape = 21,\n             color = \"white\",\n             fill = \"black\",\n             aes(longitude,latitude))+\n  coord_sf(xlim = c(-80,-76.8), ylim = c(41.8,43.5))+\n  labs(x = \"Longitude\",\n       y = \"Latitude\",\n       fill = \"Max Temperature (°C)\",\n       title= \"Era5 Native Resolution (0.1° x 0.1°)\")\n# max_t_ori\n\n\nKrigs = t2m_max_daily_Krig$X1\nKrig_df &lt;- as.data.frame(Krigs[[1]], xy = TRUE)\ncolnames(Krig_df)[3] &lt;- \"values\"\n\n\n\nmax_t_krig = Krig_df %&gt;% \n  filter(!is.na(values)) %&gt;% \n  ggplot()+\n  geom_sf(data = ny_shape1,\n              # aes(x=long, y = lat, group = group),\n               fill= \"white\",\n               color = \"black\")+\n  geom_tile(aes(x, y, fill = values-273.15))+\n  geom_sf(data = ny_shape1,\n              # aes(x=long, y = lat, group = group),\n               fill= NA,\n               color = \"black\")+\n  scale_fill_viridis(option=\"B\", limits = c(5,18), breaks =seq(5, 18, by = 3))+\n  # scale_color_viridis(option=\"B\")+\n  geom_point(data = wm_data2,\n             shape = 21,\n             color = \"white\",\n             fill = \"black\",\n             aes(longitude,latitude))+\n  coord_sf(xlim = c(-80,-76.8), ylim = c(41.8,43.5))+\n  labs(x = \"Longitude\",\n       y = \"Latitude\",\n       fill = \"Max Temperature (°C)\",\n       title= \"Era5 Kriged (0.02° x 0.02°)\")\n\nmax_t_ori + max_t_krig +\n  plot_layout(ncol = 2, guides = \"collect\") &\n  theme_map()&\n  theme(legend.position = \"bottom\",\n        legend.text = element_text(size = 9))\n\n\n\n\n\n\n\n# ggsave(\"figs/max_t.png\",dpi=900, height = 8, width=14, bg = \"white\")\n\n\n\nRelative humidity\nHow to calculate relative humidity here link and here link\n\\[RH = \\frac{es(d2m)}{es(t2m)}*100\\]\n\\(es()\\) is Saturation vapor pressure and can be calculated as\n\\[es(T) = \\alpha_1*exp( \\alpha_3*(\\frac{T-t0}{T-\\alpha_4}))\\],\nwhere \\(T\\) is the temperature, \\(\\alpha_1 = 611.21\\), \\(\\alpha_2 = 17.502\\), \\(\\alpha_3 = 32.19\\).\nUsing these resources, I build the function to calculates RH from Temperature and dew point.\n\nes =function(Temp){\nt0 = 273.16\n  \nalpha1 = 611.21\nalpha3 = 17.502\nalpha4 = 32.19\n\nalpha1*exp( alpha3*((Temp-t0)/(Temp-alpha4)))\n\n}\n\n#test\nTdp = 14+273.15\nT2m = 31+273.15\nes(Tdp)/es(T2m)*100\n\n[1] 35.55801\n\n\nCreating the raster object of relative humidity\n\nlayer_i = 1:nlayers(d2m_mean_daily_Krig)\n\ncalculate_RH = function(i, d2m_stack, t2m_stack){\n  es(d2m_stack[[i]])/es(t2m_stack[[i]])*100\n}\n\nlist_rh = lapply(layer_i, calculate_RH, d2m_stack = d2m_mean_daily_Krig,t2m_stack= t2m_mean_daily_Krig)\n\nrh_mean_daily_Krig = brick(list_rh)\n\n\n\nwriteCDF(rast(rh_mean_daily_Krig),\n         \"data_era5/kriged/rh_mean_daily_Krig.nc\",\n         overwrite=TRUE)\n\n\nrh_mean_daily_Krig = stack(\"data_era5/kriged/rh_mean_daily_Krig.nc\")\nplot(rh_mean_daily_Krig$X1, main = \"Relative Humidity\")",
    "crumbs": [
      "Data sources",
      "Weather data"
    ]
  },
  {
    "objectID": "code_weather_white-mold.html#extracting-data",
    "href": "code_weather_white-mold.html#extracting-data",
    "title": "Weather data",
    "section": "Extracting data",
    "text": "Extracting data\n\nwm_data2_uni = wm_data2 %&gt;% \n  group_by(subject) %&gt;% \n  slice(1L)\n\n\ncoords&lt;-data.frame(lon=wm_data2_uni$longitude, lat=wm_data2_uni$latitude)\ncoordinates(coords)&lt;-c(\"lon\",\"lat\")\n\n\nDew point\n\n\next_d2m = extract(d2m_mean_daily_Krig, coords)\ncolnames(ext_d2m) = as.character(data_era5$date)\next_d2m_coord = cbind(longitude=wm_data2_uni$longitude, \n                      latitude=wm_data2_uni$latitude,\n                      ext_d2m )\n\nd2m_mean_wm = as.data.frame(ext_d2m_coord) %&gt;%\n  mutate(subject =wm_data2_uni$subject) %&gt;% \n  pivot_longer(3:644,\n               values_to = \"d2m\",\n               names_to = \"date\")%&gt;% \n  mutate(date = as.Date(date))\nd2m_mean_wm\n\n# A tibble: 241,392 × 5\n   longitude latitude subject date         d2m\n       &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt; &lt;date&gt;     &lt;dbl&gt;\n 1     -77.9     42.9       1 2006-04-01  282.\n 2     -77.9     42.9       1 2006-04-02  271.\n 3     -77.9     42.9       1 2006-04-03  276.\n 4     -77.9     42.9       1 2006-04-04  270.\n 5     -77.9     42.9       1 2006-04-05  266.\n 6     -77.9     42.9       1 2006-04-06  272.\n 7     -77.9     42.9       1 2006-04-07  275.\n 8     -77.9     42.9       1 2006-04-08  268.\n 9     -77.9     42.9       1 2006-04-09  268.\n10     -77.9     42.9       1 2006-04-10  270.\n# ℹ 241,382 more rows\n\n\n\nMean Temperature\n\n\next_t2m_mean = extract(t2m_mean_daily_Krig, coords)\ncolnames(ext_t2m_mean) = as.character(data_era5$date)\next_t2m_mean_coord = cbind(longitude=wm_data2_uni$longitude,\n                           latitude=wm_data2_uni$latitude,\n                           ext_t2m_mean)\n\nt2m_mean_wm = as.data.frame(ext_t2m_mean_coord) %&gt;%\n  mutate(subject =wm_data2_uni$subject)%&gt;%\n  pivot_longer(3:644,\n               values_to = \"t2m_mean\",\n               names_to = \"date\")%&gt;% \n  mutate(date = as.Date(date))\n\n\nMax Temperature\n\n\next_t2m_max = extract(t2m_max_daily_Krig, coords)\ncolnames(ext_t2m_max) = as.character(data_era5$date)\next_t2m_max_coord = cbind(longitude=wm_data2_uni$longitude,\n                           latitude=wm_data2_uni$latitude,\n                           ext_t2m_max)\n\nt2m_max_wm = as.data.frame(ext_t2m_max_coord) %&gt;%\n  mutate(subject =wm_data2_uni$subject)%&gt;%\n  pivot_longer(3:644,\n               values_to = \"t2m_max\",\n               names_to = \"date\")%&gt;% \n  mutate(date = as.Date(date))\n\n\nMin Temperature\n\n\next_t2m_min = extract(t2m_min_daily_Krig, coords)\ncolnames(ext_t2m_min) = as.character(data_era5$date)\next_t2m_min_coord = cbind(longitude=wm_data2_uni$longitude,\n                           latitude=wm_data2_uni$latitude,\n                           ext_t2m_min)\n\nt2m_min_wm = as.data.frame(ext_t2m_min_coord) %&gt;%\n  mutate(subject =wm_data2_uni$subject)%&gt;%\n  pivot_longer(3:644,\n               values_to = \"t2m_min\",\n               names_to = \"date\")%&gt;% \n  mutate(date = as.Date(date))\n\n\nPressure\n\n\next_sp = extract(sp_mean_daily_Krig, coords)\ncolnames(ext_sp) = as.character(data_era5$date)\next_sp_coord = cbind(longitude=wm_data2_uni$longitude, \n                      latitude=wm_data2_uni$latitude,\n                      ext_sp )\n\nsp_mean_wm = as.data.frame(ext_sp_coord) %&gt;%\n  mutate(subject =wm_data2_uni$subject)%&gt;%\n  pivot_longer(3:644,\n               values_to = \"sp\",\n               names_to = \"date\")%&gt;% \n  mutate(date = as.Date(date))\n\n\nSoil moisture\n\n\next_sm = extract(swvl1_mean_daily_Krig, coords)\ncolnames(ext_sm) = as.character(data_era5$date)\next_sm_coord = cbind(longitude=wm_data2_uni$longitude, \n                      latitude=wm_data2_uni$latitude,\n                      ext_sm )\n\nsm_mean_wm = as.data.frame(ext_sm_coord)%&gt;%\n  mutate(subject =wm_data2_uni$subject) %&gt;%\n  pivot_longer(3:644,\n               values_to = \"sm\",\n               names_to = \"date\")%&gt;% \n  mutate(date = as.Date(date))\n\n\nRelative humidity\n\n\next_rh = extract(rh_mean_daily_Krig, coords)\ncolnames(ext_rh) = as.character(data_era5$date)\next_rh_coord = cbind(longitude=wm_data2_uni$longitude,\n                     latitude=wm_data2_uni$latitude,\n                     ext_rh )\n\nrh_mean_wm = as.data.frame(ext_rh_coord)%&gt;%\n  mutate(subject =wm_data2_uni$subject)%&gt;%\n  pivot_longer(3:644,\n               values_to = \"rh\",\n               names_to = \"date\") %&gt;% \n  mutate(date = as.Date(date))\n\n\nSoil temperature\n\n\next_st = extract(stl1_mean_daily_Krig, coords)\ncolnames(ext_st) = as.character(data_era5$date)\next_st_coord = cbind(longitude=wm_data2_uni$longitude,\n                     latitude=wm_data2_uni$latitude,\n                     ext_st )\n\nst_mean_wm = as.data.frame(ext_st_coord) %&gt;% \n  mutate(subject =wm_data2_uni$subject)%&gt;%\n  pivot_longer(3:644,\n               values_to = \"st\",\n               names_to = \"date\") %&gt;% \n  mutate(date = as.Date(date))\n\nBinding data sets\n\nweather_all_wm = d2m_mean_wm %&gt;%\n  bind_cols(t2m_mean_wm[,5],\n            t2m_max_wm[,5],\n            t2m_min_wm[,5],\n            sp_mean_wm[,5],\n            sm_mean_wm[,5],\n            rh_mean_wm[,5],\n            st_mean_wm[,5])\nweather_all_wm\n\n# A tibble: 241,392 × 12\n   longitude latitude subject date         d2m t2m_mean t2m_max t2m_min     sp\n       &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt; &lt;date&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1     -77.9     42.9       1 2006-04-01  282.     284.    289.    281. 97567.\n 2     -77.9     42.9       1 2006-04-02  271.     279.    285.    273. 98578.\n 3     -77.9     42.9       1 2006-04-03  276.     282.    290.    277. 97387.\n 4     -77.9     42.9       1 2006-04-04  270.     277.    284.    273. 97250.\n 5     -77.9     42.9       1 2006-04-05  266.     273.    277.    270. 97416.\n 6     -77.9     42.9       1 2006-04-06  272.     276.    283.    273. 97868.\n 7     -77.9     42.9       1 2006-04-07  275.     278.    281.    275. 97193.\n 8     -77.9     42.9       1 2006-04-08  268.     274.    277.    271. 97920.\n 9     -77.9     42.9       1 2006-04-09  268.     275.    282.    270. 98859.\n10     -77.9     42.9       1 2006-04-10  270.     278.    288.    272. 99093.\n# ℹ 241,382 more rows\n# ℹ 3 more variables: sm &lt;dbl&gt;, rh &lt;dbl&gt;, st &lt;dbl&gt;\n\n\n\nSaving data\n\ndata_full = weather_all_wm %&gt;%\n  dplyr::select(-latitude, -longitude) %&gt;% \n full_join(wm_data2, by = \"subject\") \n  \nwrite.csv(data_full, \"data_white-mold/data_model_plus_weather.csv\",row.names = F)\n\n\ndata_full = read.csv(\"data_white-mold/data_model_plus_weather.csv\") %&gt;% \n  mutate(date = as.Date(date),\n         sampling.date =  as.Date(sampling.date),\n         planting.date = as.Date(planting.date))\n\nfilter between planting and sampling dates\n\ndata_fill_filtered = data_full %&gt;% \n  filter(date &gt;= (planting.date-30) & date &lt;= sampling.date)\n\n\nwrite.csv(data_fill_filtered, \"data_white-mold/data_model_plus_weather_filtered.csv\",row.names = F)\n\n\ndata_fill_filtered = read.csv(\"data_white-mold/data_model_plus_weather_filtered.csv\")",
    "crumbs": [
      "Data sources",
      "Weather data"
    ]
  },
  {
    "objectID": "code_weather_white-mold.html#session-info",
    "href": "code_weather_white-mold.html#session-info",
    "title": "Weather data",
    "section": "Session Info",
    "text": "Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Portuguese_Brazil.utf8  LC_CTYPE=Portuguese_Brazil.utf8   \n[3] LC_MONETARY=Portuguese_Brazil.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Portuguese_Brazil.utf8    \n\ntime zone: America/Sao_Paulo\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] ncdf4_1.23             MetBrewer_0.2.0        terra_1.8-5           \n [4] ggthemes_5.1.0         viridis_0.6.5          viridisLite_0.4.2     \n [7] sf_1.0-19              KrigR_0.9.4            spatstat_3.3-0        \n[10] spatstat.linnet_3.2-3  spatstat.model_3.3-3   rpart_4.1.23          \n[13] spatstat.explore_3.3-3 nlme_3.1-164           spatstat.random_3.3-2 \n[16] spatstat.geom_3.3-4    spatstat.univar_3.1-1  spatstat.data_3.1-4   \n[19] raster_3.6-30          sp_2.1-4               patchwork_1.3.0       \n[22] cowplot_1.1.3          lubridate_1.9.3        forcats_1.0.0         \n[25] stringr_1.5.1          dplyr_1.1.4            purrr_1.0.2           \n[28] readr_2.1.5            tidyr_1.3.1            tibble_3.2.1          \n[31] ggplot2_3.5.1          tidyverse_2.0.0       \n\nloaded via a namespace (and not attached):\n [1] DBI_1.2.3             pbapply_1.7-2         deldir_2.0-4         \n [4] gridExtra_2.3         s2_1.1.7              rlang_1.1.4          \n [7] magrittr_2.0.3        e1071_1.7-16          compiler_4.4.1       \n[10] mgcv_1.9-1            systemfonts_1.1.0     maps_3.4.2.1         \n[13] vctrs_0.6.5           ecmwfr_2.0.3          wk_0.9.4             \n[16] crayon_1.5.3          pkgconfig_2.0.3       fastmap_1.2.0        \n[19] labeling_0.4.3        utf8_1.2.4            rmarkdown_2.28       \n[22] tzdb_0.4.0            ragg_1.3.3            automap_1.1-12       \n[25] xfun_0.48             cachem_1.1.0          jsonlite_1.8.9       \n[28] progress_1.2.3        goftest_1.2-3         reshape_0.8.9        \n[31] spatstat.utils_3.1-1  prettyunits_1.2.0     parallel_4.4.1       \n[34] R6_2.5.1              stringi_1.8.4         stars_0.6-7          \n[37] Rcpp_1.0.13           iterators_1.0.14      knitr_1.48           \n[40] tensor_1.5            zoo_1.8-12            snow_0.4-4           \n[43] FNN_1.1.4.1           Matrix_1.7-0          splines_4.4.1        \n[46] timechange_0.3.0      tidyselect_1.2.1      rstudioapi_0.17.0    \n[49] abind_1.4-8           yaml_2.3.10           codetools_0.2-20     \n[52] lattice_0.22-6        intervals_0.15.5      plyr_1.8.9           \n[55] withr_3.0.2           evaluate_1.0.1        units_0.8-5          \n[58] proxy_0.4-27          polyclip_1.10-7       xts_0.14.1           \n[61] pillar_1.9.0          KernSmooth_2.23-24    renv_1.1.2           \n[64] foreach_1.5.2         generics_0.1.3        spacetime_1.3-2      \n[67] hms_1.1.3             munsell_0.5.1         scales_1.3.0         \n[70] class_7.3-22          glue_1.8.0            mapproj_1.2.11       \n[73] tools_4.4.1           grid_4.4.1            colorspace_2.1-1     \n[76] cli_3.6.3             spatstat.sparse_3.1-0 gstat_2.1-2          \n[79] textshaping_0.4.0     fansi_1.0.6           doSNOW_1.0.20        \n[82] gtable_0.3.5          digest_0.6.37         classInt_0.4-10      \n[85] htmlwidgets_1.6.4     farver_2.1.2          memoise_2.0.1        \n[88] htmltools_0.5.8.1     lifecycle_1.0.4       httr_1.4.7",
    "crumbs": [
      "Data sources",
      "Weather data"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About this Study and Repository",
    "section": "",
    "text": "Kaique S. Alves1,2; Denis A. Shah3; Helene. R. Dillard4; Emerson M. Del Ponte1; Sarah J. Pethybridge2*\n\n1 Departamento de Fitopatologia, Universidade Federal de Viçosa, Viçosa, MG 36570-900, Brazil\n2 Plant Pathology & Plant-Microbe Biology Section, School of Integrative Plant Science, Cornell University, Geneva, NY 14456, USA\n3 Department of Plant Pathology, Kansas State University, Manhattan, KS 66506, USA\n4 College of Agricultural and Environmental Sciences, University of California, Davis, CA 95616, USA\n*Corresponding author: Sarah J. Pethybridge\nEmail: sjp277@cornell.edu",
    "crumbs": [
      "Repository",
      "About this Study and Repository"
    ]
  },
  {
    "objectID": "index.html#summary",
    "href": "index.html#summary",
    "title": "About this Study and Repository",
    "section": "Summary",
    "text": "Summary\nWe developed a workflow for enhancing small observational datasets to model plant disease risk. Using white mold in snap beans as a case study, we fused georeferenced disease observations with soil variables and features from downscaled weather data. Functional data analysis identified key periods in the weather series, and random forests highlighted critical predictors, which were used to build a simpler logistic regression model. This approach demonstrates how machine learning can guide the development of calibrated, practical models, addressing overfitting and aligning analytical complexity with dataset size.",
    "crumbs": [
      "Repository",
      "About this Study and Repository"
    ]
  },
  {
    "objectID": "index.html#about-this-website",
    "href": "index.html#about-this-website",
    "title": "About this Study and Repository",
    "section": "About this website",
    "text": "About this website\nThis website is intended to help you reproduce our complete data analysis workflow using R. It contains all the scripts and documentation you need to replicate the study and understand each step of the process.\nThe full research compendium with data and scripts used for this study is stored in an Open Science Framework repository and can be accessed here.",
    "crumbs": [
      "Repository",
      "About this Study and Repository"
    ]
  },
  {
    "objectID": "index.html#general-instructions",
    "href": "index.html#general-instructions",
    "title": "About this Study and Repository",
    "section": "General instructions",
    "text": "General instructions\nThe analyses code were written using R version 4.4.1 and scripted on Quarto (.qmd) documents. Before starting, please ensure you have the necessary software installed:\n\nR:\nIf you haven’t installed R yet, download it from the Comprehensive R Archive Network (CRAN).\nRStudio:\nFor an enhanced development environment, download RStudio from the RStudio Download Page.\nQuarto: Follow instruction on the Get Started page of the Quarto website to get started with Quarto documents in R.",
    "crumbs": [
      "Repository",
      "About this Study and Repository"
    ]
  },
  {
    "objectID": "index.html#downloading-the-codes-repository",
    "href": "index.html#downloading-the-codes-repository",
    "title": "About this Study and Repository",
    "section": "Downloading the codes repository",
    "text": "Downloading the codes repository\nTo Download the codes and run the analysis in your machine, please click here. This will download a .zip file. You should unzip it before runing the codes.\n\nCloning\nTo clone the repository, you’ll need Git installed on your system. Then, simply open your terminal (or command prompt on Windows) and run the following command:\n\ngit clone https://github.com/AlvesKS/paper-white-mold-transfer-learning.git",
    "crumbs": [
      "Repository",
      "About this Study and Repository"
    ]
  },
  {
    "objectID": "index.html#running-the-codes",
    "href": "index.html#running-the-codes",
    "title": "About this Study and Repository",
    "section": "Running the codes",
    "text": "Running the codes\nAfter the downloading the repository from the link provided above, refer to the Getting started page to run the codes in your machine.\n\nRepository Structure\n\nRunning the Codes:\nAs noted in the “Running the codes” section, once you download the repository, refer to the Getting started page to execute the R scripts on your machine.\nData Sources:\nThe repository is organized into several key sections:\n\nGetting started: Initial setup instructions for downloading the data necessary to run the analysis. You should start here!\nWeather data: Datasets and scripts related to weather variables.\nSoil data: Code for processing soil data.\nCanopy closure: Predicting canopy closure.\nFuse the white mold datasets: Procedures to merge and clean the datasets.\n\nFunctional Data Analysis:\nThis section is divided into:\n\nExploratory Analysis: Preliminary data exploration and visualization.\nFunction-on-Scalar Regressions for the white mold data: Advanced functional data analysis to identify significant weather windows.\n\nData Analysis:\nContains the final analysis and modeling:\n\nModels for White Mold: Detailed machine learning modeling, validation and their interpretation.\n\n\n\n\nHow to Reproduce the Analysis\n\nDownload the Repository:\nClone or download the repository from the link provided above.\nInstall Software:\nMake sure that both R and RStudio are installed on your system using the links provided above.\nRun the Codes:\nOpen the getting-started.qmd file in RStudio and follow the step-by-step instructions within to run the code sections sequentially.\nExplore the Sections:\nNavigate through the sections on Data Sources, Functional Data Analysis, and Data Analysis to understand the workflow and explore the results.\n\nBy following these steps, you will be able to fully reproduce the data analysis presented in this study. If you encounter any issues, please refer to the documentation within each section or reach out to the repository maintainer for assistance.",
    "crumbs": [
      "Repository",
      "About this Study and Repository"
    ]
  },
  {
    "objectID": "index.html#research-compendium",
    "href": "index.html#research-compendium",
    "title": "About this Study and Repository",
    "section": "Research compendium",
    "text": "Research compendium\n\n\n\n\n\n\nPlease cite this research compendium as:\n\n\n\nAlves, K. S., Shah, D. A., Dillard, H. R., Del Ponte, E. M., & Pethybridge, S. J. (2022, August 3). Research Compendium: Safer and Smarter: Leveraging Transfer Learning and Data Fusion of Disease and Environmental Data for Modeling Plant Disease Risk. https://doi.org/10.17605/OSF.IO/V53PY",
    "crumbs": [
      "Repository",
      "About this Study and Repository"
    ]
  },
  {
    "objectID": "index.html#original-article",
    "href": "index.html#original-article",
    "title": "About this Study and Repository",
    "section": "Original article",
    "text": "Original article\n\n\n\n\n\n\nThis repository contains the data and code for our article:\n\n\n\nAlves, K.S., Shah, D.A., Dillard, H.R., Del Ponte, E.M., Pethybridge, S.J. (YYYY) Safer and Smarter: Leveraging Transfer Learning and Data Fusion of Disease and Environmental Data for Modeling Plant Disease Risk. Name of journal/book https://doi.org/xxx/xxx",
    "crumbs": [
      "Repository",
      "About this Study and Repository"
    ]
  },
  {
    "objectID": "index.html#read-the-preprint",
    "href": "index.html#read-the-preprint",
    "title": "About this Study and Repository",
    "section": "Read the Preprint",
    "text": "Read the Preprint\n\n\n\n\n\n\nOur pre-print is online on the OSF preprint server:\n\n\n\nAlves, K.S., Shah, D.A., Dillard, H.R., Del Ponte, E.M., Pethybridge, S.J. (YYYY) Safer and Smarter: Leveraging Transfer Learning and Data Fusion of Disease and Environmental Data for Modeling Plant Disease Risk. Online at &lt;doi.org/10.31219/osf.io/9tgau&gt;",
    "crumbs": [
      "Repository",
      "About this Study and Repository"
    ]
  },
  {
    "objectID": "index.html#licensing",
    "href": "index.html#licensing",
    "title": "About this Study and Repository",
    "section": "Licensing",
    "text": "Licensing\nCode: MIT year: 2022, copyright holder: Kaique S. Alves",
    "crumbs": [
      "Repository",
      "About this Study and Repository"
    ]
  },
  {
    "objectID": "index.html#repository-maintainer",
    "href": "index.html#repository-maintainer",
    "title": "About this Study and Repository",
    "section": "Repository maintainer",
    "text": "Repository maintainer\nKaique S. Alves\n\nPlant Health Scientist within the Global Plant Health team of Breeding - Bayer Crop sciecce\nD.Sc in the Plant Pathology graduate program from Universidade Federal de Viçosa in Brazil.\nVisiting Scholar in the EVADE program of the Plant Pathology & Plant-Microbe Biology Section, School of Integrative Plant Science, Cornell University.\n\nPersonal website | GitHub | Google Scholar",
    "crumbs": [
      "Repository",
      "About this Study and Repository"
    ]
  },
  {
    "objectID": "code_get_started_data.html",
    "href": "code_get_started_data.html",
    "title": "Getting started",
    "section": "",
    "text": "To ensure you have all the data needed to reproduce our analysis, please download the datasets used in our study using the commands below. Each dataset is provided as a ZIP archive, so we recommend downloading them in binary mode to prevent file corruption.",
    "crumbs": [
      "Data sources",
      "Getting started"
    ]
  },
  {
    "objectID": "code_get_started_data.html#about",
    "href": "code_get_started_data.html#about",
    "title": "Getting started",
    "section": "",
    "text": "To ensure you have all the data needed to reproduce our analysis, please download the datasets used in our study using the commands below. Each dataset is provided as a ZIP archive, so we recommend downloading them in binary mode to prevent file corruption.",
    "crumbs": [
      "Data sources",
      "Getting started"
    ]
  },
  {
    "objectID": "code_get_started_data.html#downloading-the-data",
    "href": "code_get_started_data.html#downloading-the-data",
    "title": "Getting started",
    "section": "Downloading the data",
    "text": "Downloading the data\nThe code bellow downloads the zip files, unzips them and let you preview the contents of each of the files. Make sure to run all these comands before runing any of the scripts of this repository.\n\nDisease data\n\ndownload.file(\"https://osf.io/download/ehy26/\", destfile = \"data_white-mold.zip\", mode = \"wb\")\nunzip(\"data_white-mold.zip\", list = FALSE)\n\n\n\nWeather data\n\ndownload.file(\"https://osf.io/download/k39bd/\", destfile = \"data_era5.zip\", mode = \"wb\")\nunzip(\"data_era5.zip\", list = FALSE)\n\n\n\nSoil data\n\ndownload.file(\"https://osf.io/download/2jfcz/\", destfile = \"soil_images.zip\", mode = \"wb\")\nunzip(\"soil_images.zip\", list = FALSE)",
    "crumbs": [
      "Data sources",
      "Getting started"
    ]
  },
  {
    "objectID": "code_get_started_data.html#session-info",
    "href": "code_get_started_data.html#session-info",
    "title": "Getting started",
    "section": "Session Info",
    "text": "Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Portuguese_Brazil.utf8  LC_CTYPE=Portuguese_Brazil.utf8   \n[3] LC_MONETARY=Portuguese_Brazil.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Portuguese_Brazil.utf8    \n\ntime zone: America/Sao_Paulo\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.1    fastmap_1.2.0     cli_3.6.3        \n [5] htmltools_0.5.8.1 tools_4.4.1       rstudioapi_0.17.0 yaml_2.3.10      \n [9] rmarkdown_2.28    knitr_1.48        jsonlite_1.8.9    xfun_0.48        \n[13] digest_0.6.37     rlang_1.1.4       renv_1.1.2        evaluate_1.0.1",
    "crumbs": [
      "Data sources",
      "Getting started"
    ]
  },
  {
    "objectID": "code_soil_variables_white-mold.html",
    "href": "code_soil_variables_white-mold.html",
    "title": "Soil data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(daymetr)\nlibrary(raster)\nlibrary(spatstat)\nlibrary(KrigR)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(viridis)\nlibrary(terra)\nlibrary(XPolaris)\nlibrary(MetBrewer)\n\n\n\n\n\n\n\nWarning\n\n\n\nAbout XPolaris\nXPolaris has been removed from CRAN. To install the GitHub available version:\n\nif (!require(XPolaris)) {\n  if (!require(devtools)) {\n    install.packages(\"devtools\")\n  }\n  devtools::install_github(\"lhmrosso/XPolaris\")\n}",
    "crumbs": [
      "Data sources",
      "Soil data"
    ]
  },
  {
    "objectID": "code_soil_variables_white-mold.html#packages",
    "href": "code_soil_variables_white-mold.html#packages",
    "title": "Soil data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(cowplot)\nlibrary(patchwork)\nlibrary(daymetr)\nlibrary(raster)\nlibrary(spatstat)\nlibrary(KrigR)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(viridis)\nlibrary(terra)\nlibrary(XPolaris)\nlibrary(MetBrewer)\n\n\n\n\n\n\n\nWarning\n\n\n\nAbout XPolaris\nXPolaris has been removed from CRAN. To install the GitHub available version:\n\nif (!require(XPolaris)) {\n  if (!require(devtools)) {\n    install.packages(\"devtools\")\n  }\n  devtools::install_github(\"lhmrosso/XPolaris\")\n}",
    "crumbs": [
      "Data sources",
      "Soil data"
    ]
  },
  {
    "objectID": "code_soil_variables_white-mold.html#white-mold-data",
    "href": "code_soil_variables_white-mold.html#white-mold-data",
    "title": "Soil data",
    "section": "White mold data",
    "text": "White mold data\nLoading white mold data. We are going to use the field coordinates (latitude and longitude) to extract the data from the rasters of soil variables.\n\n\n\n\n\n\nAbout the Data\n\n\n\nIf you have not downloaded the data necessary to run these analysis, please refer to getting started section before runnig the code below\n\n\n\nwm_data = read.csv(\"data_white-mold/WhiteMoldSurveyWrangledData.csv\")\n\n\nRemoving missing coordinates\nThere are some missing coordinates in the dataset. Here we remove them.\n\nwm_data2 = wm_data %&gt;% \n  filter(!is.na(latitude))",
    "crumbs": [
      "Data sources",
      "Soil data"
    ]
  },
  {
    "objectID": "code_soil_variables_white-mold.html#soil-variables",
    "href": "code_soil_variables_white-mold.html#soil-variables",
    "title": "Soil data",
    "section": "Soil variables",
    "text": "Soil variables\n\nLocations\nSetting up the coordinated of each quadrat we need to download the soil data\n\n# exkansas\nny_locations = data.frame(ID = c(\"NY1\",\"NY2\",\"NY3\",\"NY4\",\"NY5\",\"NY6\",\"NY7\",\"NY8\"),\n                          lat  = c(42,  42,  42, 42, 43,   43,   43,   43),\n                          long = c(-77, -78,-79, -80,-77, -78.0,-79.0, -80))\n\nxplot(locations = ny_locations)+\n  geom_point(data = wm_data2 , size = 0.3,\n             aes(longitude,latitude))+\n  coord_map( xlim = c(-81,-75),\n             ylim = c(41.5,44.5))\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\n\n\n\n\n\n\n# max(wm_data2$longitude)\n\n\n\nDownload soil data\n\nny_images &lt;- ximages(locations = ny_locations,\n                      statistics = c('mean'),\n                      variables = c('ph','om','clay',\"sand\",\"silt\",\"bd\", \"hb\",\"n\",\"alpha\",\"ksat\",\"lambda\",\"theta_r\",\"theta_s\"),\n                      layersdepths = c('0_5'),\n                      localPath = file.path(\"soil_images\"))\n\nHere we read the metadata of the downloaded data. It containg the information regarding the directory of each downloaded file.\n\n\nMerge images\n\nOrganic matter\nWe are going to filter only the lines contain information regarding Organic matter\n\nom_df_images = ny_images %&gt;% \n  filter(variables == \"om\")\n\nAs you can notice in the next figure, the raster objects are downloaded in separate raster of 01 degree size\n\n# read all files for organic matter\nom_stack = lapply(om_df_images$local_file, brick)\n\n\npar(mfrow = c(2,4))\n\nfor(i in 1:length(om_stack)){\nplot(om_stack[[i]])\n}\n\n\n\n\n\n\n\n\nTherefore, we use the function merge() to create a single raster object covering the whole study region\n\nom_ny_raster =merge(om_stack[[1]],\n                    om_stack[[2]],\n                    om_stack[[3]],\n                    om_stack[[4]],\n                    om_stack[[5]],\n                    om_stack[[6]],\n                    om_stack[[7]],\n                    om_stack[[8]])\n\nplot(exp(om_ny_raster))\n\n\n\n\n\n\n\n\n\n\n\nAutomatization\nInstead of doing the above step for each variable by hand, here we created a function to do all the steps automatically.\n\nget_soil_var = function(var, data){\n  data_filtered = data %&gt;% \n  filter(variables == var)\n\nstack_list = lapply(data_filtered$local_file, brick)\n  \n  \n  merged_raster = merge(stack_list[[1]],\n                    stack_list[[2]],\n                    stack_list[[3]],\n                    stack_list[[4]],\n                    stack_list[[5]],\n                    stack_list[[6]],\n                    stack_list[[7]],\n                    stack_list[[8]]\n                    )\n  return(merged_raster)\n}\n\nsoil_vars = c('ph','om','clay',\"sand\",\"silt\",\"bd\", \"hb\",\"n\",\"alpha\",\"ksat\",\"lambda\",\"theta_r\",\"theta_s\")\n\nselected_vars = c('ph','om','clay',\"sand\",\"silt\",\"bd\",\"theta_r\",\"theta_s\")\n\n\nsoil_variables_list = lapply(soil_vars,get_soil_var, data = ny_images)\nnames(soil_variables_list) = soil_vars\n\nsaveRDS(soil_variables_list, \"soil_images/list_soil_variables_raster.rds\")\n\n\naggre_var_list = lapply(soil_variables_list, aggregate,fact=30)\n\nsaveRDS(aggre_var_list, \"soil_images/list_soil_variables_raster_aggregated.rds\")\n\n\n\nPlot soil maps\n\nNY shape file\nLoading New York state shape file.\n\nny_shape1 = read_sf(\"shape_files/cugir-007865/cugir-007865/cty036.shp\")\n\n\n\nCropping raster files\nWe use the function lappy() to crop all variables’ rasters using the NY shape file as a mask.\n\naggre_var_list2 = lapply(aggre_var_list, mask, ny_shape1)\n\nThen here we create a function for ploting the soil maps.\n\nactual_var_names = c(\"Soil pH in water\", \"Soil organic matter\",\"Clay\",\"Sand\",\"Silt\",\"Bulk density\",\"Residual soil water content\",\"Saturated soil water content\")\nactual_var_symbol = c(\"pH\", \"OM\",\"Clay\",\"Sand\",\"Silt\",\"BD\",\"\\u03B8r\",\"\\u03B8s\")\nactual_var_units = c(\"\", \"(%)\",\"(%)\",\"(%)\",\"(%)\",\"(g/cm³)\",\"(m³/m³)\",\"(m³/m³)\")\n\nplot_gg_raster =  function(X,raster, var){\n  # actual_var_names[X]\n\nif(var[X] == \"om\"){xx=1}else{xx = 0}\n  \nas.data.frame(raster[[var[X]]], xy = T) %&gt;%\n    filter(layer !=\"NaN\", x&lt; -76.8) %&gt;%\n  mutate(layer = case_when(xx ==1~ exp(layer),\n                           xx ==0~ layer)) %&gt;% \n    ggplot(aes())+\n    geom_raster(aes(x, y, fill = layer))+\n    scale_fill_viridis(option =\"B\",guide = guide_colorbar(barwidth = 0.2, barheight =5 ))+\n    geom_sf(data = ny_shape1,\n                 # aes(x=long, y = lat, group = group),\n                 fill= NA,\n                 linewidth =0.2,\n                 alpha = 0.5,\n                 color = \"white\")+\n  # geom_point(data = wm_data2 , size = 0.1,color = \"white\",\n  #            aes(longitude,latitude))+\n    # coord_quickmap(xlim = c(-80,-76.8), ylim = c(42,43.35))+\n    coord_sf(xlim = c(-80,-76.8), ylim = c(42,43.35))+\n    theme_map()+\n    labs(title =paste(\"    \",actual_var_names[X]),\n         fill = paste(actual_var_symbol[X],actual_var_units[X]))\n}\n\n# selected_vars\n# aggre_var_list[[1]]\n# plot_gg_raster(1,aggre_var_list2, var = selected_vars[1] )\n\n\nas.data.frame(aggre_var_list2$ph, xy = T) %&gt;%\n    filter(layer !=\"NaN\", x&lt; -76.8) %&gt;%\n  # mutate(layer = case_when(xx ==1~ exp(layer),\n  #                          xx ==0~ layer)) %&gt;% \n    ggplot(aes())+\n    geom_raster(aes(x, y, fill = layer))+\n    scale_fill_viridis(option =\"B\",guide = guide_colorbar(barwidth = 0.2, barheight =5 ))+\n    geom_sf(data = ny_shape1,\n                 # aes(x=long, y = lat, group = group),\n                 fill= NA,\n                 linewidth =0.2,\n                 alpha = 0.5,\n                 color = \"white\")\n\n\n\n\n\n\n\n\n\n\nCombo soil maps\nHere we plot all maps into a single combo fiure\n\ndo.call(patchwork::wrap_plots, lapply(X =1:length(selected_vars) , FUN =plot_gg_raster, raster = aggre_var_list2, var = selected_vars))+\n  plot_layout(ncol = 2)+\n  plot_annotation(tag_levels = \"A\")&\n  theme(legend.position = \"right\",\n        legend.text = element_text(size = 5),\n        legend.title = element_text(size = 5),\n        plot.title =  element_text(size = 7, face = \"bold\"))\n\n\n\n\n\n\n\nggsave(\"figs/maps/soil_maps.png\", dpi = 900, height = 7, width = 7, bg = \"white\")",
    "crumbs": [
      "Data sources",
      "Soil data"
    ]
  },
  {
    "objectID": "code_soil_variables_white-mold.html#extract-variables-to-location",
    "href": "code_soil_variables_white-mold.html#extract-variables-to-location",
    "title": "Soil data",
    "section": "Extract variables to location",
    "text": "Extract variables to location\n\nwm_data2_uni = wm_data2 %&gt;% \n  group_by(subject) %&gt;% \n  slice(1L)\n\nSelecting coordinate columns from the white mold data set\n\ncoords&lt;-data.frame(lon=wm_data2_uni$longitude, lat=wm_data2_uni$latitude)\ncoordinates(coords)&lt;-c(\"lon\",\"lat\")\n\nExtracting variable from the original merged raster (30 meters resolution)\n\ndf1 = lapply(soil_variables_list, extract, coords@coords)\nas.data.frame(df1) %&gt;% \n  mutate(subject =wm_data2_uni$subject) %&gt;% \n  cbind(longitude=wm_data2_uni$longitude, \n        latitude=wm_data2_uni$latitude) %&gt;% \n  write.csv(\"soil_images/extracted_soil_data.csv\",row.names = F)",
    "crumbs": [
      "Data sources",
      "Soil data"
    ]
  },
  {
    "objectID": "code_soil_variables_white-mold.html#session-info",
    "href": "code_soil_variables_white-mold.html#session-info",
    "title": "Soil data",
    "section": "Session Info",
    "text": "Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Portuguese_Brazil.utf8  LC_CTYPE=Portuguese_Brazil.utf8   \n[3] LC_MONETARY=Portuguese_Brazil.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Portuguese_Brazil.utf8    \n\ntime zone: America/Sao_Paulo\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] knitr_1.48             MetBrewer_0.2.0        XPolaris_1.0.2        \n [4] terra_1.8-5            viridis_0.6.5          viridisLite_0.4.2     \n [7] sf_1.0-19              KrigR_0.9.4            spatstat_3.3-0        \n[10] spatstat.linnet_3.2-3  spatstat.model_3.3-3   rpart_4.1.23          \n[13] spatstat.explore_3.3-3 nlme_3.1-164           spatstat.random_3.3-2 \n[16] spatstat.geom_3.3-4    spatstat.univar_3.1-1  spatstat.data_3.1-4   \n[19] raster_3.6-30          sp_2.1-4               daymetr_1.7.1         \n[22] patchwork_1.3.0        cowplot_1.1.3          lubridate_1.9.3       \n[25] forcats_1.0.0          stringr_1.5.1          dplyr_1.1.4           \n[28] purrr_1.0.2            readr_2.1.5            tidyr_1.3.1           \n[31] tibble_3.2.1           ggplot2_3.5.1          tidyverse_2.0.0       \n\nloaded via a namespace (and not attached):\n [1] DBI_1.2.3             pbapply_1.7-2         deldir_2.0-4         \n [4] gridExtra_2.3         rlang_1.1.4           magrittr_2.0.3       \n [7] e1071_1.7-16          compiler_4.4.1        mgcv_1.9-1           \n[10] systemfonts_1.1.0     maps_3.4.2.1          vctrs_0.6.5          \n[13] ecmwfr_2.0.3          crayon_1.5.3          pkgconfig_2.0.3      \n[16] fastmap_1.2.0         labeling_0.4.3        utf8_1.2.4           \n[19] rmarkdown_2.28        tzdb_0.4.0            ragg_1.3.3           \n[22] automap_1.1-12        xfun_0.48             cachem_1.1.0         \n[25] jsonlite_1.8.9        progress_1.2.3        goftest_1.2-3        \n[28] reshape_0.8.9         spatstat.utils_3.1-1  prettyunits_1.2.0    \n[31] parallel_4.4.1        R6_2.5.1              stringi_1.8.4        \n[34] stars_0.6-7           Rcpp_1.0.13           iterators_1.0.14     \n[37] tensor_1.5            zoo_1.8-12            snow_0.4-4           \n[40] FNN_1.1.4.1           Matrix_1.7-0          splines_4.4.1        \n[43] timechange_0.3.0      tidyselect_1.2.1      rstudioapi_0.17.0    \n[46] abind_1.4-8           yaml_2.3.10           codetools_0.2-20     \n[49] curl_6.0.1            lattice_0.22-6        intervals_0.15.5     \n[52] plyr_1.8.9            withr_3.0.2           evaluate_1.0.1       \n[55] units_0.8-5           proxy_0.4-27          polyclip_1.10-7      \n[58] xts_0.14.1            pillar_1.9.0          KernSmooth_2.23-24   \n[61] renv_1.1.2            foreach_1.5.2         ncdf4_1.23           \n[64] generics_0.1.3        spacetime_1.3-2       hms_1.1.3            \n[67] munsell_0.5.1         scales_1.3.0          class_7.3-22         \n[70] glue_1.8.0            mapproj_1.2.11        tools_4.4.1          \n[73] grid_4.4.1            colorspace_2.1-1      cli_3.6.3            \n[76] spatstat.sparse_3.1-0 gstat_2.1-2           textshaping_0.4.0    \n[79] fansi_1.0.6           doSNOW_1.0.20         gtable_0.3.5         \n[82] digest_0.6.37         classInt_0.4-10       htmlwidgets_1.6.4    \n[85] farver_2.1.2          memoise_2.0.1         htmltools_0.5.8.1    \n[88] lifecycle_1.0.4       httr_1.4.7",
    "crumbs": [
      "Data sources",
      "Soil data"
    ]
  },
  {
    "objectID": "code_data_fusion.html",
    "href": "code_data_fusion.html",
    "title": "Merge the white mold datasets",
    "section": "",
    "text": "Merge the different sources of variables linked to the field observations:\n- the survey data (subject and wm response variable)\n- agronomic (drainage, hydrol, year, cd, harv.optim)\n- soils (ph, om, log_sand_clay, log_silt_clay)\n- canopy closure at 35 dap\n- total rain from planting to 35 dap (pre-bloom)\n- total rain from 36-50 dap (bloom through early pod fill)\n- vars identified from FDA:\n- t2m_mean: start = 0, end = 4\n- sm: start = -4, end = 3\n- sm: start = 5, end = 15 - sm: start = 17, end = 24\n- sm: start = 40, end = 49\n- stsm: start = 35, end = 44",
    "crumbs": [
      "Data sources",
      "Merge the white mold datasets"
    ]
  },
  {
    "objectID": "code_data_fusion.html#session-info",
    "href": "code_data_fusion.html#session-info",
    "title": "Merge the white mold datasets",
    "section": "Session Info",
    "text": "Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Portuguese_Brazil.utf8  LC_CTYPE=Portuguese_Brazil.utf8   \n[3] LC_MONETARY=Portuguese_Brazil.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Portuguese_Brazil.utf8    \n\ntime zone: America/Sao_Paulo\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] kableExtra_1.4.0 lubridate_1.9.3  forcats_1.0.0    stringr_1.5.1   \n [5] dplyr_1.1.4      purrr_1.0.2      readr_2.1.5      tidyr_1.3.1     \n [9] tibble_3.2.1     ggplot2_3.5.1    tidyverse_2.0.0  knitr_1.48      \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5      jsonlite_1.8.9    compiler_4.4.1    renv_1.1.2       \n [5] tidyselect_1.2.1  xml2_1.3.6        systemfonts_1.1.0 scales_1.3.0     \n [9] yaml_2.3.10       fastmap_1.2.0     here_1.0.1        R6_2.5.1         \n[13] generics_0.1.3    htmlwidgets_1.6.4 rprojroot_2.0.4   munsell_0.5.1    \n[17] svglite_2.1.3     pillar_1.9.0      tzdb_0.4.0        rlang_1.1.4      \n[21] utf8_1.2.4        stringi_1.8.4     xfun_0.48         viridisLite_0.4.2\n[25] timechange_0.3.0  cli_3.6.3         withr_3.0.2       magrittr_2.0.3   \n[29] digest_0.6.37     grid_4.4.1        rstudioapi_0.17.0 hms_1.1.3        \n[33] lifecycle_1.0.4   vctrs_0.6.5       evaluate_1.0.1    glue_1.8.0       \n[37] fansi_1.0.6       colorspace_2.1-1  rmarkdown_2.28    tools_4.4.1      \n[41] pkgconfig_2.0.3   htmltools_0.5.8.1",
    "crumbs": [
      "Data sources",
      "Merge the white mold datasets"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html",
    "href": "code_function_on_scalar.html",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "",
    "text": "library(fda)\nlibrary(refund)\nlibrary(fdatest)\nlibrary(tidyverse)\nlibrary(tictoc)\nlibrary(cowplot)\nlibrary(kableExtra)",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#packages",
    "href": "code_function_on_scalar.html#packages",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "",
    "text": "library(fda)\nlibrary(refund)\nlibrary(fdatest)\nlibrary(tidyverse)\nlibrary(tictoc)\nlibrary(cowplot)\nlibrary(kableExtra)",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#load-and-process-the-data",
    "href": "code_function_on_scalar.html#load-and-process-the-data",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "Load and process the data",
    "text": "Load and process the data\n\nwm_load &lt;- readr::read_csv(here::here(\"Data\", \"data_model_plus_weather_filtered.csv\"), show_col_types = FALSE)\n\n# Simplify things. Keep only the weather-related variables and others needed for calculations\nwm_data &lt;-\n  wm_load %&gt;%\n  dplyr::select(subject, date, planting.date, sampling.date, wm, d2m, t2m_mean, t2m_max, t2m_min, st, sp, sm, rh) %&gt;%\n  # Do the filtering steps before doing any calculations or feature engineering:\n  # wm_load has identical data for each of the sampling dates, which is why there is a filtering step on sampling.date.\n  dplyr::group_by(subject) %&gt;% \n  dplyr::filter(sampling.date == max(sampling.date)) %&gt;%\n  dplyr::ungroup() %&gt;% \n  # Add the response variable (wm present or absent; binary):\n  dplyr::group_by(subject) %&gt;% \n  dplyr::mutate(wm = (mean(wm, na.rm = T) &gt; 0)*1) %&gt;% \n  # wm as a factor:\n  dplyr::mutate(wm = factor(wm, levels = c(0, 1))) %&gt;%\n  dplyr::ungroup() %&gt;% \n  dplyr::filter(!is.na(wm)) %&gt;% \n  # Calculate dap (as a numeric):\n  dplyr::mutate(dap = as.numeric(date - planting.date)) %&gt;%\n  # Convert temperatures from Kelvin to degrees Celsius:\n  dplyr::mutate(across(d2m:st, ~ .x - 273.15)) %&gt;%\n  # dewpoint depression:\n  dplyr::mutate(dpd = t2m_mean - d2m) %&gt;%\n  # surface pressure in kPa:\n  dplyr::mutate(sp = sp/1000) %&gt;%\n  # Ratio of soil temperature to soil moisture:\n  dplyr::mutate(stsm = st/sm) %&gt;%\n  # Difference in max and min temperatures:\n  dplyr::mutate(tdiff = t2m_max - t2m_min) %&gt;%\n  # Calculate saturation vapor pressure (es):\n  dplyr::mutate(es = 0.61078 * exp((17.27 * t2m_mean) / (t2m_mean + 237.3))) %&gt;% \n  # Calculate actual vapor pressure (ea):\n  dplyr::mutate(ea = (rh / 100) * es) %&gt;% \n  # Calculate VPD (kPa):\n  dplyr::mutate(vpd = es - ea) %&gt;% \n  # estimate GDD (base 0). NB: base 0 is reasonable for snap bean; see the Jenni et al. (2000) paper. I think we want GDD to start accumulating the day after planting date onwards...\n  # I use the day after planting, because we don't know exactly what time of the day the field was planted.\n  dplyr::mutate(gddi = ifelse(dap &lt;= 0, 0, (t2m_max + t2m_min)*0.5 - 0)) %&gt;%\n  dplyr::group_by(subject) %&gt;% \n  # We don't want the gddi column after creating gdd:\n  dplyr::mutate(gdd = cumsum(gddi), .keep = \"unused\") %&gt;%\n  dplyr::ungroup() %&gt;%\n  # Keep only the columns you need:\n  dplyr::select(subject, dap, wm:gdd, -es, -ea) %&gt;%\n  # Filter the dataset to dap &lt;= 50:\n  dplyr::filter(dap &lt;= 50) %&gt;%\n  # wm as a contrast: -1 = absent, 1 = present (needed for function-on-scalar regression):\n  dplyr::mutate(wm = as.numeric(wm)-1) %&gt;%\n  dplyr::mutate(wm = 2*wm-1) \n\n\ndata.prep &lt;- function(x) {\n  # Prepare the data for function-on-scalar regression\n  # N x J matrix of the functional predictor \n  # Args:\n  #  x = unquoted variable name\n  # Returns:\n  #  a list with the following named elements:\n  #   y = N x J matrix of the functional response\n  #   x = a vector of the wm status coded as -1, 1\n  #   yind =  a vector of the evaluation days, which is of length 81 from -30 to 50\n  \n  .x &lt;- enquo(x)\n  \n  # The environmental data:\n  ez &lt;-\n    wm_data %&gt;%\n    dplyr::select(subject, dap, !!.x) %&gt;%\n    tidyr::pivot_wider(id_cols = subject, names_from = dap, names_prefix = \"\", values_from = !!.x) %&gt;%\n    dplyr::select(-subject) %&gt;%\n    as.matrix()\n  \n  colnames(ez) &lt;- NULL\n  \n  # the vector of wm status:\n  x.wm &lt;- \n    wm_data %&gt;%\n    dplyr::group_by(subject) %&gt;%\n    dplyr::summarise(wm = mean(wm)) %&gt;%\n    dplyr::pull(wm)\n  \n  # the vector of the evaluation days:\n  days &lt;- seq(-30, 50)\n  \n  # the final data frame:\n  df &lt;- list(y = ez, x = x.wm, yind = days)\n  \n  return(df)\n  \n}\n\n# Example of use:\n# dat &lt;- data.prep(d2m)\n\nfANOVA &lt;- function(dat) {\n  # Performs a functional ANOVA and plots the estimated coefs for the overall mean and \n  # the difference between wm(0) and wm(1).\n  #\n  # Args:\n  #   dat: prepared data from calling the data.prep function\n  #\n  # Returns:\n  #   a ggplot graphic of the estimated beta(t) coefs.\n  \n  \n  # k = 30 gives wiggliness, no oversmoothing\n  m2 &lt;- pffr(y~x, yind = dat$yind, data = dat,\n             bs.yindex = list(bs = \"ps\", k = 30, m = c(2, 1)), \n             bs.int = list(bs = \"ps\", k = 30, m = c(2, 1)))\n  \n  \n  # The smooth coefficients for the overall mean and beta(t):\n  z &lt;- coef(m2)$smterms\n  \n  # 1. For the overall mean:\n  a1 &lt;- z[[1]]$coef[, \"yindex.vec\"]\n  a2 &lt;- z[[1]]$coef[, \"value\"]\n  a3 &lt;- z[[1]]$coef[, \"se\"]\n  \n  # 2. For beta(t):\n  b1 &lt;- z[[2]]$coef[, \"yindex.vec\"]\n  b2 &lt;- z[[2]]$coef[, \"value\"]\n  b3 &lt;- z[[2]]$coef[, \"se\"]\n\n  \n  # Extract parts of the output to produce a more attractive plot in ggplot.\n  # The coefs for the overall mean, adding an approx. 95% CI:\n  X &lt;- data.frame(x = a1, coef.mean = a2, lower.mean = a2 - 1.96*a3, upper.mean = a2 + 1.96*a3, coef = \"Overall mean\")\n  \n  # The coefs for the time trend of the mean difference between epidemics and non-epidemics:\n  Y &lt;- data.frame(x = b1, coef.mean = b2, lower.mean = b2 - 1.96*b3, upper.mean = b2 + 1.96*b3, coef = \"Difference\")\n  \n  Z &lt;- rbind(X, Y)\n  \n  # To get different lines in each facet, you need another data.frame:\n  hline.data &lt;- data.frame(z = c(0), coef = c(\"Difference\"))\n  \n  breaks &lt;- seq(-30, 50, 10)\n  labels &lt;- seq(-30, 50, 10)\n  \n  Z %&gt;%\n  dplyr::mutate(coef = factor(coef, levels = c(\"Difference\", \"Overall mean\"))) %&gt;%\n  ggplot(., aes(x = x, y = coef.mean)) +\n  annotate(\"rect\", ymin = -Inf, ymax = Inf, xmin = 35, xmax = 50, fill = \"steelblue\", alpha = 0.2) +\n  geom_ribbon(aes(ymin = lower.mean, ymax = upper.mean), alpha = 0.2) +\n  geom_path(linewidth = 1.2) +\n  geom_hline(aes(yintercept = 0), color = \"grey\", linetype = \"dashed\") +\n  geom_vline(aes(xintercept = 0), color = \"grey\", linetype = \"dashed\") +\n  theme_bw() +\n  facet_grid(coef ~ ., scales = \"free_y\") +\n  theme(strip.text = element_text(face = \"bold\", size = rel(1.0))) +\n  scale_x_continuous(name = \"Days relative to sowing\", breaks = breaks, labels = labels) +\n  theme(axis.title.x = element_text(face = \"bold\", size = 11)) +\n  ylab(\"Coefficient function\") +\n  theme(axis.title.y = element_text(face = \"bold\", size = 11))\n  }",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#dew-point",
    "href": "code_function_on_scalar.html#dew-point",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "Dew point",
    "text": "Dew point\n\nfANOVA(dat = data.prep(d2m))\n\nusing seWithMean for  s(yindex.vec) .",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#mean-air-temperature",
    "href": "code_function_on_scalar.html#mean-air-temperature",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "Mean air temperature",
    "text": "Mean air temperature\n\nfANOVA(dat = data.prep(t2m_mean))\n\nusing seWithMean for  s(yindex.vec) .",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#max-air-temperature",
    "href": "code_function_on_scalar.html#max-air-temperature",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "Max air temperature",
    "text": "Max air temperature\n\nfANOVA(dat = data.prep(t2m_max))\n\nusing seWithMean for  s(yindex.vec) .",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#min-air-temperature",
    "href": "code_function_on_scalar.html#min-air-temperature",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "Min air temperature",
    "text": "Min air temperature\n\nfANOVA(dat = data.prep(t2m_min))\n\nusing seWithMean for  s(yindex.vec) .",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#max---min-air-temperature",
    "href": "code_function_on_scalar.html#max---min-air-temperature",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "Max - Min air temperature",
    "text": "Max - Min air temperature\n\nfANOVA(dat = data.prep(tdiff))\n\nusing seWithMean for  s(yindex.vec) .",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#temperature-dewpoint-depression",
    "href": "code_function_on_scalar.html#temperature-dewpoint-depression",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "Temperature-Dewpoint depression",
    "text": "Temperature-Dewpoint depression\n\nfANOVA(dat = data.prep(dpd))\n\nusing seWithMean for  s(yindex.vec) .",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#soil-temperature",
    "href": "code_function_on_scalar.html#soil-temperature",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "Soil temperature",
    "text": "Soil temperature\n\nfANOVA(dat = data.prep(st))\n\nusing seWithMean for  s(yindex.vec) .",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#soil-moisture",
    "href": "code_function_on_scalar.html#soil-moisture",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "Soil moisture",
    "text": "Soil moisture\n\nfANOVA(dat = data.prep(sm))\n\nusing seWithMean for  s(yindex.vec) .",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#soil-temperaturesoil-moisture-ratio",
    "href": "code_function_on_scalar.html#soil-temperaturesoil-moisture-ratio",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "soil temperature:soil moisture ratio",
    "text": "soil temperature:soil moisture ratio\n\nfANOVA(dat = data.prep(stsm))\n\nusing seWithMean for  s(yindex.vec) .",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#surface-pressure",
    "href": "code_function_on_scalar.html#surface-pressure",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "Surface pressure",
    "text": "Surface pressure\n\nfANOVA(dat = data.prep(sp))\n\nusing seWithMean for  s(yindex.vec) .",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#relative-humidity",
    "href": "code_function_on_scalar.html#relative-humidity",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "Relative humidity",
    "text": "Relative humidity\n\nfANOVA(dat = data.prep(rh))\n\nusing seWithMean for  s(yindex.vec) .",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#vpd",
    "href": "code_function_on_scalar.html#vpd",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "VPD",
    "text": "VPD\n\nfANOVA(dat = data.prep(vpd))\n\nusing seWithMean for  s(yindex.vec) .",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#growing-degree-days",
    "href": "code_function_on_scalar.html#growing-degree-days",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "Growing degree days",
    "text": "Growing degree days\n\nfANOVA(dat = data.prep(gdd))\n\nusing seWithMean for  s(yindex.vec) .\n\n\n\n\n\n\n\n\n\n\n\n\n# We will illustrate fdatest with the d2m variable.\n\n# For fdatest, we need to create two separate matrices (for wm = 0 and wm = 1), which we can then put into a list for convenience.\nwm0 &lt;- \n  wm_data %&gt;%\n  dplyr::filter(wm == -1) %&gt;%\n  dplyr::select(subject, dap, d2m) %&gt;%\n  tidyr::pivot_wider(id_cols = subject, names_from = dap, names_prefix = \"\", values_from = d2m) %&gt;%\n  dplyr::select(-subject) %&gt;%\n  as.matrix()\n# and ...\ncolnames(wm0) &lt;- NULL\n\nwm1 &lt;- \n  wm_data %&gt;%\n  dplyr::filter(wm == 1) %&gt;%\n  dplyr::select(subject, dap, d2m) %&gt;%\n  tidyr::pivot_wider(id_cols = subject, names_from = dap, names_prefix = \"\", values_from = d2m) %&gt;%\n  dplyr::select(-subject) %&gt;%\n  as.matrix()\ncolnames(wm1) &lt;- NULL\n\n# Performing the ITP:\nITP.result &lt;- fdatest::ITP2bspline(wm0, wm1, B = 100)\n\n# The function generates a print line for each iteration. To suppress that, wrap within sink:\ntic()\n{ sink(type = \"message\"); ITP.result &lt;- fdatest::ITP2bspline(wm0, wm1, B = 1000); sink() }\ntoc()  # 9.18 sec\n\n# Plotting the results of the ITP:\n# (there are two plots. The first is of the individual curves. The 2nd is of the adjusted p-values)\nplot(ITP.result, main = NULL, xrange = c(1, 365), xlab = 'Day')\n\n# Plotting the p-values heatmap\n# (I'm not finding it all that telling)\nITPimage(ITP.result, abscissa.range = c(0, 1))\n\n# Selecting the significant components at 5% level:\nwhich(ITP.result$corrected.pval &lt; 0.05)\n\n# Which corresponds to the following days (relative to sowing, where sowing is day = 0):\nseq(-30, 50)[31:32]\n\n# NEXT:\n# Take the above code, and wrap into functions to (i) prep the data for input into fdatest, (ii) calling the fdatest functions to estimate the adjusted p values\n\n\nmake_kable &lt;- function(...) {\n  # kable and kableExtra styling to avoid repetitively calling the styling over and over again\n  # See: https://stackoverflow.com/questions/73718600/option-to-specify-default-kableextra-styling-in-rmarkdown\n  # knitr::kable(...) %&gt;%\n  kable(..., format = \"html\", row.names = TRUE, align = 'l') %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\"), position = \"left\", font_size = 11, full_width = FALSE) \n}\n\ndo.fdatest &lt;- function(x, alpha) {\n  # Performs an Interval Testing Procedure for testing the difference between the two functional wm populations evaluated on a uniform grid\n  #\n  # Args:\n  #  x = unquoted variable name, e.g. d2m\n  #  alpha = numeric giving the p valaue level for the test, e.g. 0.05\n  #\n  # Returns:\n  #   a tibble of the variable with a list vector of the days (relative to sowing) where the two populations differ functionally\n  \n  # Data prep for input to fdatest\n  .x &lt;- enquo(x)\n  # wm0 = N x J matrix of the functional data for wm absent\n  wm0 &lt;- \n    wm_data %&gt;%\n    dplyr::filter(wm == -1) %&gt;%\n    dplyr::select(subject, dap, !!.x) %&gt;%\n    tidyr::pivot_wider(id_cols = subject, names_from = dap, names_prefix = \"\", values_from = !!.x) %&gt;%\n    dplyr::select(-subject) %&gt;%\n    as.matrix()\n    # and ...\n    colnames(wm0) &lt;- NULL\n    \n  # wm1 = N x J matrix of the functional data for wm present  \n  wm1 &lt;- \n    wm_data %&gt;%\n    dplyr::filter(wm == 1) %&gt;%\n    dplyr::select(subject, dap, !!.x) %&gt;%\n    tidyr::pivot_wider(id_cols = subject, names_from = dap, names_prefix = \"\", values_from = !!.x) %&gt;%\n    dplyr::select(-subject) %&gt;%\n    as.matrix()\n    colnames(wm1) &lt;- NULL\n  \n  dat &lt;- list(wm0 = wm0, wm1 = wm1)\n  \n  # The function generates a print line for each iteration. To suppress that, wrap within sink:\n  # (set a seed for reproducibility)\n  { sink(nullfile()); set.seed(86754309); foo &lt;- ITP2bspline(dat$wm0, dat$wm1, B = 10000); sink() }\n\n  # Selecting the significant components at the specified alpha level:\n  # Which corresponds to the following days (relative to sowing, where sowing is day = 0):\n  z &lt;- seq(-30, 50)[which(foo$corrected.pval &lt; alpha)]\n  \n  res &lt;- tibble(var = rlang::as_name(.x), days = list(z))\n  \n  return(res)\n}\n\n# Example of use:\n# do.fdatest(x = d2m, alpha = 0.05)\n\nfilter.iwt &lt;- function(x) {\n  # Filter the interval-wise testing results to see the days that were significant\n  # Args:\n  #  x = the series (quoted character string)\n  # Returns:\n  #  a vector of the days where the series was different between wm = 0 and wm = 1\n  iwt |&gt;\n  dplyr::filter(var == x) |&gt;\n  purrr::pluck(\"days\", 1)\n}\n\n\nzee &lt;- function(series) {\n  # Output the start and end days of significant windows within a time series\n  # Args:\n  #  series = quoted character name of the series\n  # Returns:\n  #  a table\n  v &lt;- \n    filter.iwt(x = series) %&gt;% \n    split(., cumsum(c(1, diff(.) != 1)))\n  \n  # Create an empty data frame with three named columns:\n  z &lt;- data.frame(matrix(\n    vector(), 0, 3, dimnames = list(c(), c(\"series\", \"start\", \"end\"))), \n    stringsAsFactors = F)\n  \n  # Now loop over v to pick out the start and end of the continuous windows:\n  for (i in 1:length(v)) {\n    b &lt;- purrr::pluck(v, i) %&gt;%  dplyr::first()\n    c &lt;- purrr::pluck(v, i) %&gt;%  dplyr::last()\n    z[i, \"series\"] &lt;- series\n    z[i, \"start\"] &lt;- b\n    z[i, \"end\"] &lt;- c\n    } # end for loop\n  return(z)\n}\n\n# Examples of use:\n# zee(\"t2m_mean\")\n# zee(\"d2m\")\n\n#  Example of the workflow:\n###---###\n# tst.d2m &lt;- do.fdatest(x = d2m)\n\n# You could output this way:\n# pluck(iwt.list, \"tst.d2m\") %&gt;% \n#   make_kable()\n\n# But this shows the windows in a cleaner format:  \n# zee(\"d2m\") %&gt;% \n  # make_kable()\n###---###\n\n# Now we're ready to roll...",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#p-0.05",
    "href": "code_function_on_scalar.html#p-0.05",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "P < 0.05",
    "text": "P &lt; 0.05\n\n\nload(here::here(\"FunctionalDataAnalysis\", \"FunctiononScalar\", \"iwt05.RData\"))\n\n\nDew point\n\nzee(\"d2m\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nd2m\n0\n0\n\n\n\n\n\n\n\n\n\nMean air temperature\n\nzee(\"t2m_mean\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nt2m_mean\n0\n4\n\n\n\n\n\n\n\n\n\nMax air temperature\n\nzee(\"t2m_max\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nt2m_max\n0\n4\n\n\n\n\n\n\n\n\n\nMin air temperature\n\nzee(\"t2m_min\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nt2m_min\n0\n1\n\n\n\n\n\n\n\n\n\nMax - Min air temperature\n\nzee(\"tdiff\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\ntdiff\nNA\nNA\n\n\n\n\n\n\n\n\n\nTemperature-Dewpoint depression\n\nzee(\"dpd\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\ndpd\nNA\nNA\n\n\n\n\n\n\n\n\n\nSoil temperature\n\nzee(\"st\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nst\n1\n3\n\n\n\n\n\n\n\n\n\nSoil moisture\n\nzee(\"sm\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nsm\n-4\n3\n\n\n2\nsm\n5\n15\n\n\n3\nsm\n17\n24\n\n\n4\nsm\n40\n49\n\n\n\n\n\n\n\n\n\nsoil temperature:soil moisture ratio\n\nzee(\"stsm\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nstsm\n24\n25\n\n\n2\nstsm\n35\n44\n\n\n\n\n\n\n\n\n\nSurface pressure\n\nzee(\"sp\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nsp\nNA\nNA\n\n\n\n\n\n\n\n\n\nRelative humidity\n\nzee(\"rh\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nrh\nNA\nNA\n\n\n\n\n\n\n\n\n\nVPD\n\nzee(\"vpd\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nvpd\nNA\nNA\n\n\n\n\n\n\n\n\n\n# Let's use the filter.iwt function to have a look at the intervals that were significant (all series, ignoring gdd):\n# air temperature-related\nfilter.iwt(x = \"t2m_mean\")\nfilter.iwt(x = \"t2m_max\")\nfilter.iwt(x = \"t2m_min\")\nfilter.iwt(x = \"tdiff\")\nfilter.iwt(x = \"d2m\") \nfilter.iwt(x = \"dpd\")\n\n# soil-related\nfilter.iwt(x = \"sm\")\nfilter.iwt(x = \"st\")\nfilter.iwt(x = \"stsm\")\n\n# surface pressure\nfilter.iwt(x = \"sp\")\n\n# moisture-related\nfilter.iwt(x = \"rh\")\nfilter.iwt(x = \"vpd\")\n\n# We want to be able to cut the significant periods into continuous windows should those exist.\n# The code to do so is adapted from here:\n# https://stackoverflow.com/questions/5222061/create-grouping-variable-for-consecutive-sequences-and-split-vector\n# Examples:\nfilter.iwt(x = \"t2m_max\") %&gt;% split(., cumsum(c(1, diff(.) != 1)))\nfilter.iwt(x = \"stsm\") %&gt;% split(., cumsum(c(1, diff(.) != 1)))\n\n\n# What we're ultimately interested in is parsing the significant dates into any continuous segments.\n# The find the start and end of these segments, so that we can input those to a dataframe for plotting. For that we use the function `zee` (see the functions chunk)\n\n# Map over e and bind the rows:\ng05 &lt;- purrr::map(e, zee) |&gt; list_rbind()",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#p-0.01",
    "href": "code_function_on_scalar.html#p-0.01",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "P < 0.01",
    "text": "P &lt; 0.01\n\n\nload(here::here(\"FunctionalDataAnalysis\", \"FunctiononScalar\", \"iwt01.RData\"))\n\n\nDew point\n\nzee(\"d2m\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nd2m\n0\n0\n\n\n\n\n\n\n\n\n\nMean air temperature\n\nzee(\"t2m_mean\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nt2m_mean\n0\n1\n\n\n\n\n\n\n\n\n\nMax air temperature\n\nzee(\"t2m_max\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nt2m_max\n0\n0\n\n\n\n\n\n\n\n\n\nMin air temperature\n\nzee(\"t2m_min\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nt2m_min\n0\n0\n\n\n\n\n\n\n\n\n\nMax - Min air temperature\n\nzee(\"tdiff\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\ntdiff\nNA\nNA\n\n\n\n\n\n\n\n\n\nTemperature-Dewpoint depression\n\nzee(\"dpd\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\ndpd\nNA\nNA\n\n\n\n\n\n\n\n\n\nSoil temperature\n\nzee(\"st\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nst\n1\n1\n\n\n\n\n\n\n\n\n\nSoil moisture\n\nzee(\"sm\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nsm\n12\n12\n\n\n2\nsm\n46\n48\n\n\n\n\n\n\n\n\n\nsoil temperature:soil moisture ratio\n\nzee(\"stsm\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nstsm\nNA\nNA\n\n\n\n\n\n\n\n\n\nSurface pressure\n\nzee(\"sp\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nsp\nNA\nNA\n\n\n\n\n\n\n\n\n\nRelative humidity\n\nzee(\"rh\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nrh\nNA\nNA\n\n\n\n\n\n\n\n\n\nVPD\n\nzee(\"vpd\") %&gt;% \n  make_kable()\n\n\n\n\n\nseries\nstart\nend\n\n\n\n\n1\nvpd\nNA\nNA\n\n\n\n\n\n\n\n\n\n# What we're ultimately interested in is parsing the significant dates into any continuous segments.\n# The find the start and end of these segments, so that we can input those to a dataframe for plotting. For that we use the function `zee` (see the functions chunk)\n\n# Map over e and bind the rows:\ng01 &lt;- purrr::map(e, zee) |&gt; list_rbind()",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#p-0.05-1",
    "href": "code_function_on_scalar.html#p-0.05-1",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "P < 0.05",
    "text": "P &lt; 0.05\n\n# Code for plotting the segments derived from here:\n# https://stackoverflow.com/questions/35322919/grouped-data-by-factor-with-geom-segment\n\ng05 %&gt;% \n  ggplot(., aes(ymin = start, ymax = end, x = series)) + \n  # Changing the order on the x-axis for the categories:\n  scale_x_discrete(limits = rev(c(\"t2m_mean\", \"t2m_max\", \"t2m_min\", \"tdiff\", \"d2m\", \"dpd\", \"sm\", \"st\", \"stsm\", \"sp\", \"rh\", \"vpd\")), labels = var_labels, name = NULL) + \n  # Pay attention to layers. Do coord_flip first, then annotations to appear in the background, segment the topmost layer:\n  coord_flip() +\n  # Add some guides:\n  geom_hline(aes(yintercept = 0), color = \"grey\", linetype = \"dashed\") +\n  annotate(\"rect\", xmin = -Inf, xmax = Inf, ymin = 35, ymax = 50, fill = \"steelblue\", alpha = 0.2) +\n    geom_linerange(colour = \"grey20\", position = position_dodge(width = 0.2), linewidth = 3, na.rm = TRUE) + \n    # theme_bw() +\n    theme_half_open(font_size = 12)+\n    labs(x = \"Series\", y = \"Days relative to sowing\") +\n    theme(axis.title = element_text(face = \"bold\", size = 11))\n\n\n\n\n\n\n\nggsave(\"figs/fda_difference_period_p05.png\", dpi = 600, bg = \"white\")\nggsave(\"figs/fda_difference_period_p05.pdf\", dpi = 600, bg = \"white\")",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#p-0.01-1",
    "href": "code_function_on_scalar.html#p-0.01-1",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "P < 0.01",
    "text": "P &lt; 0.01\n\n# Code for plotting the segments derived from here:\n# https://stackoverflow.com/questions/35322919/grouped-data-by-factor-with-geom-segment\n\ng01 %&gt;% \n  ggplot(., aes(ymin = start, ymax = end, x = series)) + \n  # Changing the order on the x-axis for the categories:\n  scale_x_discrete(limits = rev(c(\"t2m_mean\", \"t2m_max\", \"t2m_min\", \"tdiff\", \"d2m\", \"dpd\", \"sm\", \"st\", \"stsm\", \"sp\", \"rh\", \"vpd\")), labels = var_labels, name = NULL) + \n  # Pay attention to layers. Do coord_flip first, then annotations to appear in the background, segment the topmost layer:\n  coord_flip() +\n  # Add some guides:\n  geom_hline(aes(yintercept = 0), color = \"grey\", linetype = \"dashed\") +\n  annotate(\"rect\", xmin = -Inf, xmax = Inf, ymin = 35, ymax = 50, fill = \"steelblue\", alpha = 0.2) +\n    geom_linerange(colour = \"grey20\", position = position_dodge(width = 0.2), linewidth = 3, na.rm = TRUE) + \n    # theme_bw() +\n    theme_half_open(font_size = 12)+\n    labs(x = \"Series\", y = \"Days relative to sowing\") +\n    theme(axis.title = element_text(face = \"bold\", size = 11))\n\n\n\n\n\n\n\n# ggsave(\"figs/fda_difference_period_p01.png\", dpi = 600, bg = \"white\")\n# ggsave(\"figs/fda_difference_period_p01.pdf\", dpi = 600, bg = \"white\")\n\n\n\nweather_vars &lt;- function(whichvar, start, end) {\n  # Calculate the weather-based summary variable for each observation\n  # Args:\n  #  whichvar = unquoted character string of the variable (series), e.g., sm\n  #  start = the start day relative to sowing\n  #  end = the end day of the window relative to sowing\n  # Returns:\n  #  a data frame with two columns (subject, the weather-based summary variable)\n  \n  var &lt;- enquo(whichvar)\n  # Create a name for the column to hold the summary variable:\n  var_name &lt;- paste(rlang::as_name(var), start, end, sep = \"_\")\n  \n  wm_data %&gt;%\n    dplyr::select(subject, dap, !!var) %&gt;%\n    dplyr::filter(dap %in% c(start:end)) %&gt;%\n    dplyr::group_by(subject) %&gt;%\n    dplyr::summarise(\"{var_name}\" := mean(!!var))\n  } # end of function\n\n\n# These are the variables and windows we'll create summaries for:\n# t2m_mean  start = 0, end = 4\n# sm start = -4, end = 3\n# sm start = 5, end = 15\n# sm start = 17, end = 24\n# sm start = 40, end = 49\n# stsm start = 35, end = 44\n\nu &lt;- weather_vars(whichvar = t2m_mean, start = 0, end = 4)\nv &lt;- weather_vars(whichvar = sm, start = -4, end = 3)\nw &lt;- weather_vars(whichvar = sm, start = 5, end = 15)\nx &lt;- weather_vars(whichvar = sm, start = 17, end = 24)\ny &lt;- weather_vars(whichvar = sm, start = 40, end = 49)\nz &lt;- weather_vars(whichvar = stsm, start = 35, end = 44)\n\nweather.vars &lt;- purrr::reduce(list(u, v, w, x, y, z), dplyr::left_join, by = \"subject\")\n\n# Save the weather.vars data frame:\nsave(weather.vars, file = \"WeatherVars.RData\")",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  },
  {
    "objectID": "code_function_on_scalar.html#session-info",
    "href": "code_function_on_scalar.html#session-info",
    "title": "Function-on-Scalar regressions for the white mold data",
    "section": "Session Info",
    "text": "Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Portuguese_Brazil.utf8  LC_CTYPE=Portuguese_Brazil.utf8   \n[3] LC_MONETARY=Portuguese_Brazil.utf8 LC_NUMERIC=C                      \n[5] LC_TIME=Portuguese_Brazil.utf8    \n\ntime zone: America/Sao_Paulo\ntzcode source: internal\n\nattached base packages:\n[1] splines   stats     graphics  grDevices datasets  utils     methods  \n[8] base     \n\nother attached packages:\n [1] kableExtra_1.4.0 cowplot_1.1.3    tictoc_1.2.1     lubridate_1.9.3 \n [5] forcats_1.0.0    stringr_1.5.1    dplyr_1.1.4      purrr_1.0.2     \n [9] readr_2.1.5      tidyr_1.3.1      tibble_3.2.1     ggplot2_3.5.1   \n[13] tidyverse_2.0.0  fdatest_2.1.1    refund_0.1-37    fda_6.2.0       \n[17] deSolve_1.40     fds_1.8          RCurl_1.98-1.16  rainbow_3.8     \n[21] pcaPP_2.0-5      MASS_7.3-60.2    knitr_1.48      \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1   hdrcde_3.4         viridisLite_0.4.2  RLRsim_3.1-8      \n [5] farver_2.1.2       bitops_1.0-9       fastmap_1.2.0      pracma_2.4.4      \n [9] digest_0.6.37      timechange_0.3.0   lifecycle_1.0.4    cluster_2.1.6     \n[13] magrittr_2.0.3     compiler_4.4.1     rlang_1.1.4        tools_4.4.1       \n[17] utf8_1.2.4         yaml_2.3.10        labeling_0.4.3     grpreg_3.5.0      \n[21] htmlwidgets_1.6.4  bit_4.5.0          mclust_6.1.1       here_1.0.1        \n[25] xml2_1.3.6         abind_1.4-8        KernSmooth_2.23-24 gamm4_0.2-6       \n[29] withr_3.0.2        grid_4.4.1         fansi_1.0.6        colorspace_2.1-1  \n[33] scales_1.3.0       cli_3.6.3          mvtnorm_1.3-2      crayon_1.5.3      \n[37] rmarkdown_2.28     ragg_1.3.3         generics_0.1.3     rstudioapi_0.17.0 \n[41] tzdb_0.4.0         magic_1.6-1        minqa_1.2.8        parallel_4.4.1    \n[45] vctrs_0.6.5        boot_1.3-30        Matrix_1.7-0       jsonlite_1.8.9    \n[49] hms_1.1.3          bit64_4.5.2        systemfonts_1.1.0  glue_1.8.0        \n[53] nloptr_2.1.1       stringi_1.8.4      gtable_0.3.5       lme4_1.1-35.5     \n[57] munsell_0.5.1      pillar_1.9.0       htmltools_0.5.8.1  R6_2.5.1          \n[61] textshaping_0.4.0  ks_1.14.3          rprojroot_2.0.4    vroom_1.6.5       \n[65] evaluate_1.0.1     lattice_0.22-6     highr_0.11         renv_1.1.2        \n[69] pbs_1.1            Rcpp_1.0.13        svglite_2.1.3      nlme_3.1-164      \n[73] mgcv_1.9-1         xfun_0.48          pkgconfig_2.0.3",
    "crumbs": [
      "Functional data analysis",
      "Function-on-Scalar regressions for the white mold data"
    ]
  }
]